{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  0, 13, 26, 12,  0, 13, 24, 26,  8, 13, 26, 19,  7,  4, 26,  0, 13,\n",
      "         0, 17])\n",
      "man many in the anar\n"
     ]
    }
   ],
   "source": [
    "tokeniser = Tokeniser()\n",
    "print(tokeniser.encode(train_inputs[0]))\n",
    "print(tokeniser.decode(tokeniser.encode(train_inputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset\n",
    "\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['input_ids'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n",
      "torch.Size([4, 20, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2080,  0.3486, -1.4109,  ..., -0.0000,  0.9507,  1.3731],\n",
       "         [-0.6865,  0.0462,  0.9581,  ..., -1.9168, -0.1615, -0.5618],\n",
       "         [-2.7787, -0.5571,  1.1729,  ..., -0.2707, -2.6766, -0.8761],\n",
       "         ...,\n",
       "         [-3.4495, -0.7200, -0.0000,  ...,  2.2209,  1.2695, -1.8376],\n",
       "         [ 0.3983, -3.9499, -1.0337,  ...,  1.6758, -1.2004,  1.3699],\n",
       "         [-0.0000, -0.8123,  1.8915,  ..., -2.4869,  0.8191, -0.9790]],\n",
       "\n",
       "        [[ 2.0925,  1.2849,  0.4627,  ..., -2.8872,  1.0966, -0.7056],\n",
       "         [-3.5753, -0.8238, -0.9908,  ...,  0.3585,  0.3943, -0.9186],\n",
       "         [-4.2916, -0.9698,  0.5930,  ..., -1.9347, -0.5093, -1.9573],\n",
       "         ...,\n",
       "         [-3.5811, -0.3371,  0.6335,  ...,  2.2270, -1.4859, -0.2825],\n",
       "         [ 2.0938, -1.0107,  0.1994,  ..., -0.8914, -1.3681,  0.7034],\n",
       "         [-3.9533, -0.8123,  1.8915,  ..., -0.0000,  0.8191, -0.9790]],\n",
       "\n",
       "        [[ 2.2594,  0.3950, -1.0377,  ..., -1.0905, -0.0249, -1.0819],\n",
       "         [-3.5688, -1.2639, -0.1081,  ..., -1.5100,  0.8680, -1.4419],\n",
       "         [-2.1754,  0.0861,  0.0000,  ...,  2.6868, -3.9225, -0.5695],\n",
       "         ...,\n",
       "         [-3.4430, -1.1601,  0.5144,  ...,  0.3524,  1.7432, -2.3608],\n",
       "         [ 0.7244, -1.9082, -0.2870,  ...,  1.1795, -0.0000,  0.9044],\n",
       "         [-2.4911, -0.1785,  2.6317,  ...,  0.2553,  0.0495,  0.7860]],\n",
       "\n",
       "        [[ 2.0925,  1.2849,  0.4627,  ..., -2.8872,  1.0966, -0.7056],\n",
       "         [ 1.8241,  0.4453, -0.6811,  ...,  0.7736, -0.2196, -1.5819],\n",
       "         [-3.9302, -0.0000,  1.2219,  ..., -0.0000, -2.8094,  0.0000],\n",
       "         ...,\n",
       "         [-1.9809, -0.5264,  1.2546,  ...,  3.0946,  0.9737, -0.5958],\n",
       "         [ 2.0938, -1.0107,  0.1994,  ..., -0.8914, -0.0000,  0.7034],\n",
       "         [-3.9533, -0.8123,  1.8915,  ..., -2.4869,  0.8191, -0.9790]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERTEmbedding().forward(dataset_test['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, length: int, embed_dim: int, dropout: int):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : int\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    encoder_block : nn.TransformerEncoder\n",
    "        Transformer Encoder.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 27, length: int = 20, embed_dim: int = 768, dropout: int = 0.1, attention_heads: int = 12, layers: int = 2):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        embed_dim : int, optional\n",
    "            Dimensionality of the token and position embeddings. Defaults to 768.\n",
    "        dropout : int, optional\n",
    "            Dropout probability, used for regularisation. Defaults to 0.1.\n",
    "        attention_heads : int, optional\n",
    "            The number of attention heads in the Transformer encoder layer. Defaults to 12.\n",
    "        layers : int, optional\n",
    "            The number of Transformer encoder layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()  # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size, length, embed_dim, dropout) # embedding layer which combines token and position embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, attention_heads, dim_feedforward=embed_dim * 4, dropout=dropout, activation=\"gelu\") # instance of transformer encoder layer\n",
    "        self.encoder_block = nn.TransformerEncoder(encoder_layer, layers) # full transformer encoder consisting of multiple layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "        embeddings = embeddings.transpose(0, 1) # rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\n",
    "\n",
    "        encoder_output = self.encoder_block(embeddings) # pass embeddings through transformer encoder block\n",
    "\n",
    "        logits = self.classifier(encoder_output.transpose(0, 1)) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrozi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset_test['input_ids'][0]).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
