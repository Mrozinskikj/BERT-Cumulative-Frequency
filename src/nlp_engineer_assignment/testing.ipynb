{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, length: int, embed_dim: int, dropout: int):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : int\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    encoder_block : nn.TransformerEncoder\n",
    "        Transformer Encoder.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 27, length: int = 20, embed_dim: int = 768, dropout: int = 0.1, attention_heads: int = 12, layers: int = 2):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        embed_dim : int, optional\n",
    "            Dimensionality of the token and position embeddings. Defaults to 768.\n",
    "        dropout : int, optional\n",
    "            Dropout probability, used for regularisation. Defaults to 0.1.\n",
    "        attention_heads : int, optional\n",
    "            The number of attention heads in the Transformer encoder layer. Defaults to 12.\n",
    "        layers : int, optional\n",
    "            The number of Transformer encoder layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()  # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size, length, embed_dim, dropout) # embedding layer which combines token and position embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, attention_heads, dim_feedforward=embed_dim * 4, dropout=dropout, activation=\"gelu\") # instance of transformer encoder layer\n",
    "        self.encoder_block = nn.TransformerEncoder(encoder_layer, layers) # full transformer encoder consisting of multiple layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "        embeddings = embeddings.transpose(0, 1) # rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\n",
    "\n",
    "        encoder_output = self.encoder_block(embeddings) # pass embeddings through transformer encoder block\n",
    "\n",
    "        logits = self.classifier(encoder_output.transpose(0, 1)) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrozi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)\n",
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model : BERT, dataset_test: dict, loss_fn: nn.CrossEntropyLoss) -> float:\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    batches = len(dataset_test['input_ids']) # number of batches in the test dataset\n",
    "    loss_total = 0\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        for batch in range(batches):\n",
    "            \n",
    "            logits = model(dataset_test['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_test['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "\n",
    "            loss_batch = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            loss_total += loss_batch.item()\n",
    "\n",
    "    loss_average = loss_total / batches # loss is the average of all batches\n",
    "    print(f'eval loss: {round(loss_average,2)}')\n",
    "    model.train() # revert model to training mode\n",
    "    return loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(plot_data: dict):\n",
    "    fig, axs = plt.subplots(len(plot_data.keys()), 1, figsize=(8, 6), sharex=True) # create subplots\n",
    "\n",
    "    for p,plot in enumerate(plot_data.keys()): # plot x,y of each subplot in plot_data\n",
    "        axs[p].plot(plot_data[plot]['x'],plot_data[plot]['y'])\n",
    "        axs[p].set_title(plot)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(dataset_train: dict, dataset_test: dict, learning_rate: float = 1e-6, epochs: int = 1, eval_every: int = 250) -> BERT:\n",
    "    \"\"\"\n",
    "    Creates and trains a BERT model for cumulative frequency classification given a training dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the training data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        Refer to 'dataset_train'.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for the optimiser (magnitiude of weight updates per step). Defaults to 1e-6.\n",
    "    epochs : int, optional\n",
    "        The number of epochs for training. Each epoch corresponds to one full iteration through training data. Defaults to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BERT\n",
    "        The trained BERT model.\n",
    "    \"\"\"\n",
    "    plot_data = {'train':{'x':[],'y':[]}, 'test':{'x':[],'y':[]}} # dict storing x,y plot data for training progress\n",
    "\n",
    "    model = BERT() # initialise model\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate) # initialise AdamW optimiser\n",
    "    loss_fn = nn.CrossEntropyLoss() # initialise cross-entropy loss function for classification\n",
    "\n",
    "    batches = len(dataset_train['input_ids']) # number of batches in the training dataset\n",
    "    for epoch in range(epochs): # iterate through epochs\n",
    "        for batch in range(batches): # iterate through batches in epoch\n",
    "            \n",
    "            if batch%eval_every == 0: # perform evaluation on test split at set intervals\n",
    "                loss_test = evaluate(model, dataset_test, loss_fn)\n",
    "                plot_data['test']['x'].append(batch)\n",
    "                plot_data['test']['y'].append(loss_test)\n",
    "\n",
    "            logits = model(dataset_train['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "            \n",
    "            loss = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            \n",
    "            optimiser.zero_grad() # zero the gradients from previous step (no gradient accumulation)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimiser.step() # update model weights\n",
    "\n",
    "            plot_data['train']['x'].append(batch)\n",
    "            plot_data['train']['y'].append(loss.item())\n",
    "            print(f'step: {batch*(epoch+1)}/{batches*epochs} loss: {round(loss.item(),2)}')\n",
    "    \n",
    "    plot_train(plot_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrozi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 1.19\n",
      "step: 0/2500 loss: 1.05\n",
      "step: 1/2500 loss: 1.33\n",
      "step: 2/2500 loss: 1.15\n",
      "step: 3/2500 loss: 1.17\n",
      "step: 4/2500 loss: 1.11\n",
      "step: 5/2500 loss: 1.22\n",
      "step: 6/2500 loss: 1.12\n",
      "step: 7/2500 loss: 1.15\n",
      "step: 8/2500 loss: 1.15\n",
      "step: 9/2500 loss: 1.17\n",
      "step: 10/2500 loss: 1.11\n",
      "step: 11/2500 loss: 1.15\n",
      "step: 12/2500 loss: 1.11\n",
      "step: 13/2500 loss: 1.18\n",
      "step: 14/2500 loss: 0.99\n",
      "step: 15/2500 loss: 1.21\n",
      "step: 16/2500 loss: 1.11\n",
      "step: 17/2500 loss: 1.05\n",
      "step: 18/2500 loss: 1.17\n",
      "step: 19/2500 loss: 1.15\n",
      "step: 20/2500 loss: 1.05\n",
      "step: 21/2500 loss: 1.05\n",
      "step: 22/2500 loss: 1.12\n",
      "step: 23/2500 loss: 1.15\n",
      "step: 24/2500 loss: 1.02\n",
      "step: 25/2500 loss: 1.1\n",
      "step: 26/2500 loss: 1.08\n",
      "step: 27/2500 loss: 1.04\n",
      "step: 28/2500 loss: 1.07\n",
      "step: 29/2500 loss: 1.11\n",
      "step: 30/2500 loss: 1.08\n",
      "step: 31/2500 loss: 1.03\n",
      "step: 32/2500 loss: 1.11\n",
      "step: 33/2500 loss: 1.03\n",
      "step: 34/2500 loss: 1.13\n",
      "step: 35/2500 loss: 1.05\n",
      "step: 36/2500 loss: 1.09\n",
      "step: 37/2500 loss: 1.09\n",
      "step: 38/2500 loss: 1.02\n",
      "step: 39/2500 loss: 1.08\n",
      "step: 40/2500 loss: 1.07\n",
      "step: 41/2500 loss: 0.98\n",
      "step: 42/2500 loss: 1.04\n",
      "step: 43/2500 loss: 1.1\n",
      "step: 44/2500 loss: 0.99\n",
      "step: 45/2500 loss: 1.09\n",
      "step: 46/2500 loss: 0.97\n",
      "step: 47/2500 loss: 1.07\n",
      "step: 48/2500 loss: 1.05\n",
      "step: 49/2500 loss: 0.98\n",
      "step: 50/2500 loss: 1.11\n",
      "step: 51/2500 loss: 1.05\n",
      "step: 52/2500 loss: 1.01\n",
      "step: 53/2500 loss: 1.02\n",
      "step: 54/2500 loss: 1.02\n",
      "step: 55/2500 loss: 0.96\n",
      "step: 56/2500 loss: 0.97\n",
      "step: 57/2500 loss: 1.02\n",
      "step: 58/2500 loss: 0.93\n",
      "step: 59/2500 loss: 1.01\n",
      "step: 60/2500 loss: 1.02\n",
      "step: 61/2500 loss: 1.01\n",
      "step: 62/2500 loss: 1.05\n",
      "step: 63/2500 loss: 0.94\n",
      "step: 64/2500 loss: 0.97\n",
      "step: 65/2500 loss: 1.06\n",
      "step: 66/2500 loss: 1.04\n",
      "step: 67/2500 loss: 0.93\n",
      "step: 68/2500 loss: 0.95\n",
      "step: 69/2500 loss: 0.96\n",
      "step: 70/2500 loss: 0.99\n",
      "step: 71/2500 loss: 0.96\n",
      "step: 72/2500 loss: 0.95\n",
      "step: 73/2500 loss: 1.0\n",
      "step: 74/2500 loss: 1.03\n",
      "step: 75/2500 loss: 1.05\n",
      "step: 76/2500 loss: 1.0\n",
      "step: 77/2500 loss: 0.96\n",
      "step: 78/2500 loss: 1.03\n",
      "step: 79/2500 loss: 0.99\n",
      "step: 80/2500 loss: 1.01\n",
      "step: 81/2500 loss: 0.9\n",
      "step: 82/2500 loss: 1.0\n",
      "step: 83/2500 loss: 0.95\n",
      "step: 84/2500 loss: 0.99\n",
      "step: 85/2500 loss: 0.91\n",
      "step: 86/2500 loss: 1.03\n",
      "step: 87/2500 loss: 1.08\n",
      "step: 88/2500 loss: 1.01\n",
      "step: 89/2500 loss: 0.94\n",
      "step: 90/2500 loss: 0.96\n",
      "step: 91/2500 loss: 0.91\n",
      "step: 92/2500 loss: 0.94\n",
      "step: 93/2500 loss: 0.98\n",
      "step: 94/2500 loss: 0.97\n",
      "step: 95/2500 loss: 0.96\n",
      "step: 96/2500 loss: 0.96\n",
      "step: 97/2500 loss: 0.96\n",
      "step: 98/2500 loss: 0.93\n",
      "step: 99/2500 loss: 0.91\n",
      "step: 100/2500 loss: 0.94\n",
      "step: 101/2500 loss: 0.97\n",
      "step: 102/2500 loss: 0.92\n",
      "step: 103/2500 loss: 1.03\n",
      "step: 104/2500 loss: 0.92\n",
      "step: 105/2500 loss: 1.03\n",
      "step: 106/2500 loss: 0.86\n",
      "step: 107/2500 loss: 0.93\n",
      "step: 108/2500 loss: 0.91\n",
      "step: 109/2500 loss: 0.91\n",
      "step: 110/2500 loss: 0.9\n",
      "step: 111/2500 loss: 0.98\n",
      "step: 112/2500 loss: 0.98\n",
      "step: 113/2500 loss: 0.95\n",
      "step: 114/2500 loss: 0.92\n",
      "step: 115/2500 loss: 0.89\n",
      "step: 116/2500 loss: 0.98\n",
      "step: 117/2500 loss: 0.97\n",
      "step: 118/2500 loss: 0.99\n",
      "step: 119/2500 loss: 0.83\n",
      "step: 120/2500 loss: 1.06\n",
      "step: 121/2500 loss: 0.96\n",
      "step: 122/2500 loss: 0.92\n",
      "step: 123/2500 loss: 0.88\n",
      "step: 124/2500 loss: 0.99\n",
      "step: 125/2500 loss: 0.87\n",
      "step: 126/2500 loss: 0.93\n",
      "step: 127/2500 loss: 0.93\n",
      "step: 128/2500 loss: 0.88\n",
      "step: 129/2500 loss: 0.89\n",
      "step: 130/2500 loss: 1.0\n",
      "step: 131/2500 loss: 0.99\n",
      "step: 132/2500 loss: 0.89\n",
      "step: 133/2500 loss: 1.0\n",
      "step: 134/2500 loss: 0.88\n",
      "step: 135/2500 loss: 0.89\n",
      "step: 136/2500 loss: 1.08\n",
      "step: 137/2500 loss: 0.84\n",
      "step: 138/2500 loss: 0.83\n",
      "step: 139/2500 loss: 0.92\n",
      "step: 140/2500 loss: 0.91\n",
      "step: 141/2500 loss: 0.85\n",
      "step: 142/2500 loss: 0.9\n",
      "step: 143/2500 loss: 0.93\n",
      "step: 144/2500 loss: 0.98\n",
      "step: 145/2500 loss: 0.9\n",
      "step: 146/2500 loss: 0.97\n",
      "step: 147/2500 loss: 0.85\n",
      "step: 148/2500 loss: 0.98\n",
      "step: 149/2500 loss: 0.81\n",
      "step: 150/2500 loss: 0.87\n",
      "step: 151/2500 loss: 0.97\n",
      "step: 152/2500 loss: 0.93\n",
      "step: 153/2500 loss: 0.94\n",
      "step: 154/2500 loss: 0.89\n",
      "step: 155/2500 loss: 0.91\n",
      "step: 156/2500 loss: 0.86\n",
      "step: 157/2500 loss: 0.84\n",
      "step: 158/2500 loss: 0.86\n",
      "step: 159/2500 loss: 0.92\n",
      "step: 160/2500 loss: 0.9\n",
      "step: 161/2500 loss: 0.84\n",
      "step: 162/2500 loss: 0.87\n",
      "step: 163/2500 loss: 0.92\n",
      "step: 164/2500 loss: 0.87\n",
      "step: 165/2500 loss: 0.9\n",
      "step: 166/2500 loss: 0.85\n",
      "step: 167/2500 loss: 0.86\n",
      "step: 168/2500 loss: 0.89\n",
      "step: 169/2500 loss: 0.85\n",
      "step: 170/2500 loss: 0.89\n",
      "step: 171/2500 loss: 0.79\n",
      "step: 172/2500 loss: 0.9\n",
      "step: 173/2500 loss: 0.84\n",
      "step: 174/2500 loss: 0.9\n",
      "step: 175/2500 loss: 0.91\n",
      "step: 176/2500 loss: 0.82\n",
      "step: 177/2500 loss: 0.91\n",
      "step: 178/2500 loss: 0.97\n",
      "step: 179/2500 loss: 0.83\n",
      "step: 180/2500 loss: 0.9\n",
      "step: 181/2500 loss: 0.86\n",
      "step: 182/2500 loss: 0.84\n",
      "step: 183/2500 loss: 0.87\n",
      "step: 184/2500 loss: 0.81\n",
      "step: 185/2500 loss: 0.88\n",
      "step: 186/2500 loss: 0.76\n",
      "step: 187/2500 loss: 1.11\n",
      "step: 188/2500 loss: 0.84\n",
      "step: 189/2500 loss: 0.81\n",
      "step: 190/2500 loss: 0.85\n",
      "step: 191/2500 loss: 0.88\n",
      "step: 192/2500 loss: 0.93\n",
      "step: 193/2500 loss: 0.88\n",
      "step: 194/2500 loss: 0.88\n",
      "step: 195/2500 loss: 0.9\n",
      "step: 196/2500 loss: 0.85\n",
      "step: 197/2500 loss: 0.89\n",
      "step: 198/2500 loss: 0.92\n",
      "step: 199/2500 loss: 0.89\n",
      "step: 200/2500 loss: 0.82\n",
      "step: 201/2500 loss: 0.94\n",
      "step: 202/2500 loss: 0.82\n",
      "step: 203/2500 loss: 0.82\n",
      "step: 204/2500 loss: 0.95\n",
      "step: 205/2500 loss: 0.74\n",
      "step: 206/2500 loss: 0.89\n",
      "step: 207/2500 loss: 0.94\n",
      "step: 208/2500 loss: 0.88\n",
      "step: 209/2500 loss: 0.85\n",
      "step: 210/2500 loss: 0.87\n",
      "step: 211/2500 loss: 1.0\n",
      "step: 212/2500 loss: 0.88\n",
      "step: 213/2500 loss: 0.87\n",
      "step: 214/2500 loss: 0.86\n",
      "step: 215/2500 loss: 0.82\n",
      "step: 216/2500 loss: 0.89\n",
      "step: 217/2500 loss: 0.81\n",
      "step: 218/2500 loss: 0.81\n",
      "step: 219/2500 loss: 0.9\n",
      "step: 220/2500 loss: 0.85\n",
      "step: 221/2500 loss: 0.82\n",
      "step: 222/2500 loss: 0.77\n",
      "step: 223/2500 loss: 0.76\n",
      "step: 224/2500 loss: 0.9\n",
      "step: 225/2500 loss: 0.87\n",
      "step: 226/2500 loss: 0.76\n",
      "step: 227/2500 loss: 0.75\n",
      "step: 228/2500 loss: 0.87\n",
      "step: 229/2500 loss: 0.78\n",
      "step: 230/2500 loss: 0.79\n",
      "step: 231/2500 loss: 0.9\n",
      "step: 232/2500 loss: 0.9\n",
      "step: 233/2500 loss: 0.9\n",
      "step: 234/2500 loss: 0.82\n",
      "step: 235/2500 loss: 0.83\n",
      "step: 236/2500 loss: 0.91\n",
      "step: 237/2500 loss: 0.83\n",
      "step: 238/2500 loss: 0.74\n",
      "step: 239/2500 loss: 0.81\n",
      "step: 240/2500 loss: 0.79\n",
      "step: 241/2500 loss: 0.76\n",
      "step: 242/2500 loss: 0.87\n",
      "step: 243/2500 loss: 0.9\n",
      "step: 244/2500 loss: 0.8\n",
      "step: 245/2500 loss: 0.83\n",
      "step: 246/2500 loss: 0.82\n",
      "step: 247/2500 loss: 0.8\n",
      "step: 248/2500 loss: 0.85\n",
      "step: 249/2500 loss: 0.82\n",
      "eval loss: 0.82\n",
      "step: 250/2500 loss: 0.76\n",
      "step: 251/2500 loss: 0.93\n",
      "step: 252/2500 loss: 0.79\n",
      "step: 253/2500 loss: 0.78\n",
      "step: 254/2500 loss: 0.89\n",
      "step: 255/2500 loss: 0.77\n",
      "step: 256/2500 loss: 0.79\n",
      "step: 257/2500 loss: 0.87\n",
      "step: 258/2500 loss: 0.85\n",
      "step: 259/2500 loss: 0.83\n",
      "step: 260/2500 loss: 0.76\n",
      "step: 261/2500 loss: 0.92\n",
      "step: 262/2500 loss: 0.83\n",
      "step: 263/2500 loss: 0.73\n",
      "step: 264/2500 loss: 0.85\n",
      "step: 265/2500 loss: 0.89\n",
      "step: 266/2500 loss: 0.86\n",
      "step: 267/2500 loss: 0.81\n",
      "step: 268/2500 loss: 0.88\n",
      "step: 269/2500 loss: 0.77\n",
      "step: 270/2500 loss: 0.85\n",
      "step: 271/2500 loss: 0.83\n",
      "step: 272/2500 loss: 0.73\n",
      "step: 273/2500 loss: 0.84\n",
      "step: 274/2500 loss: 0.82\n",
      "step: 275/2500 loss: 0.81\n",
      "step: 276/2500 loss: 0.82\n",
      "step: 277/2500 loss: 0.87\n",
      "step: 278/2500 loss: 0.83\n",
      "step: 279/2500 loss: 0.79\n",
      "step: 280/2500 loss: 0.83\n",
      "step: 281/2500 loss: 0.9\n",
      "step: 282/2500 loss: 0.88\n",
      "step: 283/2500 loss: 0.76\n",
      "step: 284/2500 loss: 0.8\n",
      "step: 285/2500 loss: 0.75\n",
      "step: 286/2500 loss: 0.83\n",
      "step: 287/2500 loss: 0.81\n",
      "step: 288/2500 loss: 0.78\n",
      "step: 289/2500 loss: 0.74\n",
      "step: 290/2500 loss: 0.83\n",
      "step: 291/2500 loss: 0.75\n",
      "step: 292/2500 loss: 0.81\n",
      "step: 293/2500 loss: 0.77\n",
      "step: 294/2500 loss: 0.78\n",
      "step: 295/2500 loss: 0.89\n",
      "step: 296/2500 loss: 0.97\n",
      "step: 297/2500 loss: 0.86\n",
      "step: 298/2500 loss: 0.81\n",
      "step: 299/2500 loss: 0.86\n",
      "step: 300/2500 loss: 0.81\n",
      "step: 301/2500 loss: 0.76\n",
      "step: 302/2500 loss: 0.9\n",
      "step: 303/2500 loss: 0.77\n",
      "step: 304/2500 loss: 0.82\n",
      "step: 305/2500 loss: 0.78\n",
      "step: 306/2500 loss: 0.88\n",
      "step: 307/2500 loss: 0.87\n",
      "step: 308/2500 loss: 0.88\n",
      "step: 309/2500 loss: 0.82\n",
      "step: 310/2500 loss: 0.76\n",
      "step: 311/2500 loss: 0.78\n",
      "step: 312/2500 loss: 0.77\n",
      "step: 313/2500 loss: 0.83\n",
      "step: 314/2500 loss: 0.92\n",
      "step: 315/2500 loss: 0.76\n",
      "step: 316/2500 loss: 0.91\n",
      "step: 317/2500 loss: 0.78\n",
      "step: 318/2500 loss: 0.81\n",
      "step: 319/2500 loss: 0.89\n",
      "step: 320/2500 loss: 0.74\n",
      "step: 321/2500 loss: 0.78\n",
      "step: 322/2500 loss: 0.7\n",
      "step: 323/2500 loss: 0.83\n",
      "step: 324/2500 loss: 0.84\n",
      "step: 325/2500 loss: 0.77\n",
      "step: 326/2500 loss: 0.85\n",
      "step: 327/2500 loss: 0.86\n",
      "step: 328/2500 loss: 0.8\n",
      "step: 329/2500 loss: 0.77\n",
      "step: 330/2500 loss: 0.8\n",
      "step: 331/2500 loss: 0.89\n",
      "step: 332/2500 loss: 0.91\n",
      "step: 333/2500 loss: 0.71\n",
      "step: 334/2500 loss: 0.8\n",
      "step: 335/2500 loss: 0.78\n",
      "step: 336/2500 loss: 0.76\n",
      "step: 337/2500 loss: 0.84\n",
      "step: 338/2500 loss: 0.96\n",
      "step: 339/2500 loss: 0.77\n",
      "step: 340/2500 loss: 0.77\n",
      "step: 341/2500 loss: 0.73\n",
      "step: 342/2500 loss: 0.87\n",
      "step: 343/2500 loss: 0.81\n",
      "step: 344/2500 loss: 0.77\n",
      "step: 345/2500 loss: 0.81\n",
      "step: 346/2500 loss: 0.73\n",
      "step: 347/2500 loss: 0.78\n",
      "step: 348/2500 loss: 0.83\n",
      "step: 349/2500 loss: 0.84\n",
      "step: 350/2500 loss: 0.78\n",
      "step: 351/2500 loss: 0.82\n",
      "step: 352/2500 loss: 0.72\n",
      "step: 353/2500 loss: 0.81\n",
      "step: 354/2500 loss: 0.73\n",
      "step: 355/2500 loss: 0.71\n",
      "step: 356/2500 loss: 0.84\n",
      "step: 357/2500 loss: 0.82\n",
      "step: 358/2500 loss: 0.77\n",
      "step: 359/2500 loss: 0.81\n",
      "step: 360/2500 loss: 0.83\n",
      "step: 361/2500 loss: 0.82\n",
      "step: 362/2500 loss: 0.79\n",
      "step: 363/2500 loss: 0.75\n",
      "step: 364/2500 loss: 0.8\n",
      "step: 365/2500 loss: 0.86\n",
      "step: 366/2500 loss: 0.76\n",
      "step: 367/2500 loss: 0.75\n",
      "step: 368/2500 loss: 0.83\n",
      "step: 369/2500 loss: 0.76\n",
      "step: 370/2500 loss: 0.77\n",
      "step: 371/2500 loss: 0.81\n",
      "step: 372/2500 loss: 0.87\n",
      "step: 373/2500 loss: 0.74\n",
      "step: 374/2500 loss: 0.72\n",
      "step: 375/2500 loss: 0.82\n",
      "step: 376/2500 loss: 0.88\n",
      "step: 377/2500 loss: 0.74\n",
      "step: 378/2500 loss: 0.65\n",
      "step: 379/2500 loss: 0.72\n",
      "step: 380/2500 loss: 0.65\n",
      "step: 381/2500 loss: 0.84\n",
      "step: 382/2500 loss: 0.76\n",
      "step: 383/2500 loss: 0.83\n",
      "step: 384/2500 loss: 0.7\n",
      "step: 385/2500 loss: 0.76\n",
      "step: 386/2500 loss: 0.78\n",
      "step: 387/2500 loss: 0.83\n",
      "step: 388/2500 loss: 0.75\n",
      "step: 389/2500 loss: 0.76\n",
      "step: 390/2500 loss: 0.82\n",
      "step: 391/2500 loss: 0.77\n",
      "step: 392/2500 loss: 0.76\n",
      "step: 393/2500 loss: 0.78\n",
      "step: 394/2500 loss: 0.77\n",
      "step: 395/2500 loss: 0.73\n",
      "step: 396/2500 loss: 0.69\n",
      "step: 397/2500 loss: 0.79\n",
      "step: 398/2500 loss: 0.77\n",
      "step: 399/2500 loss: 0.7\n",
      "step: 400/2500 loss: 0.79\n",
      "step: 401/2500 loss: 0.89\n",
      "step: 402/2500 loss: 0.77\n",
      "step: 403/2500 loss: 0.84\n",
      "step: 404/2500 loss: 0.74\n",
      "step: 405/2500 loss: 0.84\n",
      "step: 406/2500 loss: 0.7\n",
      "step: 407/2500 loss: 0.72\n",
      "step: 408/2500 loss: 0.79\n",
      "step: 409/2500 loss: 0.87\n",
      "step: 410/2500 loss: 0.79\n",
      "step: 411/2500 loss: 0.79\n",
      "step: 412/2500 loss: 0.75\n",
      "step: 413/2500 loss: 0.74\n",
      "step: 414/2500 loss: 0.79\n",
      "step: 415/2500 loss: 0.81\n",
      "step: 416/2500 loss: 0.84\n",
      "step: 417/2500 loss: 0.74\n",
      "step: 418/2500 loss: 0.76\n",
      "step: 419/2500 loss: 0.8\n",
      "step: 420/2500 loss: 0.73\n",
      "step: 421/2500 loss: 0.77\n",
      "step: 422/2500 loss: 0.73\n",
      "step: 423/2500 loss: 0.79\n",
      "step: 424/2500 loss: 0.69\n",
      "step: 425/2500 loss: 0.75\n",
      "step: 426/2500 loss: 0.82\n",
      "step: 427/2500 loss: 0.81\n",
      "step: 428/2500 loss: 0.96\n",
      "step: 429/2500 loss: 0.85\n",
      "step: 430/2500 loss: 0.77\n",
      "step: 431/2500 loss: 0.73\n",
      "step: 432/2500 loss: 0.74\n",
      "step: 433/2500 loss: 0.88\n",
      "step: 434/2500 loss: 0.76\n",
      "step: 435/2500 loss: 0.87\n",
      "step: 436/2500 loss: 0.88\n",
      "step: 437/2500 loss: 0.75\n",
      "step: 438/2500 loss: 0.79\n",
      "step: 439/2500 loss: 0.73\n",
      "step: 440/2500 loss: 0.78\n",
      "step: 441/2500 loss: 0.83\n",
      "step: 442/2500 loss: 0.78\n",
      "step: 443/2500 loss: 0.76\n",
      "step: 444/2500 loss: 0.68\n",
      "step: 445/2500 loss: 0.75\n",
      "step: 446/2500 loss: 0.89\n",
      "step: 447/2500 loss: 0.72\n",
      "step: 448/2500 loss: 0.78\n",
      "step: 449/2500 loss: 0.82\n",
      "step: 450/2500 loss: 0.74\n",
      "step: 451/2500 loss: 0.8\n",
      "step: 452/2500 loss: 0.79\n",
      "step: 453/2500 loss: 0.88\n",
      "step: 454/2500 loss: 0.67\n",
      "step: 455/2500 loss: 0.76\n",
      "step: 456/2500 loss: 0.71\n",
      "step: 457/2500 loss: 0.79\n",
      "step: 458/2500 loss: 0.71\n",
      "step: 459/2500 loss: 0.66\n",
      "step: 460/2500 loss: 0.74\n",
      "step: 461/2500 loss: 0.62\n",
      "step: 462/2500 loss: 0.74\n",
      "step: 463/2500 loss: 0.88\n",
      "step: 464/2500 loss: 0.74\n",
      "step: 465/2500 loss: 0.79\n",
      "step: 466/2500 loss: 0.73\n",
      "step: 467/2500 loss: 0.76\n",
      "step: 468/2500 loss: 0.75\n",
      "step: 469/2500 loss: 0.85\n",
      "step: 470/2500 loss: 0.76\n",
      "step: 471/2500 loss: 0.77\n",
      "step: 472/2500 loss: 0.75\n",
      "step: 473/2500 loss: 0.83\n",
      "step: 474/2500 loss: 0.82\n",
      "step: 475/2500 loss: 0.74\n",
      "step: 476/2500 loss: 0.81\n",
      "step: 477/2500 loss: 0.64\n",
      "step: 478/2500 loss: 0.83\n",
      "step: 479/2500 loss: 0.7\n",
      "step: 480/2500 loss: 0.8\n",
      "step: 481/2500 loss: 0.75\n",
      "step: 482/2500 loss: 0.81\n",
      "step: 483/2500 loss: 0.79\n",
      "step: 484/2500 loss: 0.78\n",
      "step: 485/2500 loss: 0.77\n",
      "step: 486/2500 loss: 0.72\n",
      "step: 487/2500 loss: 0.8\n",
      "step: 488/2500 loss: 0.73\n",
      "step: 489/2500 loss: 0.78\n",
      "step: 490/2500 loss: 0.77\n",
      "step: 491/2500 loss: 0.73\n",
      "step: 492/2500 loss: 0.8\n",
      "step: 493/2500 loss: 0.74\n",
      "step: 494/2500 loss: 0.77\n",
      "step: 495/2500 loss: 0.7\n",
      "step: 496/2500 loss: 0.78\n",
      "step: 497/2500 loss: 0.77\n",
      "step: 498/2500 loss: 0.74\n",
      "step: 499/2500 loss: 0.73\n",
      "eval loss: 0.74\n",
      "step: 500/2500 loss: 0.7\n",
      "step: 501/2500 loss: 0.75\n",
      "step: 502/2500 loss: 0.71\n",
      "step: 503/2500 loss: 0.66\n",
      "step: 504/2500 loss: 0.71\n",
      "step: 505/2500 loss: 0.74\n",
      "step: 506/2500 loss: 0.77\n",
      "step: 507/2500 loss: 0.74\n",
      "step: 508/2500 loss: 0.75\n",
      "step: 509/2500 loss: 0.75\n",
      "step: 510/2500 loss: 0.7\n",
      "step: 511/2500 loss: 0.8\n",
      "step: 512/2500 loss: 0.77\n",
      "step: 513/2500 loss: 0.82\n",
      "step: 514/2500 loss: 0.71\n",
      "step: 515/2500 loss: 0.76\n",
      "step: 516/2500 loss: 0.72\n",
      "step: 517/2500 loss: 0.84\n",
      "step: 518/2500 loss: 0.75\n",
      "step: 519/2500 loss: 0.78\n",
      "step: 520/2500 loss: 0.82\n",
      "step: 521/2500 loss: 0.74\n",
      "step: 522/2500 loss: 0.81\n",
      "step: 523/2500 loss: 0.79\n",
      "step: 524/2500 loss: 0.73\n",
      "step: 525/2500 loss: 0.81\n",
      "step: 526/2500 loss: 0.75\n",
      "step: 527/2500 loss: 0.69\n",
      "step: 528/2500 loss: 0.67\n",
      "step: 529/2500 loss: 0.71\n",
      "step: 530/2500 loss: 0.78\n",
      "step: 531/2500 loss: 0.67\n",
      "step: 532/2500 loss: 0.73\n",
      "step: 533/2500 loss: 0.73\n",
      "step: 534/2500 loss: 0.72\n",
      "step: 535/2500 loss: 0.7\n",
      "step: 536/2500 loss: 0.75\n",
      "step: 537/2500 loss: 0.71\n",
      "step: 538/2500 loss: 0.65\n",
      "step: 539/2500 loss: 0.85\n",
      "step: 540/2500 loss: 0.74\n",
      "step: 541/2500 loss: 0.71\n",
      "step: 542/2500 loss: 0.75\n",
      "step: 543/2500 loss: 0.79\n",
      "step: 544/2500 loss: 0.74\n",
      "step: 545/2500 loss: 0.74\n",
      "step: 546/2500 loss: 0.79\n",
      "step: 547/2500 loss: 0.74\n",
      "step: 548/2500 loss: 0.71\n",
      "step: 549/2500 loss: 0.78\n",
      "step: 550/2500 loss: 0.77\n",
      "step: 551/2500 loss: 0.77\n",
      "step: 552/2500 loss: 0.69\n",
      "step: 553/2500 loss: 0.7\n",
      "step: 554/2500 loss: 0.7\n",
      "step: 555/2500 loss: 0.74\n",
      "step: 556/2500 loss: 0.73\n",
      "step: 557/2500 loss: 0.66\n",
      "step: 558/2500 loss: 0.67\n",
      "step: 559/2500 loss: 0.71\n",
      "step: 560/2500 loss: 0.73\n",
      "step: 561/2500 loss: 0.67\n",
      "step: 562/2500 loss: 0.7\n",
      "step: 563/2500 loss: 0.72\n",
      "step: 564/2500 loss: 0.72\n",
      "step: 565/2500 loss: 0.67\n",
      "step: 566/2500 loss: 0.83\n",
      "step: 567/2500 loss: 0.76\n",
      "step: 568/2500 loss: 0.82\n",
      "step: 569/2500 loss: 0.74\n",
      "step: 570/2500 loss: 0.62\n",
      "step: 571/2500 loss: 0.76\n",
      "step: 572/2500 loss: 0.74\n",
      "step: 573/2500 loss: 0.86\n",
      "step: 574/2500 loss: 0.75\n",
      "step: 575/2500 loss: 0.75\n",
      "step: 576/2500 loss: 0.68\n",
      "step: 577/2500 loss: 0.8\n",
      "step: 578/2500 loss: 0.75\n",
      "step: 579/2500 loss: 0.77\n",
      "step: 580/2500 loss: 0.89\n",
      "step: 581/2500 loss: 0.76\n",
      "step: 582/2500 loss: 0.8\n",
      "step: 583/2500 loss: 0.86\n",
      "step: 584/2500 loss: 0.79\n",
      "step: 585/2500 loss: 0.77\n",
      "step: 586/2500 loss: 0.83\n",
      "step: 587/2500 loss: 0.74\n",
      "step: 588/2500 loss: 0.74\n",
      "step: 589/2500 loss: 0.75\n",
      "step: 590/2500 loss: 0.94\n",
      "step: 591/2500 loss: 0.71\n",
      "step: 592/2500 loss: 0.76\n",
      "step: 593/2500 loss: 0.73\n",
      "step: 594/2500 loss: 0.71\n",
      "step: 595/2500 loss: 0.8\n",
      "step: 596/2500 loss: 0.91\n",
      "step: 597/2500 loss: 0.63\n",
      "step: 598/2500 loss: 0.67\n",
      "step: 599/2500 loss: 0.82\n",
      "step: 600/2500 loss: 0.64\n",
      "step: 601/2500 loss: 0.84\n",
      "step: 602/2500 loss: 0.73\n",
      "step: 603/2500 loss: 0.76\n",
      "step: 604/2500 loss: 0.74\n",
      "step: 605/2500 loss: 0.77\n",
      "step: 606/2500 loss: 0.73\n",
      "step: 607/2500 loss: 0.77\n",
      "step: 608/2500 loss: 0.82\n",
      "step: 609/2500 loss: 0.78\n",
      "step: 610/2500 loss: 0.79\n",
      "step: 611/2500 loss: 0.63\n",
      "step: 612/2500 loss: 0.7\n",
      "step: 613/2500 loss: 0.78\n",
      "step: 614/2500 loss: 0.77\n",
      "step: 615/2500 loss: 0.75\n",
      "step: 616/2500 loss: 0.76\n",
      "step: 617/2500 loss: 0.73\n",
      "step: 618/2500 loss: 0.69\n",
      "step: 619/2500 loss: 0.74\n",
      "step: 620/2500 loss: 0.7\n",
      "step: 621/2500 loss: 0.76\n",
      "step: 622/2500 loss: 0.76\n",
      "step: 623/2500 loss: 0.77\n",
      "step: 624/2500 loss: 0.77\n",
      "step: 625/2500 loss: 0.65\n",
      "step: 626/2500 loss: 0.83\n",
      "step: 627/2500 loss: 0.76\n",
      "step: 628/2500 loss: 0.75\n",
      "step: 629/2500 loss: 0.76\n",
      "step: 630/2500 loss: 0.77\n",
      "step: 631/2500 loss: 0.67\n",
      "step: 632/2500 loss: 0.74\n",
      "step: 633/2500 loss: 0.7\n",
      "step: 634/2500 loss: 0.7\n",
      "step: 635/2500 loss: 0.77\n",
      "step: 636/2500 loss: 0.67\n",
      "step: 637/2500 loss: 0.75\n",
      "step: 638/2500 loss: 0.78\n",
      "step: 639/2500 loss: 0.68\n",
      "step: 640/2500 loss: 0.74\n",
      "step: 641/2500 loss: 0.69\n",
      "step: 642/2500 loss: 0.7\n",
      "step: 643/2500 loss: 0.71\n",
      "step: 644/2500 loss: 0.69\n",
      "step: 645/2500 loss: 0.73\n",
      "step: 646/2500 loss: 0.67\n",
      "step: 647/2500 loss: 0.79\n",
      "step: 648/2500 loss: 0.69\n",
      "step: 649/2500 loss: 0.65\n",
      "step: 650/2500 loss: 0.8\n",
      "step: 651/2500 loss: 0.73\n",
      "step: 652/2500 loss: 0.69\n",
      "step: 653/2500 loss: 0.82\n",
      "step: 654/2500 loss: 0.83\n",
      "step: 655/2500 loss: 0.68\n",
      "step: 656/2500 loss: 0.76\n",
      "step: 657/2500 loss: 0.79\n",
      "step: 658/2500 loss: 0.71\n",
      "step: 659/2500 loss: 0.73\n",
      "step: 660/2500 loss: 0.68\n",
      "step: 661/2500 loss: 0.72\n",
      "step: 662/2500 loss: 0.76\n",
      "step: 663/2500 loss: 0.74\n",
      "step: 664/2500 loss: 0.67\n",
      "step: 665/2500 loss: 0.73\n",
      "step: 666/2500 loss: 0.72\n",
      "step: 667/2500 loss: 0.77\n",
      "step: 668/2500 loss: 0.65\n",
      "step: 669/2500 loss: 0.61\n",
      "step: 670/2500 loss: 0.7\n",
      "step: 671/2500 loss: 0.7\n",
      "step: 672/2500 loss: 0.75\n",
      "step: 673/2500 loss: 0.78\n",
      "step: 674/2500 loss: 0.71\n",
      "step: 675/2500 loss: 0.66\n",
      "step: 676/2500 loss: 0.75\n",
      "step: 677/2500 loss: 0.7\n",
      "step: 678/2500 loss: 0.68\n",
      "step: 679/2500 loss: 0.72\n",
      "step: 680/2500 loss: 0.74\n",
      "step: 681/2500 loss: 0.76\n",
      "step: 682/2500 loss: 0.68\n",
      "step: 683/2500 loss: 0.76\n",
      "step: 684/2500 loss: 0.69\n",
      "step: 685/2500 loss: 0.73\n",
      "step: 686/2500 loss: 0.77\n",
      "step: 687/2500 loss: 0.75\n",
      "step: 688/2500 loss: 0.74\n",
      "step: 689/2500 loss: 0.76\n",
      "step: 690/2500 loss: 0.76\n",
      "step: 691/2500 loss: 0.86\n",
      "step: 692/2500 loss: 0.81\n",
      "step: 693/2500 loss: 0.64\n",
      "step: 694/2500 loss: 0.73\n",
      "step: 695/2500 loss: 0.62\n",
      "step: 696/2500 loss: 0.75\n",
      "step: 697/2500 loss: 0.67\n",
      "step: 698/2500 loss: 0.74\n",
      "step: 699/2500 loss: 0.69\n",
      "step: 700/2500 loss: 0.75\n",
      "step: 701/2500 loss: 0.72\n",
      "step: 702/2500 loss: 0.84\n",
      "step: 703/2500 loss: 0.7\n",
      "step: 704/2500 loss: 0.65\n",
      "step: 705/2500 loss: 0.73\n",
      "step: 706/2500 loss: 0.77\n",
      "step: 707/2500 loss: 0.78\n",
      "step: 708/2500 loss: 0.72\n",
      "step: 709/2500 loss: 0.71\n",
      "step: 710/2500 loss: 0.65\n",
      "step: 711/2500 loss: 0.7\n",
      "step: 712/2500 loss: 0.75\n",
      "step: 713/2500 loss: 0.72\n",
      "step: 714/2500 loss: 0.65\n",
      "step: 715/2500 loss: 0.73\n",
      "step: 716/2500 loss: 0.79\n",
      "step: 717/2500 loss: 0.73\n",
      "step: 718/2500 loss: 0.75\n",
      "step: 719/2500 loss: 0.66\n",
      "step: 720/2500 loss: 0.69\n",
      "step: 721/2500 loss: 0.77\n",
      "step: 722/2500 loss: 0.77\n",
      "step: 723/2500 loss: 0.69\n",
      "step: 724/2500 loss: 0.67\n",
      "step: 725/2500 loss: 0.66\n",
      "step: 726/2500 loss: 0.73\n",
      "step: 727/2500 loss: 0.66\n",
      "step: 728/2500 loss: 0.64\n",
      "step: 729/2500 loss: 0.65\n",
      "step: 730/2500 loss: 0.78\n",
      "step: 731/2500 loss: 0.75\n",
      "step: 732/2500 loss: 0.73\n",
      "step: 733/2500 loss: 0.59\n",
      "step: 734/2500 loss: 0.87\n",
      "step: 735/2500 loss: 0.71\n",
      "step: 736/2500 loss: 0.71\n",
      "step: 737/2500 loss: 0.77\n",
      "step: 738/2500 loss: 0.7\n",
      "step: 739/2500 loss: 0.78\n",
      "step: 740/2500 loss: 0.77\n",
      "step: 741/2500 loss: 0.69\n",
      "step: 742/2500 loss: 0.84\n",
      "step: 743/2500 loss: 0.7\n",
      "step: 744/2500 loss: 0.79\n",
      "step: 745/2500 loss: 0.6\n",
      "step: 746/2500 loss: 0.72\n",
      "step: 747/2500 loss: 0.63\n",
      "step: 748/2500 loss: 0.66\n",
      "step: 749/2500 loss: 0.8\n",
      "eval loss: 0.7\n",
      "step: 750/2500 loss: 0.78\n",
      "step: 751/2500 loss: 0.66\n",
      "step: 752/2500 loss: 0.7\n",
      "step: 753/2500 loss: 0.66\n",
      "step: 754/2500 loss: 0.69\n",
      "step: 755/2500 loss: 0.77\n",
      "step: 756/2500 loss: 0.67\n",
      "step: 757/2500 loss: 0.77\n",
      "step: 758/2500 loss: 0.71\n",
      "step: 759/2500 loss: 0.69\n",
      "step: 760/2500 loss: 0.67\n",
      "step: 761/2500 loss: 0.68\n",
      "step: 762/2500 loss: 0.71\n",
      "step: 763/2500 loss: 0.71\n",
      "step: 764/2500 loss: 0.73\n",
      "step: 765/2500 loss: 0.66\n",
      "step: 766/2500 loss: 0.82\n",
      "step: 767/2500 loss: 0.63\n",
      "step: 768/2500 loss: 0.7\n",
      "step: 769/2500 loss: 0.69\n",
      "step: 770/2500 loss: 0.78\n",
      "step: 771/2500 loss: 0.66\n",
      "step: 772/2500 loss: 0.74\n",
      "step: 773/2500 loss: 0.76\n",
      "step: 774/2500 loss: 0.7\n",
      "step: 775/2500 loss: 0.66\n",
      "step: 776/2500 loss: 0.73\n",
      "step: 777/2500 loss: 0.74\n",
      "step: 778/2500 loss: 0.69\n",
      "step: 779/2500 loss: 0.76\n",
      "step: 780/2500 loss: 0.7\n",
      "step: 781/2500 loss: 0.78\n",
      "step: 782/2500 loss: 0.7\n",
      "step: 783/2500 loss: 0.81\n",
      "step: 784/2500 loss: 0.74\n",
      "step: 785/2500 loss: 0.67\n",
      "step: 786/2500 loss: 0.79\n",
      "step: 787/2500 loss: 0.68\n",
      "step: 788/2500 loss: 0.72\n",
      "step: 789/2500 loss: 0.69\n",
      "step: 790/2500 loss: 0.68\n",
      "step: 791/2500 loss: 0.69\n",
      "step: 792/2500 loss: 0.72\n",
      "step: 793/2500 loss: 0.69\n",
      "step: 794/2500 loss: 0.78\n",
      "step: 795/2500 loss: 0.66\n",
      "step: 796/2500 loss: 0.69\n",
      "step: 797/2500 loss: 0.79\n",
      "step: 798/2500 loss: 0.64\n",
      "step: 799/2500 loss: 0.73\n",
      "step: 800/2500 loss: 0.77\n",
      "step: 801/2500 loss: 0.63\n",
      "step: 802/2500 loss: 0.85\n",
      "step: 803/2500 loss: 0.64\n",
      "step: 804/2500 loss: 0.8\n",
      "step: 805/2500 loss: 0.67\n",
      "step: 806/2500 loss: 0.64\n",
      "step: 807/2500 loss: 0.71\n",
      "step: 808/2500 loss: 0.68\n",
      "step: 809/2500 loss: 0.66\n",
      "step: 810/2500 loss: 0.79\n",
      "step: 811/2500 loss: 0.69\n",
      "step: 812/2500 loss: 0.72\n",
      "step: 813/2500 loss: 0.76\n",
      "step: 814/2500 loss: 0.79\n",
      "step: 815/2500 loss: 0.62\n",
      "step: 816/2500 loss: 0.69\n",
      "step: 817/2500 loss: 0.66\n",
      "step: 818/2500 loss: 0.75\n",
      "step: 819/2500 loss: 0.69\n",
      "step: 820/2500 loss: 0.78\n",
      "step: 821/2500 loss: 0.72\n",
      "step: 822/2500 loss: 0.71\n",
      "step: 823/2500 loss: 0.76\n",
      "step: 824/2500 loss: 0.74\n",
      "step: 825/2500 loss: 0.76\n",
      "step: 826/2500 loss: 0.76\n",
      "step: 827/2500 loss: 0.76\n",
      "step: 828/2500 loss: 0.77\n",
      "step: 829/2500 loss: 0.72\n",
      "step: 830/2500 loss: 0.79\n",
      "step: 831/2500 loss: 0.79\n",
      "step: 832/2500 loss: 0.69\n",
      "step: 833/2500 loss: 0.82\n",
      "step: 834/2500 loss: 0.7\n",
      "step: 835/2500 loss: 0.72\n",
      "step: 836/2500 loss: 0.73\n",
      "step: 837/2500 loss: 0.72\n",
      "step: 838/2500 loss: 0.74\n",
      "step: 839/2500 loss: 0.63\n",
      "step: 840/2500 loss: 0.73\n",
      "step: 841/2500 loss: 0.83\n",
      "step: 842/2500 loss: 0.69\n",
      "step: 843/2500 loss: 0.71\n",
      "step: 844/2500 loss: 0.72\n",
      "step: 845/2500 loss: 0.74\n",
      "step: 846/2500 loss: 0.71\n",
      "step: 847/2500 loss: 0.69\n",
      "step: 848/2500 loss: 0.76\n",
      "step: 849/2500 loss: 0.65\n",
      "step: 850/2500 loss: 0.77\n",
      "step: 851/2500 loss: 0.82\n",
      "step: 852/2500 loss: 0.72\n",
      "step: 853/2500 loss: 0.79\n",
      "step: 854/2500 loss: 0.7\n",
      "step: 855/2500 loss: 0.72\n",
      "step: 856/2500 loss: 0.66\n",
      "step: 857/2500 loss: 0.81\n",
      "step: 858/2500 loss: 0.74\n",
      "step: 859/2500 loss: 0.75\n",
      "step: 860/2500 loss: 0.72\n",
      "step: 861/2500 loss: 0.65\n",
      "step: 862/2500 loss: 0.63\n",
      "step: 863/2500 loss: 0.69\n",
      "step: 864/2500 loss: 0.68\n",
      "step: 865/2500 loss: 0.83\n",
      "step: 866/2500 loss: 0.84\n",
      "step: 867/2500 loss: 0.74\n",
      "step: 868/2500 loss: 0.68\n",
      "step: 869/2500 loss: 0.66\n",
      "step: 870/2500 loss: 0.69\n",
      "step: 871/2500 loss: 0.75\n",
      "step: 872/2500 loss: 0.69\n",
      "step: 873/2500 loss: 0.71\n",
      "step: 874/2500 loss: 0.61\n",
      "step: 875/2500 loss: 0.91\n",
      "step: 876/2500 loss: 0.68\n",
      "step: 877/2500 loss: 0.75\n",
      "step: 878/2500 loss: 0.71\n",
      "step: 879/2500 loss: 0.65\n",
      "step: 880/2500 loss: 0.78\n",
      "step: 881/2500 loss: 0.69\n",
      "step: 882/2500 loss: 0.7\n",
      "step: 883/2500 loss: 0.72\n",
      "step: 884/2500 loss: 0.77\n",
      "step: 885/2500 loss: 0.65\n",
      "step: 886/2500 loss: 0.73\n",
      "step: 887/2500 loss: 0.65\n",
      "step: 888/2500 loss: 0.83\n",
      "step: 889/2500 loss: 0.7\n",
      "step: 890/2500 loss: 0.74\n",
      "step: 891/2500 loss: 0.6\n",
      "step: 892/2500 loss: 0.68\n",
      "step: 893/2500 loss: 0.66\n",
      "step: 894/2500 loss: 0.79\n",
      "step: 895/2500 loss: 0.66\n",
      "step: 896/2500 loss: 0.58\n",
      "step: 897/2500 loss: 0.68\n",
      "step: 898/2500 loss: 0.68\n",
      "step: 899/2500 loss: 0.85\n",
      "step: 900/2500 loss: 0.73\n",
      "step: 901/2500 loss: 0.76\n",
      "step: 902/2500 loss: 0.79\n",
      "step: 903/2500 loss: 0.64\n",
      "step: 904/2500 loss: 0.71\n",
      "step: 905/2500 loss: 0.79\n",
      "step: 906/2500 loss: 0.68\n",
      "step: 907/2500 loss: 0.71\n",
      "step: 908/2500 loss: 0.66\n",
      "step: 909/2500 loss: 0.77\n",
      "step: 910/2500 loss: 0.74\n",
      "step: 911/2500 loss: 0.76\n",
      "step: 912/2500 loss: 0.78\n",
      "step: 913/2500 loss: 0.69\n",
      "step: 914/2500 loss: 0.76\n",
      "step: 915/2500 loss: 0.78\n",
      "step: 916/2500 loss: 0.63\n",
      "step: 917/2500 loss: 0.66\n",
      "step: 918/2500 loss: 0.72\n",
      "step: 919/2500 loss: 0.81\n",
      "step: 920/2500 loss: 0.73\n",
      "step: 921/2500 loss: 0.7\n",
      "step: 922/2500 loss: 0.72\n",
      "step: 923/2500 loss: 0.69\n",
      "step: 924/2500 loss: 0.72\n",
      "step: 925/2500 loss: 0.72\n",
      "step: 926/2500 loss: 0.75\n",
      "step: 927/2500 loss: 0.68\n",
      "step: 928/2500 loss: 0.76\n",
      "step: 929/2500 loss: 0.85\n",
      "step: 930/2500 loss: 0.69\n",
      "step: 931/2500 loss: 0.85\n",
      "step: 932/2500 loss: 0.65\n",
      "step: 933/2500 loss: 0.66\n",
      "step: 934/2500 loss: 0.69\n",
      "step: 935/2500 loss: 0.68\n",
      "step: 936/2500 loss: 0.67\n",
      "step: 937/2500 loss: 0.66\n",
      "step: 938/2500 loss: 0.77\n",
      "step: 939/2500 loss: 0.7\n",
      "step: 940/2500 loss: 0.7\n",
      "step: 941/2500 loss: 0.69\n",
      "step: 942/2500 loss: 0.68\n",
      "step: 943/2500 loss: 0.76\n",
      "step: 944/2500 loss: 0.83\n",
      "step: 945/2500 loss: 0.71\n",
      "step: 946/2500 loss: 0.74\n",
      "step: 947/2500 loss: 0.64\n",
      "step: 948/2500 loss: 0.61\n",
      "step: 949/2500 loss: 0.75\n",
      "step: 950/2500 loss: 0.67\n",
      "step: 951/2500 loss: 0.72\n",
      "step: 952/2500 loss: 0.66\n",
      "step: 953/2500 loss: 0.77\n",
      "step: 954/2500 loss: 0.69\n",
      "step: 955/2500 loss: 0.7\n",
      "step: 956/2500 loss: 0.7\n",
      "step: 957/2500 loss: 0.62\n",
      "step: 958/2500 loss: 0.65\n",
      "step: 959/2500 loss: 0.64\n",
      "step: 960/2500 loss: 0.77\n",
      "step: 961/2500 loss: 0.73\n",
      "step: 962/2500 loss: 0.68\n",
      "step: 963/2500 loss: 0.8\n",
      "step: 964/2500 loss: 0.7\n",
      "step: 965/2500 loss: 0.75\n",
      "step: 966/2500 loss: 0.77\n",
      "step: 967/2500 loss: 0.7\n",
      "step: 968/2500 loss: 0.68\n",
      "step: 969/2500 loss: 0.68\n",
      "step: 970/2500 loss: 0.71\n",
      "step: 971/2500 loss: 0.78\n",
      "step: 972/2500 loss: 0.72\n",
      "step: 973/2500 loss: 0.62\n",
      "step: 974/2500 loss: 0.67\n",
      "step: 975/2500 loss: 0.65\n",
      "step: 976/2500 loss: 0.71\n",
      "step: 977/2500 loss: 0.7\n",
      "step: 978/2500 loss: 0.74\n",
      "step: 979/2500 loss: 0.65\n",
      "step: 980/2500 loss: 0.77\n",
      "step: 981/2500 loss: 0.77\n",
      "step: 982/2500 loss: 0.77\n",
      "step: 983/2500 loss: 0.65\n",
      "step: 984/2500 loss: 0.81\n",
      "step: 985/2500 loss: 0.76\n",
      "step: 986/2500 loss: 0.66\n",
      "step: 987/2500 loss: 0.82\n",
      "step: 988/2500 loss: 0.71\n",
      "step: 989/2500 loss: 0.69\n",
      "step: 990/2500 loss: 0.71\n",
      "step: 991/2500 loss: 0.71\n",
      "step: 992/2500 loss: 0.63\n",
      "step: 993/2500 loss: 0.65\n",
      "step: 994/2500 loss: 0.67\n",
      "step: 995/2500 loss: 0.7\n",
      "step: 996/2500 loss: 0.69\n",
      "step: 997/2500 loss: 0.73\n",
      "step: 998/2500 loss: 0.59\n",
      "step: 999/2500 loss: 0.77\n",
      "eval loss: 0.68\n",
      "step: 1000/2500 loss: 0.71\n",
      "step: 1001/2500 loss: 0.6\n",
      "step: 1002/2500 loss: 0.76\n",
      "step: 1003/2500 loss: 0.72\n",
      "step: 1004/2500 loss: 0.72\n",
      "step: 1005/2500 loss: 0.78\n",
      "step: 1006/2500 loss: 0.75\n",
      "step: 1007/2500 loss: 0.66\n",
      "step: 1008/2500 loss: 0.67\n",
      "step: 1009/2500 loss: 0.62\n",
      "step: 1010/2500 loss: 0.65\n",
      "step: 1011/2500 loss: 0.66\n",
      "step: 1012/2500 loss: 0.73\n",
      "step: 1013/2500 loss: 0.74\n",
      "step: 1014/2500 loss: 0.77\n",
      "step: 1015/2500 loss: 0.82\n",
      "step: 1016/2500 loss: 0.67\n",
      "step: 1017/2500 loss: 0.67\n",
      "step: 1018/2500 loss: 0.8\n",
      "step: 1019/2500 loss: 0.7\n",
      "step: 1020/2500 loss: 0.7\n",
      "step: 1021/2500 loss: 0.71\n",
      "step: 1022/2500 loss: 0.61\n",
      "step: 1023/2500 loss: 0.83\n",
      "step: 1024/2500 loss: 0.68\n",
      "step: 1025/2500 loss: 0.78\n",
      "step: 1026/2500 loss: 0.68\n",
      "step: 1027/2500 loss: 0.65\n",
      "step: 1028/2500 loss: 0.76\n",
      "step: 1029/2500 loss: 0.69\n",
      "step: 1030/2500 loss: 0.63\n",
      "step: 1031/2500 loss: 0.67\n",
      "step: 1032/2500 loss: 0.75\n",
      "step: 1033/2500 loss: 0.77\n",
      "step: 1034/2500 loss: 0.64\n",
      "step: 1035/2500 loss: 0.65\n",
      "step: 1036/2500 loss: 0.72\n",
      "step: 1037/2500 loss: 0.71\n",
      "step: 1038/2500 loss: 0.62\n",
      "step: 1039/2500 loss: 0.72\n",
      "step: 1040/2500 loss: 0.64\n",
      "step: 1041/2500 loss: 0.58\n",
      "step: 1042/2500 loss: 0.82\n",
      "step: 1043/2500 loss: 0.69\n",
      "step: 1044/2500 loss: 0.68\n",
      "step: 1045/2500 loss: 0.74\n",
      "step: 1046/2500 loss: 0.67\n",
      "step: 1047/2500 loss: 0.62\n",
      "step: 1048/2500 loss: 0.63\n",
      "step: 1049/2500 loss: 0.7\n",
      "step: 1050/2500 loss: 0.7\n",
      "step: 1051/2500 loss: 0.66\n",
      "step: 1052/2500 loss: 0.75\n",
      "step: 1053/2500 loss: 0.69\n",
      "step: 1054/2500 loss: 0.62\n",
      "step: 1055/2500 loss: 0.54\n",
      "step: 1056/2500 loss: 0.77\n",
      "step: 1057/2500 loss: 0.7\n",
      "step: 1058/2500 loss: 0.73\n",
      "step: 1059/2500 loss: 0.79\n",
      "step: 1060/2500 loss: 0.71\n",
      "step: 1061/2500 loss: 0.77\n",
      "step: 1062/2500 loss: 0.78\n",
      "step: 1063/2500 loss: 0.69\n",
      "step: 1064/2500 loss: 0.63\n",
      "step: 1065/2500 loss: 0.8\n",
      "step: 1066/2500 loss: 0.65\n",
      "step: 1067/2500 loss: 0.7\n",
      "step: 1068/2500 loss: 0.79\n",
      "step: 1069/2500 loss: 0.68\n",
      "step: 1070/2500 loss: 0.73\n",
      "step: 1071/2500 loss: 0.69\n",
      "step: 1072/2500 loss: 0.79\n",
      "step: 1073/2500 loss: 0.66\n",
      "step: 1074/2500 loss: 0.64\n",
      "step: 1075/2500 loss: 0.7\n",
      "step: 1076/2500 loss: 0.8\n",
      "step: 1077/2500 loss: 0.65\n",
      "step: 1078/2500 loss: 0.81\n",
      "step: 1079/2500 loss: 0.66\n",
      "step: 1080/2500 loss: 0.62\n",
      "step: 1081/2500 loss: 0.75\n",
      "step: 1082/2500 loss: 0.66\n",
      "step: 1083/2500 loss: 0.69\n",
      "step: 1084/2500 loss: 0.66\n",
      "step: 1085/2500 loss: 0.63\n",
      "step: 1086/2500 loss: 0.7\n",
      "step: 1087/2500 loss: 0.59\n",
      "step: 1088/2500 loss: 0.66\n",
      "step: 1089/2500 loss: 0.67\n",
      "step: 1090/2500 loss: 0.69\n",
      "step: 1091/2500 loss: 0.67\n",
      "step: 1092/2500 loss: 0.67\n",
      "step: 1093/2500 loss: 0.68\n",
      "step: 1094/2500 loss: 0.92\n",
      "step: 1095/2500 loss: 0.78\n",
      "step: 1096/2500 loss: 0.69\n",
      "step: 1097/2500 loss: 0.68\n",
      "step: 1098/2500 loss: 0.67\n",
      "step: 1099/2500 loss: 0.72\n",
      "step: 1100/2500 loss: 0.67\n",
      "step: 1101/2500 loss: 0.74\n",
      "step: 1102/2500 loss: 0.7\n",
      "step: 1103/2500 loss: 0.64\n",
      "step: 1104/2500 loss: 0.75\n",
      "step: 1105/2500 loss: 0.72\n",
      "step: 1106/2500 loss: 0.72\n",
      "step: 1107/2500 loss: 0.74\n",
      "step: 1108/2500 loss: 0.63\n",
      "step: 1109/2500 loss: 0.73\n",
      "step: 1110/2500 loss: 0.76\n",
      "step: 1111/2500 loss: 0.59\n",
      "step: 1112/2500 loss: 0.71\n",
      "step: 1113/2500 loss: 0.68\n",
      "step: 1114/2500 loss: 0.74\n",
      "step: 1115/2500 loss: 0.65\n",
      "step: 1116/2500 loss: 0.74\n",
      "step: 1117/2500 loss: 0.66\n",
      "step: 1118/2500 loss: 0.62\n",
      "step: 1119/2500 loss: 0.63\n",
      "step: 1120/2500 loss: 0.66\n",
      "step: 1121/2500 loss: 0.6\n",
      "step: 1122/2500 loss: 0.6\n",
      "step: 1123/2500 loss: 0.66\n",
      "step: 1124/2500 loss: 0.65\n",
      "step: 1125/2500 loss: 0.71\n",
      "step: 1126/2500 loss: 0.56\n",
      "step: 1127/2500 loss: 0.68\n",
      "step: 1128/2500 loss: 0.72\n",
      "step: 1129/2500 loss: 0.62\n",
      "step: 1130/2500 loss: 0.7\n",
      "step: 1131/2500 loss: 0.67\n",
      "step: 1132/2500 loss: 0.69\n",
      "step: 1133/2500 loss: 0.67\n",
      "step: 1134/2500 loss: 0.75\n",
      "step: 1135/2500 loss: 0.67\n",
      "step: 1136/2500 loss: 0.7\n",
      "step: 1137/2500 loss: 0.66\n",
      "step: 1138/2500 loss: 0.68\n",
      "step: 1139/2500 loss: 0.71\n",
      "step: 1140/2500 loss: 0.69\n",
      "step: 1141/2500 loss: 0.75\n",
      "step: 1142/2500 loss: 0.63\n",
      "step: 1143/2500 loss: 0.61\n",
      "step: 1144/2500 loss: 0.59\n",
      "step: 1145/2500 loss: 0.64\n",
      "step: 1146/2500 loss: 0.74\n",
      "step: 1147/2500 loss: 0.61\n",
      "step: 1148/2500 loss: 0.65\n",
      "step: 1149/2500 loss: 0.64\n",
      "step: 1150/2500 loss: 0.75\n",
      "step: 1151/2500 loss: 0.65\n",
      "step: 1152/2500 loss: 0.74\n",
      "step: 1153/2500 loss: 0.68\n",
      "step: 1154/2500 loss: 0.62\n",
      "step: 1155/2500 loss: 0.74\n",
      "step: 1156/2500 loss: 0.67\n",
      "step: 1157/2500 loss: 0.71\n",
      "step: 1158/2500 loss: 0.9\n",
      "step: 1159/2500 loss: 0.77\n",
      "step: 1160/2500 loss: 0.82\n",
      "step: 1161/2500 loss: 0.76\n",
      "step: 1162/2500 loss: 0.67\n",
      "step: 1163/2500 loss: 0.81\n",
      "step: 1164/2500 loss: 0.71\n",
      "step: 1165/2500 loss: 0.57\n",
      "step: 1166/2500 loss: 0.73\n",
      "step: 1167/2500 loss: 0.81\n",
      "step: 1168/2500 loss: 0.69\n",
      "step: 1169/2500 loss: 0.86\n",
      "step: 1170/2500 loss: 0.58\n",
      "step: 1171/2500 loss: 0.73\n",
      "step: 1172/2500 loss: 0.69\n",
      "step: 1173/2500 loss: 0.76\n",
      "step: 1174/2500 loss: 0.57\n",
      "step: 1175/2500 loss: 0.71\n",
      "step: 1176/2500 loss: 0.66\n",
      "step: 1177/2500 loss: 0.67\n",
      "step: 1178/2500 loss: 0.64\n",
      "step: 1179/2500 loss: 0.6\n",
      "step: 1180/2500 loss: 0.79\n",
      "step: 1181/2500 loss: 0.66\n",
      "step: 1182/2500 loss: 0.62\n",
      "step: 1183/2500 loss: 0.69\n",
      "step: 1184/2500 loss: 0.7\n",
      "step: 1185/2500 loss: 0.76\n",
      "step: 1186/2500 loss: 0.8\n",
      "step: 1187/2500 loss: 0.69\n",
      "step: 1188/2500 loss: 0.69\n",
      "step: 1189/2500 loss: 0.78\n",
      "step: 1190/2500 loss: 0.64\n",
      "step: 1191/2500 loss: 0.72\n",
      "step: 1192/2500 loss: 0.68\n",
      "step: 1193/2500 loss: 0.69\n",
      "step: 1194/2500 loss: 0.8\n",
      "step: 1195/2500 loss: 0.66\n",
      "step: 1196/2500 loss: 0.67\n",
      "step: 1197/2500 loss: 0.71\n",
      "step: 1198/2500 loss: 0.66\n",
      "step: 1199/2500 loss: 0.81\n",
      "step: 1200/2500 loss: 0.57\n",
      "step: 1201/2500 loss: 0.79\n",
      "step: 1202/2500 loss: 0.71\n",
      "step: 1203/2500 loss: 0.85\n",
      "step: 1204/2500 loss: 0.63\n",
      "step: 1205/2500 loss: 0.68\n",
      "step: 1206/2500 loss: 0.68\n",
      "step: 1207/2500 loss: 0.73\n",
      "step: 1208/2500 loss: 0.71\n",
      "step: 1209/2500 loss: 0.65\n",
      "step: 1210/2500 loss: 0.74\n",
      "step: 1211/2500 loss: 0.72\n",
      "step: 1212/2500 loss: 0.64\n",
      "step: 1213/2500 loss: 0.68\n",
      "step: 1214/2500 loss: 0.76\n",
      "step: 1215/2500 loss: 0.65\n",
      "step: 1216/2500 loss: 0.67\n",
      "step: 1217/2500 loss: 0.71\n",
      "step: 1218/2500 loss: 0.63\n",
      "step: 1219/2500 loss: 0.72\n",
      "step: 1220/2500 loss: 0.64\n",
      "step: 1221/2500 loss: 0.73\n",
      "step: 1222/2500 loss: 0.75\n",
      "step: 1223/2500 loss: 0.67\n",
      "step: 1224/2500 loss: 0.77\n",
      "step: 1225/2500 loss: 0.63\n",
      "step: 1226/2500 loss: 0.75\n",
      "step: 1227/2500 loss: 0.68\n",
      "step: 1228/2500 loss: 0.7\n",
      "step: 1229/2500 loss: 0.74\n",
      "step: 1230/2500 loss: 0.73\n",
      "step: 1231/2500 loss: 0.73\n",
      "step: 1232/2500 loss: 0.62\n",
      "step: 1233/2500 loss: 0.71\n",
      "step: 1234/2500 loss: 0.77\n",
      "step: 1235/2500 loss: 0.66\n",
      "step: 1236/2500 loss: 0.68\n",
      "step: 1237/2500 loss: 0.62\n",
      "step: 1238/2500 loss: 0.66\n",
      "step: 1239/2500 loss: 0.73\n",
      "step: 1240/2500 loss: 0.77\n",
      "step: 1241/2500 loss: 0.65\n",
      "step: 1242/2500 loss: 0.67\n",
      "step: 1243/2500 loss: 0.6\n",
      "step: 1244/2500 loss: 0.67\n",
      "step: 1245/2500 loss: 0.58\n",
      "step: 1246/2500 loss: 0.83\n",
      "step: 1247/2500 loss: 0.64\n",
      "step: 1248/2500 loss: 0.66\n",
      "step: 1249/2500 loss: 0.58\n",
      "eval loss: 0.67\n",
      "step: 1250/2500 loss: 0.78\n",
      "step: 1251/2500 loss: 0.59\n",
      "step: 1252/2500 loss: 0.69\n",
      "step: 1253/2500 loss: 0.67\n",
      "step: 1254/2500 loss: 0.7\n",
      "step: 1255/2500 loss: 0.62\n",
      "step: 1256/2500 loss: 0.79\n",
      "step: 1257/2500 loss: 0.76\n",
      "step: 1258/2500 loss: 0.68\n",
      "step: 1259/2500 loss: 0.6\n",
      "step: 1260/2500 loss: 0.7\n",
      "step: 1261/2500 loss: 0.66\n",
      "step: 1262/2500 loss: 0.71\n",
      "step: 1263/2500 loss: 0.63\n",
      "step: 1264/2500 loss: 0.69\n",
      "step: 1265/2500 loss: 0.59\n",
      "step: 1266/2500 loss: 0.72\n",
      "step: 1267/2500 loss: 0.65\n",
      "step: 1268/2500 loss: 0.72\n",
      "step: 1269/2500 loss: 0.64\n",
      "step: 1270/2500 loss: 0.66\n",
      "step: 1271/2500 loss: 0.51\n",
      "step: 1272/2500 loss: 0.87\n",
      "step: 1273/2500 loss: 0.73\n",
      "step: 1274/2500 loss: 0.7\n",
      "step: 1275/2500 loss: 0.74\n",
      "step: 1276/2500 loss: 0.67\n",
      "step: 1277/2500 loss: 0.7\n",
      "step: 1278/2500 loss: 0.79\n",
      "step: 1279/2500 loss: 0.68\n",
      "step: 1280/2500 loss: 0.64\n",
      "step: 1281/2500 loss: 0.65\n",
      "step: 1282/2500 loss: 0.66\n",
      "step: 1283/2500 loss: 0.79\n",
      "step: 1284/2500 loss: 0.74\n",
      "step: 1285/2500 loss: 0.72\n",
      "step: 1286/2500 loss: 0.73\n",
      "step: 1287/2500 loss: 0.56\n",
      "step: 1288/2500 loss: 0.68\n",
      "step: 1289/2500 loss: 0.71\n",
      "step: 1290/2500 loss: 0.73\n",
      "step: 1291/2500 loss: 0.78\n",
      "step: 1292/2500 loss: 0.69\n",
      "step: 1293/2500 loss: 0.63\n",
      "step: 1294/2500 loss: 0.67\n",
      "step: 1295/2500 loss: 0.69\n",
      "step: 1296/2500 loss: 0.73\n",
      "step: 1297/2500 loss: 0.65\n",
      "step: 1298/2500 loss: 0.69\n",
      "step: 1299/2500 loss: 0.65\n",
      "step: 1300/2500 loss: 0.62\n",
      "step: 1301/2500 loss: 0.69\n",
      "step: 1302/2500 loss: 0.64\n",
      "step: 1303/2500 loss: 0.6\n",
      "step: 1304/2500 loss: 0.63\n",
      "step: 1305/2500 loss: 0.62\n",
      "step: 1306/2500 loss: 0.81\n",
      "step: 1307/2500 loss: 0.71\n",
      "step: 1308/2500 loss: 0.6\n",
      "step: 1309/2500 loss: 0.74\n",
      "step: 1310/2500 loss: 0.55\n",
      "step: 1311/2500 loss: 0.78\n",
      "step: 1312/2500 loss: 0.66\n",
      "step: 1313/2500 loss: 0.72\n",
      "step: 1314/2500 loss: 0.71\n",
      "step: 1315/2500 loss: 0.78\n",
      "step: 1316/2500 loss: 0.61\n",
      "step: 1317/2500 loss: 0.68\n",
      "step: 1318/2500 loss: 0.71\n",
      "step: 1319/2500 loss: 0.55\n",
      "step: 1320/2500 loss: 0.67\n",
      "step: 1321/2500 loss: 0.73\n",
      "step: 1322/2500 loss: 0.67\n",
      "step: 1323/2500 loss: 0.68\n",
      "step: 1324/2500 loss: 0.65\n",
      "step: 1325/2500 loss: 0.66\n",
      "step: 1326/2500 loss: 0.6\n",
      "step: 1327/2500 loss: 0.6\n",
      "step: 1328/2500 loss: 0.7\n",
      "step: 1329/2500 loss: 0.79\n",
      "step: 1330/2500 loss: 0.65\n",
      "step: 1331/2500 loss: 0.69\n",
      "step: 1332/2500 loss: 0.66\n",
      "step: 1333/2500 loss: 0.74\n",
      "step: 1334/2500 loss: 0.65\n",
      "step: 1335/2500 loss: 0.68\n",
      "step: 1336/2500 loss: 0.67\n",
      "step: 1337/2500 loss: 0.67\n",
      "step: 1338/2500 loss: 0.62\n",
      "step: 1339/2500 loss: 0.81\n",
      "step: 1340/2500 loss: 0.77\n",
      "step: 1341/2500 loss: 0.67\n",
      "step: 1342/2500 loss: 0.65\n",
      "step: 1343/2500 loss: 0.7\n",
      "step: 1344/2500 loss: 0.65\n",
      "step: 1345/2500 loss: 0.75\n",
      "step: 1346/2500 loss: 0.66\n",
      "step: 1347/2500 loss: 0.69\n",
      "step: 1348/2500 loss: 0.74\n",
      "step: 1349/2500 loss: 0.66\n",
      "step: 1350/2500 loss: 0.8\n",
      "step: 1351/2500 loss: 0.74\n",
      "step: 1352/2500 loss: 0.62\n",
      "step: 1353/2500 loss: 0.79\n",
      "step: 1354/2500 loss: 0.66\n",
      "step: 1355/2500 loss: 0.65\n",
      "step: 1356/2500 loss: 0.76\n",
      "step: 1357/2500 loss: 0.82\n",
      "step: 1358/2500 loss: 0.66\n",
      "step: 1359/2500 loss: 0.61\n",
      "step: 1360/2500 loss: 0.77\n",
      "step: 1361/2500 loss: 0.69\n",
      "step: 1362/2500 loss: 0.63\n",
      "step: 1363/2500 loss: 0.66\n",
      "step: 1364/2500 loss: 0.59\n",
      "step: 1365/2500 loss: 0.78\n",
      "step: 1366/2500 loss: 0.73\n",
      "step: 1367/2500 loss: 0.63\n",
      "step: 1368/2500 loss: 0.8\n",
      "step: 1369/2500 loss: 0.69\n",
      "step: 1370/2500 loss: 0.68\n",
      "step: 1371/2500 loss: 0.72\n",
      "step: 1372/2500 loss: 0.75\n",
      "step: 1373/2500 loss: 0.67\n",
      "step: 1374/2500 loss: 0.66\n",
      "step: 1375/2500 loss: 0.72\n",
      "step: 1376/2500 loss: 0.8\n",
      "step: 1377/2500 loss: 0.61\n",
      "step: 1378/2500 loss: 0.71\n",
      "step: 1379/2500 loss: 0.64\n",
      "step: 1380/2500 loss: 0.67\n",
      "step: 1381/2500 loss: 0.66\n",
      "step: 1382/2500 loss: 0.63\n",
      "step: 1383/2500 loss: 0.71\n",
      "step: 1384/2500 loss: 0.64\n",
      "step: 1385/2500 loss: 0.63\n",
      "step: 1386/2500 loss: 0.84\n",
      "step: 1387/2500 loss: 0.68\n",
      "step: 1388/2500 loss: 0.65\n",
      "step: 1389/2500 loss: 0.68\n",
      "step: 1390/2500 loss: 0.67\n",
      "step: 1391/2500 loss: 0.62\n",
      "step: 1392/2500 loss: 0.71\n",
      "step: 1393/2500 loss: 0.89\n",
      "step: 1394/2500 loss: 0.88\n",
      "step: 1395/2500 loss: 0.66\n",
      "step: 1396/2500 loss: 0.82\n",
      "step: 1397/2500 loss: 0.66\n",
      "step: 1398/2500 loss: 0.59\n",
      "step: 1399/2500 loss: 0.66\n",
      "step: 1400/2500 loss: 0.79\n",
      "step: 1401/2500 loss: 0.76\n",
      "step: 1402/2500 loss: 0.72\n",
      "step: 1403/2500 loss: 0.61\n",
      "step: 1404/2500 loss: 0.61\n",
      "step: 1405/2500 loss: 0.66\n",
      "step: 1406/2500 loss: 0.69\n",
      "step: 1407/2500 loss: 0.74\n",
      "step: 1408/2500 loss: 0.67\n",
      "step: 1409/2500 loss: 0.63\n",
      "step: 1410/2500 loss: 0.74\n",
      "step: 1411/2500 loss: 0.76\n",
      "step: 1412/2500 loss: 0.67\n",
      "step: 1413/2500 loss: 0.77\n",
      "step: 1414/2500 loss: 0.7\n",
      "step: 1415/2500 loss: 0.66\n",
      "step: 1416/2500 loss: 0.69\n",
      "step: 1417/2500 loss: 0.71\n",
      "step: 1418/2500 loss: 0.68\n",
      "step: 1419/2500 loss: 0.72\n",
      "step: 1420/2500 loss: 0.66\n",
      "step: 1421/2500 loss: 0.7\n",
      "step: 1422/2500 loss: 0.75\n",
      "step: 1423/2500 loss: 0.73\n",
      "step: 1424/2500 loss: 0.64\n",
      "step: 1425/2500 loss: 0.67\n",
      "step: 1426/2500 loss: 0.64\n",
      "step: 1427/2500 loss: 0.7\n",
      "step: 1428/2500 loss: 0.65\n",
      "step: 1429/2500 loss: 0.68\n",
      "step: 1430/2500 loss: 0.73\n",
      "step: 1431/2500 loss: 0.67\n",
      "step: 1432/2500 loss: 0.61\n",
      "step: 1433/2500 loss: 0.69\n",
      "step: 1434/2500 loss: 0.77\n",
      "step: 1435/2500 loss: 0.84\n",
      "step: 1436/2500 loss: 0.85\n",
      "step: 1437/2500 loss: 0.61\n",
      "step: 1438/2500 loss: 0.62\n",
      "step: 1439/2500 loss: 0.62\n",
      "step: 1440/2500 loss: 0.66\n",
      "step: 1441/2500 loss: 0.69\n",
      "step: 1442/2500 loss: 0.66\n",
      "step: 1443/2500 loss: 0.78\n",
      "step: 1444/2500 loss: 0.73\n",
      "step: 1445/2500 loss: 0.7\n",
      "step: 1446/2500 loss: 0.67\n",
      "step: 1447/2500 loss: 0.63\n",
      "step: 1448/2500 loss: 0.72\n",
      "step: 1449/2500 loss: 0.73\n",
      "step: 1450/2500 loss: 0.82\n",
      "step: 1451/2500 loss: 0.72\n",
      "step: 1452/2500 loss: 0.67\n",
      "step: 1453/2500 loss: 0.7\n",
      "step: 1454/2500 loss: 0.75\n",
      "step: 1455/2500 loss: 0.73\n",
      "step: 1456/2500 loss: 0.71\n",
      "step: 1457/2500 loss: 0.62\n",
      "step: 1458/2500 loss: 0.73\n",
      "step: 1459/2500 loss: 0.6\n",
      "step: 1460/2500 loss: 0.64\n",
      "step: 1461/2500 loss: 0.65\n",
      "step: 1462/2500 loss: 0.77\n",
      "step: 1463/2500 loss: 0.75\n",
      "step: 1464/2500 loss: 0.71\n",
      "step: 1465/2500 loss: 0.58\n",
      "step: 1466/2500 loss: 0.69\n",
      "step: 1467/2500 loss: 0.73\n",
      "step: 1468/2500 loss: 0.59\n",
      "step: 1469/2500 loss: 0.68\n",
      "step: 1470/2500 loss: 0.61\n",
      "step: 1471/2500 loss: 0.75\n",
      "step: 1472/2500 loss: 0.85\n",
      "step: 1473/2500 loss: 0.63\n",
      "step: 1474/2500 loss: 0.58\n",
      "step: 1475/2500 loss: 0.76\n",
      "step: 1476/2500 loss: 0.69\n",
      "step: 1477/2500 loss: 0.74\n",
      "step: 1478/2500 loss: 0.69\n",
      "step: 1479/2500 loss: 0.56\n",
      "step: 1480/2500 loss: 0.67\n",
      "step: 1481/2500 loss: 0.57\n",
      "step: 1482/2500 loss: 0.66\n",
      "step: 1483/2500 loss: 0.62\n",
      "step: 1484/2500 loss: 0.67\n",
      "step: 1485/2500 loss: 0.66\n",
      "step: 1486/2500 loss: 0.64\n",
      "step: 1487/2500 loss: 0.73\n",
      "step: 1488/2500 loss: 0.68\n",
      "step: 1489/2500 loss: 0.61\n",
      "step: 1490/2500 loss: 0.73\n",
      "step: 1491/2500 loss: 0.65\n",
      "step: 1492/2500 loss: 0.73\n",
      "step: 1493/2500 loss: 0.62\n",
      "step: 1494/2500 loss: 0.71\n",
      "step: 1495/2500 loss: 0.75\n",
      "step: 1496/2500 loss: 0.68\n",
      "step: 1497/2500 loss: 0.65\n",
      "step: 1498/2500 loss: 0.69\n",
      "step: 1499/2500 loss: 0.76\n",
      "eval loss: 0.67\n",
      "step: 1500/2500 loss: 0.62\n",
      "step: 1501/2500 loss: 0.84\n",
      "step: 1502/2500 loss: 0.77\n",
      "step: 1503/2500 loss: 0.72\n",
      "step: 1504/2500 loss: 0.53\n",
      "step: 1505/2500 loss: 0.67\n",
      "step: 1506/2500 loss: 0.8\n",
      "step: 1507/2500 loss: 0.71\n",
      "step: 1508/2500 loss: 0.58\n",
      "step: 1509/2500 loss: 0.58\n",
      "step: 1510/2500 loss: 0.61\n",
      "step: 1511/2500 loss: 0.84\n",
      "step: 1512/2500 loss: 0.69\n",
      "step: 1513/2500 loss: 0.76\n",
      "step: 1514/2500 loss: 0.83\n",
      "step: 1515/2500 loss: 0.74\n",
      "step: 1516/2500 loss: 0.62\n",
      "step: 1517/2500 loss: 0.65\n",
      "step: 1518/2500 loss: 0.79\n",
      "step: 1519/2500 loss: 0.64\n",
      "step: 1520/2500 loss: 0.83\n",
      "step: 1521/2500 loss: 0.66\n",
      "step: 1522/2500 loss: 0.73\n",
      "step: 1523/2500 loss: 0.68\n",
      "step: 1524/2500 loss: 0.96\n",
      "step: 1525/2500 loss: 0.72\n",
      "step: 1526/2500 loss: 0.73\n",
      "step: 1527/2500 loss: 0.7\n",
      "step: 1528/2500 loss: 0.68\n",
      "step: 1529/2500 loss: 0.6\n",
      "step: 1530/2500 loss: 0.64\n",
      "step: 1531/2500 loss: 0.69\n",
      "step: 1532/2500 loss: 0.62\n",
      "step: 1533/2500 loss: 0.73\n",
      "step: 1534/2500 loss: 0.79\n",
      "step: 1535/2500 loss: 0.7\n",
      "step: 1536/2500 loss: 0.57\n",
      "step: 1537/2500 loss: 0.71\n",
      "step: 1538/2500 loss: 0.69\n",
      "step: 1539/2500 loss: 0.71\n",
      "step: 1540/2500 loss: 0.62\n",
      "step: 1541/2500 loss: 0.61\n",
      "step: 1542/2500 loss: 0.7\n",
      "step: 1543/2500 loss: 0.72\n",
      "step: 1544/2500 loss: 0.7\n",
      "step: 1545/2500 loss: 0.6\n",
      "step: 1546/2500 loss: 0.64\n",
      "step: 1547/2500 loss: 0.69\n",
      "step: 1548/2500 loss: 0.71\n",
      "step: 1549/2500 loss: 0.66\n",
      "step: 1550/2500 loss: 0.7\n",
      "step: 1551/2500 loss: 0.69\n",
      "step: 1552/2500 loss: 0.65\n",
      "step: 1553/2500 loss: 0.75\n",
      "step: 1554/2500 loss: 0.78\n",
      "step: 1555/2500 loss: 0.69\n",
      "step: 1556/2500 loss: 0.58\n",
      "step: 1557/2500 loss: 0.75\n",
      "step: 1558/2500 loss: 0.73\n",
      "step: 1559/2500 loss: 0.67\n",
      "step: 1560/2500 loss: 0.67\n",
      "step: 1561/2500 loss: 0.69\n",
      "step: 1562/2500 loss: 0.71\n",
      "step: 1563/2500 loss: 0.73\n",
      "step: 1564/2500 loss: 0.69\n",
      "step: 1565/2500 loss: 0.65\n",
      "step: 1566/2500 loss: 0.63\n",
      "step: 1567/2500 loss: 0.58\n",
      "step: 1568/2500 loss: 0.72\n",
      "step: 1569/2500 loss: 0.67\n",
      "step: 1570/2500 loss: 0.65\n",
      "step: 1571/2500 loss: 0.78\n",
      "step: 1572/2500 loss: 0.64\n",
      "step: 1573/2500 loss: 0.64\n",
      "step: 1574/2500 loss: 0.67\n",
      "step: 1575/2500 loss: 0.71\n",
      "step: 1576/2500 loss: 0.62\n",
      "step: 1577/2500 loss: 0.65\n",
      "step: 1578/2500 loss: 0.73\n",
      "step: 1579/2500 loss: 0.68\n",
      "step: 1580/2500 loss: 0.74\n",
      "step: 1581/2500 loss: 0.65\n",
      "step: 1582/2500 loss: 0.65\n",
      "step: 1583/2500 loss: 0.73\n",
      "step: 1584/2500 loss: 0.67\n",
      "step: 1585/2500 loss: 0.65\n",
      "step: 1586/2500 loss: 0.74\n",
      "step: 1587/2500 loss: 0.67\n",
      "step: 1588/2500 loss: 0.71\n",
      "step: 1589/2500 loss: 0.56\n",
      "step: 1590/2500 loss: 0.76\n",
      "step: 1591/2500 loss: 0.78\n",
      "step: 1592/2500 loss: 0.73\n",
      "step: 1593/2500 loss: 0.54\n",
      "step: 1594/2500 loss: 0.59\n",
      "step: 1595/2500 loss: 0.77\n",
      "step: 1596/2500 loss: 0.64\n",
      "step: 1597/2500 loss: 0.66\n",
      "step: 1598/2500 loss: 0.69\n",
      "step: 1599/2500 loss: 0.59\n",
      "step: 1600/2500 loss: 0.72\n",
      "step: 1601/2500 loss: 0.75\n",
      "step: 1602/2500 loss: 0.74\n",
      "step: 1603/2500 loss: 0.79\n",
      "step: 1604/2500 loss: 0.71\n",
      "step: 1605/2500 loss: 0.84\n",
      "step: 1606/2500 loss: 0.76\n",
      "step: 1607/2500 loss: 0.71\n",
      "step: 1608/2500 loss: 0.68\n",
      "step: 1609/2500 loss: 0.66\n",
      "step: 1610/2500 loss: 0.72\n",
      "step: 1611/2500 loss: 0.7\n",
      "step: 1612/2500 loss: 0.54\n",
      "step: 1613/2500 loss: 0.69\n",
      "step: 1614/2500 loss: 0.68\n",
      "step: 1615/2500 loss: 0.64\n",
      "step: 1616/2500 loss: 0.69\n",
      "step: 1617/2500 loss: 0.56\n",
      "step: 1618/2500 loss: 0.58\n",
      "step: 1619/2500 loss: 0.68\n",
      "step: 1620/2500 loss: 0.61\n",
      "step: 1621/2500 loss: 0.7\n",
      "step: 1622/2500 loss: 0.73\n",
      "step: 1623/2500 loss: 0.72\n",
      "step: 1624/2500 loss: 0.68\n",
      "step: 1625/2500 loss: 0.61\n",
      "step: 1626/2500 loss: 0.65\n",
      "step: 1627/2500 loss: 0.8\n",
      "step: 1628/2500 loss: 0.69\n",
      "step: 1629/2500 loss: 0.67\n",
      "step: 1630/2500 loss: 0.77\n",
      "step: 1631/2500 loss: 0.76\n",
      "step: 1632/2500 loss: 0.83\n",
      "step: 1633/2500 loss: 0.71\n",
      "step: 1634/2500 loss: 0.71\n",
      "step: 1635/2500 loss: 0.67\n",
      "step: 1636/2500 loss: 0.64\n",
      "step: 1637/2500 loss: 0.78\n",
      "step: 1638/2500 loss: 0.77\n",
      "step: 1639/2500 loss: 0.65\n",
      "step: 1640/2500 loss: 0.66\n",
      "step: 1641/2500 loss: 0.69\n",
      "step: 1642/2500 loss: 0.6\n",
      "step: 1643/2500 loss: 0.66\n",
      "step: 1644/2500 loss: 0.64\n",
      "step: 1645/2500 loss: 0.56\n",
      "step: 1646/2500 loss: 0.58\n",
      "step: 1647/2500 loss: 0.64\n",
      "step: 1648/2500 loss: 0.65\n",
      "step: 1649/2500 loss: 0.67\n",
      "step: 1650/2500 loss: 0.66\n",
      "step: 1651/2500 loss: 0.61\n",
      "step: 1652/2500 loss: 0.7\n",
      "step: 1653/2500 loss: 0.72\n",
      "step: 1654/2500 loss: 0.7\n",
      "step: 1655/2500 loss: 0.72\n",
      "step: 1656/2500 loss: 0.58\n",
      "step: 1657/2500 loss: 0.71\n",
      "step: 1658/2500 loss: 0.8\n",
      "step: 1659/2500 loss: 0.86\n",
      "step: 1660/2500 loss: 0.6\n",
      "step: 1661/2500 loss: 0.69\n",
      "step: 1662/2500 loss: 0.68\n",
      "step: 1663/2500 loss: 0.62\n",
      "step: 1664/2500 loss: 0.62\n",
      "step: 1665/2500 loss: 0.84\n",
      "step: 1666/2500 loss: 0.59\n",
      "step: 1667/2500 loss: 0.77\n",
      "step: 1668/2500 loss: 0.61\n",
      "step: 1669/2500 loss: 0.64\n",
      "step: 1670/2500 loss: 0.7\n",
      "step: 1671/2500 loss: 0.64\n",
      "step: 1672/2500 loss: 0.7\n",
      "step: 1673/2500 loss: 0.76\n",
      "step: 1674/2500 loss: 0.73\n",
      "step: 1675/2500 loss: 0.61\n",
      "step: 1676/2500 loss: 0.64\n",
      "step: 1677/2500 loss: 0.67\n",
      "step: 1678/2500 loss: 0.63\n",
      "step: 1679/2500 loss: 0.69\n",
      "step: 1680/2500 loss: 0.67\n",
      "step: 1681/2500 loss: 0.76\n",
      "step: 1682/2500 loss: 0.69\n",
      "step: 1683/2500 loss: 0.66\n",
      "step: 1684/2500 loss: 0.76\n",
      "step: 1685/2500 loss: 0.73\n",
      "step: 1686/2500 loss: 0.64\n",
      "step: 1687/2500 loss: 0.66\n",
      "step: 1688/2500 loss: 0.71\n",
      "step: 1689/2500 loss: 0.61\n",
      "step: 1690/2500 loss: 0.71\n",
      "step: 1691/2500 loss: 0.57\n",
      "step: 1692/2500 loss: 0.73\n",
      "step: 1693/2500 loss: 0.63\n",
      "step: 1694/2500 loss: 0.66\n",
      "step: 1695/2500 loss: 0.7\n",
      "step: 1696/2500 loss: 0.72\n",
      "step: 1697/2500 loss: 0.78\n",
      "step: 1698/2500 loss: 0.69\n",
      "step: 1699/2500 loss: 0.64\n",
      "step: 1700/2500 loss: 0.65\n",
      "step: 1701/2500 loss: 0.63\n",
      "step: 1702/2500 loss: 0.62\n",
      "step: 1703/2500 loss: 0.66\n",
      "step: 1704/2500 loss: 0.64\n",
      "step: 1705/2500 loss: 0.61\n",
      "step: 1706/2500 loss: 0.59\n",
      "step: 1707/2500 loss: 0.68\n",
      "step: 1708/2500 loss: 0.7\n",
      "step: 1709/2500 loss: 0.69\n",
      "step: 1710/2500 loss: 0.56\n",
      "step: 1711/2500 loss: 0.71\n",
      "step: 1712/2500 loss: 0.67\n",
      "step: 1713/2500 loss: 0.69\n",
      "step: 1714/2500 loss: 0.7\n",
      "step: 1715/2500 loss: 0.72\n",
      "step: 1716/2500 loss: 0.65\n",
      "step: 1717/2500 loss: 0.59\n",
      "step: 1718/2500 loss: 0.66\n",
      "step: 1719/2500 loss: 0.73\n",
      "step: 1720/2500 loss: 0.54\n",
      "step: 1721/2500 loss: 0.65\n",
      "step: 1722/2500 loss: 0.64\n",
      "step: 1723/2500 loss: 0.66\n",
      "step: 1724/2500 loss: 0.76\n",
      "step: 1725/2500 loss: 0.76\n",
      "step: 1726/2500 loss: 0.69\n",
      "step: 1727/2500 loss: 0.65\n",
      "step: 1728/2500 loss: 0.66\n",
      "step: 1729/2500 loss: 0.73\n",
      "step: 1730/2500 loss: 0.67\n",
      "step: 1731/2500 loss: 0.67\n",
      "step: 1732/2500 loss: 0.79\n",
      "step: 1733/2500 loss: 0.56\n",
      "step: 1734/2500 loss: 0.54\n",
      "step: 1735/2500 loss: 0.65\n",
      "step: 1736/2500 loss: 0.67\n",
      "step: 1737/2500 loss: 0.68\n",
      "step: 1738/2500 loss: 0.64\n",
      "step: 1739/2500 loss: 0.66\n",
      "step: 1740/2500 loss: 0.74\n",
      "step: 1741/2500 loss: 0.69\n",
      "step: 1742/2500 loss: 0.62\n",
      "step: 1743/2500 loss: 0.75\n",
      "step: 1744/2500 loss: 0.63\n",
      "step: 1745/2500 loss: 0.76\n",
      "step: 1746/2500 loss: 0.65\n",
      "step: 1747/2500 loss: 0.71\n",
      "step: 1748/2500 loss: 0.66\n",
      "step: 1749/2500 loss: 0.67\n",
      "eval loss: 0.66\n",
      "step: 1750/2500 loss: 0.7\n",
      "step: 1751/2500 loss: 0.67\n",
      "step: 1752/2500 loss: 0.64\n",
      "step: 1753/2500 loss: 0.75\n",
      "step: 1754/2500 loss: 0.54\n",
      "step: 1755/2500 loss: 0.65\n",
      "step: 1756/2500 loss: 0.71\n",
      "step: 1757/2500 loss: 0.64\n",
      "step: 1758/2500 loss: 0.75\n",
      "step: 1759/2500 loss: 0.71\n",
      "step: 1760/2500 loss: 0.7\n",
      "step: 1761/2500 loss: 0.67\n",
      "step: 1762/2500 loss: 0.61\n",
      "step: 1763/2500 loss: 0.76\n",
      "step: 1764/2500 loss: 0.65\n",
      "step: 1765/2500 loss: 0.58\n",
      "step: 1766/2500 loss: 0.65\n",
      "step: 1767/2500 loss: 0.64\n",
      "step: 1768/2500 loss: 0.69\n",
      "step: 1769/2500 loss: 0.69\n",
      "step: 1770/2500 loss: 0.74\n",
      "step: 1771/2500 loss: 0.76\n",
      "step: 1772/2500 loss: 0.56\n",
      "step: 1773/2500 loss: 0.67\n",
      "step: 1774/2500 loss: 0.85\n",
      "step: 1775/2500 loss: 0.75\n",
      "step: 1776/2500 loss: 0.68\n",
      "step: 1777/2500 loss: 0.72\n",
      "step: 1778/2500 loss: 0.62\n",
      "step: 1779/2500 loss: 0.77\n",
      "step: 1780/2500 loss: 0.64\n",
      "step: 1781/2500 loss: 0.61\n",
      "step: 1782/2500 loss: 0.65\n",
      "step: 1783/2500 loss: 0.66\n",
      "step: 1784/2500 loss: 0.59\n",
      "step: 1785/2500 loss: 0.62\n",
      "step: 1786/2500 loss: 0.69\n",
      "step: 1787/2500 loss: 0.71\n",
      "step: 1788/2500 loss: 0.64\n",
      "step: 1789/2500 loss: 0.62\n",
      "step: 1790/2500 loss: 0.63\n",
      "step: 1791/2500 loss: 0.64\n",
      "step: 1792/2500 loss: 0.6\n",
      "step: 1793/2500 loss: 0.68\n",
      "step: 1794/2500 loss: 0.58\n",
      "step: 1795/2500 loss: 0.61\n",
      "step: 1796/2500 loss: 0.74\n",
      "step: 1797/2500 loss: 0.56\n",
      "step: 1798/2500 loss: 0.67\n",
      "step: 1799/2500 loss: 0.56\n",
      "step: 1800/2500 loss: 0.62\n",
      "step: 1801/2500 loss: 0.74\n",
      "step: 1802/2500 loss: 0.58\n",
      "step: 1803/2500 loss: 0.57\n",
      "step: 1804/2500 loss: 0.76\n",
      "step: 1805/2500 loss: 0.56\n",
      "step: 1806/2500 loss: 0.72\n",
      "step: 1807/2500 loss: 0.59\n",
      "step: 1808/2500 loss: 0.69\n",
      "step: 1809/2500 loss: 0.79\n",
      "step: 1810/2500 loss: 0.68\n",
      "step: 1811/2500 loss: 0.6\n",
      "step: 1812/2500 loss: 0.54\n",
      "step: 1813/2500 loss: 0.71\n",
      "step: 1814/2500 loss: 0.69\n",
      "step: 1815/2500 loss: 0.57\n",
      "step: 1816/2500 loss: 0.61\n",
      "step: 1817/2500 loss: 0.63\n",
      "step: 1818/2500 loss: 0.54\n",
      "step: 1819/2500 loss: 0.67\n",
      "step: 1820/2500 loss: 0.68\n",
      "step: 1821/2500 loss: 0.71\n",
      "step: 1822/2500 loss: 0.63\n",
      "step: 1823/2500 loss: 0.69\n",
      "step: 1824/2500 loss: 0.61\n",
      "step: 1825/2500 loss: 0.74\n",
      "step: 1826/2500 loss: 0.71\n",
      "step: 1827/2500 loss: 0.78\n",
      "step: 1828/2500 loss: 0.64\n",
      "step: 1829/2500 loss: 0.66\n",
      "step: 1830/2500 loss: 0.64\n",
      "step: 1831/2500 loss: 0.7\n",
      "step: 1832/2500 loss: 0.75\n",
      "step: 1833/2500 loss: 0.85\n",
      "step: 1834/2500 loss: 0.57\n",
      "step: 1835/2500 loss: 0.63\n",
      "step: 1836/2500 loss: 0.76\n",
      "step: 1837/2500 loss: 0.63\n",
      "step: 1838/2500 loss: 0.66\n",
      "step: 1839/2500 loss: 0.78\n",
      "step: 1840/2500 loss: 0.78\n",
      "step: 1841/2500 loss: 0.64\n",
      "step: 1842/2500 loss: 0.76\n",
      "step: 1843/2500 loss: 0.63\n",
      "step: 1844/2500 loss: 0.81\n",
      "step: 1845/2500 loss: 0.68\n",
      "step: 1846/2500 loss: 0.68\n",
      "step: 1847/2500 loss: 0.56\n",
      "step: 1848/2500 loss: 0.81\n",
      "step: 1849/2500 loss: 0.75\n",
      "step: 1850/2500 loss: 0.73\n",
      "step: 1851/2500 loss: 0.64\n",
      "step: 1852/2500 loss: 0.58\n",
      "step: 1853/2500 loss: 0.73\n",
      "step: 1854/2500 loss: 0.66\n",
      "step: 1855/2500 loss: 0.67\n",
      "step: 1856/2500 loss: 0.6\n",
      "step: 1857/2500 loss: 0.71\n",
      "step: 1858/2500 loss: 0.69\n",
      "step: 1859/2500 loss: 0.85\n",
      "step: 1860/2500 loss: 0.8\n",
      "step: 1861/2500 loss: 0.67\n",
      "step: 1862/2500 loss: 0.65\n",
      "step: 1863/2500 loss: 0.63\n",
      "step: 1864/2500 loss: 0.7\n",
      "step: 1865/2500 loss: 0.79\n",
      "step: 1866/2500 loss: 0.63\n",
      "step: 1867/2500 loss: 0.59\n",
      "step: 1868/2500 loss: 0.69\n",
      "step: 1869/2500 loss: 0.74\n",
      "step: 1870/2500 loss: 0.57\n",
      "step: 1871/2500 loss: 0.79\n",
      "step: 1872/2500 loss: 0.64\n",
      "step: 1873/2500 loss: 0.76\n",
      "step: 1874/2500 loss: 0.61\n",
      "step: 1875/2500 loss: 0.68\n",
      "step: 1876/2500 loss: 0.75\n",
      "step: 1877/2500 loss: 0.85\n",
      "step: 1878/2500 loss: 0.77\n",
      "step: 1879/2500 loss: 0.56\n",
      "step: 1880/2500 loss: 0.57\n",
      "step: 1881/2500 loss: 0.65\n",
      "step: 1882/2500 loss: 0.58\n",
      "step: 1883/2500 loss: 0.63\n",
      "step: 1884/2500 loss: 0.66\n",
      "step: 1885/2500 loss: 0.71\n",
      "step: 1886/2500 loss: 0.67\n",
      "step: 1887/2500 loss: 0.6\n",
      "step: 1888/2500 loss: 0.67\n",
      "step: 1889/2500 loss: 0.59\n",
      "step: 1890/2500 loss: 0.68\n",
      "step: 1891/2500 loss: 0.57\n",
      "step: 1892/2500 loss: 0.59\n",
      "step: 1893/2500 loss: 0.82\n",
      "step: 1894/2500 loss: 0.58\n",
      "step: 1895/2500 loss: 0.74\n",
      "step: 1896/2500 loss: 0.69\n",
      "step: 1897/2500 loss: 0.67\n",
      "step: 1898/2500 loss: 0.75\n",
      "step: 1899/2500 loss: 0.56\n",
      "step: 1900/2500 loss: 0.63\n",
      "step: 1901/2500 loss: 0.79\n",
      "step: 1902/2500 loss: 0.59\n",
      "step: 1903/2500 loss: 0.72\n",
      "step: 1904/2500 loss: 0.58\n",
      "step: 1905/2500 loss: 0.72\n",
      "step: 1906/2500 loss: 0.64\n",
      "step: 1907/2500 loss: 0.73\n",
      "step: 1908/2500 loss: 0.78\n",
      "step: 1909/2500 loss: 0.75\n",
      "step: 1910/2500 loss: 0.66\n",
      "step: 1911/2500 loss: 0.62\n",
      "step: 1912/2500 loss: 0.9\n",
      "step: 1913/2500 loss: 0.7\n",
      "step: 1914/2500 loss: 0.71\n",
      "step: 1915/2500 loss: 0.76\n",
      "step: 1916/2500 loss: 0.66\n",
      "step: 1917/2500 loss: 0.64\n",
      "step: 1918/2500 loss: 0.61\n",
      "step: 1919/2500 loss: 0.86\n",
      "step: 1920/2500 loss: 0.75\n",
      "step: 1921/2500 loss: 0.63\n",
      "step: 1922/2500 loss: 0.59\n",
      "step: 1923/2500 loss: 0.76\n",
      "step: 1924/2500 loss: 0.64\n",
      "step: 1925/2500 loss: 0.66\n",
      "step: 1926/2500 loss: 0.72\n",
      "step: 1927/2500 loss: 0.67\n",
      "step: 1928/2500 loss: 0.78\n",
      "step: 1929/2500 loss: 0.64\n",
      "step: 1930/2500 loss: 0.62\n",
      "step: 1931/2500 loss: 0.61\n",
      "step: 1932/2500 loss: 0.64\n",
      "step: 1933/2500 loss: 0.65\n",
      "step: 1934/2500 loss: 0.64\n",
      "step: 1935/2500 loss: 0.66\n",
      "step: 1936/2500 loss: 0.67\n",
      "step: 1937/2500 loss: 0.72\n",
      "step: 1938/2500 loss: 0.62\n",
      "step: 1939/2500 loss: 0.69\n",
      "step: 1940/2500 loss: 0.7\n",
      "step: 1941/2500 loss: 0.6\n",
      "step: 1942/2500 loss: 0.79\n",
      "step: 1943/2500 loss: 0.76\n",
      "step: 1944/2500 loss: 0.68\n",
      "step: 1945/2500 loss: 0.78\n",
      "step: 1946/2500 loss: 0.72\n",
      "step: 1947/2500 loss: 0.72\n",
      "step: 1948/2500 loss: 0.76\n",
      "step: 1949/2500 loss: 0.82\n",
      "step: 1950/2500 loss: 0.69\n",
      "step: 1951/2500 loss: 0.65\n",
      "step: 1952/2500 loss: 0.71\n",
      "step: 1953/2500 loss: 0.76\n",
      "step: 1954/2500 loss: 0.61\n",
      "step: 1955/2500 loss: 0.68\n",
      "step: 1956/2500 loss: 0.74\n",
      "step: 1957/2500 loss: 0.56\n",
      "step: 1958/2500 loss: 0.67\n",
      "step: 1959/2500 loss: 0.74\n",
      "step: 1960/2500 loss: 0.56\n",
      "step: 1961/2500 loss: 0.59\n",
      "step: 1962/2500 loss: 0.63\n",
      "step: 1963/2500 loss: 0.79\n",
      "step: 1964/2500 loss: 0.59\n",
      "step: 1965/2500 loss: 0.68\n",
      "step: 1966/2500 loss: 0.66\n",
      "step: 1967/2500 loss: 0.79\n",
      "step: 1968/2500 loss: 0.58\n",
      "step: 1969/2500 loss: 0.71\n",
      "step: 1970/2500 loss: 0.6\n",
      "step: 1971/2500 loss: 0.65\n",
      "step: 1972/2500 loss: 0.82\n",
      "step: 1973/2500 loss: 0.67\n",
      "step: 1974/2500 loss: 0.67\n",
      "step: 1975/2500 loss: 0.67\n",
      "step: 1976/2500 loss: 0.68\n",
      "step: 1977/2500 loss: 0.66\n",
      "step: 1978/2500 loss: 0.72\n",
      "step: 1979/2500 loss: 0.63\n",
      "step: 1980/2500 loss: 0.66\n",
      "step: 1981/2500 loss: 0.78\n",
      "step: 1982/2500 loss: 0.73\n",
      "step: 1983/2500 loss: 0.72\n",
      "step: 1984/2500 loss: 0.7\n",
      "step: 1985/2500 loss: 0.75\n",
      "step: 1986/2500 loss: 0.7\n",
      "step: 1987/2500 loss: 0.58\n",
      "step: 1988/2500 loss: 0.66\n",
      "step: 1989/2500 loss: 0.68\n",
      "step: 1990/2500 loss: 0.64\n",
      "step: 1991/2500 loss: 0.59\n",
      "step: 1992/2500 loss: 0.65\n",
      "step: 1993/2500 loss: 0.87\n",
      "step: 1994/2500 loss: 0.54\n",
      "step: 1995/2500 loss: 0.72\n",
      "step: 1996/2500 loss: 0.68\n",
      "step: 1997/2500 loss: 0.6\n",
      "step: 1998/2500 loss: 0.72\n",
      "step: 1999/2500 loss: 0.72\n",
      "eval loss: 0.66\n",
      "step: 2000/2500 loss: 0.7\n",
      "step: 2001/2500 loss: 0.79\n",
      "step: 2002/2500 loss: 0.67\n",
      "step: 2003/2500 loss: 0.62\n",
      "step: 2004/2500 loss: 0.73\n",
      "step: 2005/2500 loss: 0.65\n",
      "step: 2006/2500 loss: 0.66\n",
      "step: 2007/2500 loss: 0.61\n",
      "step: 2008/2500 loss: 0.59\n",
      "step: 2009/2500 loss: 0.75\n",
      "step: 2010/2500 loss: 0.73\n",
      "step: 2011/2500 loss: 0.63\n",
      "step: 2012/2500 loss: 0.72\n",
      "step: 2013/2500 loss: 0.76\n",
      "step: 2014/2500 loss: 0.75\n",
      "step: 2015/2500 loss: 0.57\n",
      "step: 2016/2500 loss: 0.53\n",
      "step: 2017/2500 loss: 0.67\n",
      "step: 2018/2500 loss: 0.7\n",
      "step: 2019/2500 loss: 0.65\n",
      "step: 2020/2500 loss: 0.65\n",
      "step: 2021/2500 loss: 0.78\n",
      "step: 2022/2500 loss: 0.68\n",
      "step: 2023/2500 loss: 0.59\n",
      "step: 2024/2500 loss: 0.57\n",
      "step: 2025/2500 loss: 0.62\n",
      "step: 2026/2500 loss: 0.66\n",
      "step: 2027/2500 loss: 0.72\n",
      "step: 2028/2500 loss: 0.67\n",
      "step: 2029/2500 loss: 0.77\n",
      "step: 2030/2500 loss: 0.7\n",
      "step: 2031/2500 loss: 0.73\n",
      "step: 2032/2500 loss: 0.56\n",
      "step: 2033/2500 loss: 0.66\n",
      "step: 2034/2500 loss: 0.71\n",
      "step: 2035/2500 loss: 0.69\n",
      "step: 2036/2500 loss: 0.67\n",
      "step: 2037/2500 loss: 0.63\n",
      "step: 2038/2500 loss: 0.62\n",
      "step: 2039/2500 loss: 0.9\n",
      "step: 2040/2500 loss: 0.69\n",
      "step: 2041/2500 loss: 0.73\n",
      "step: 2042/2500 loss: 0.77\n",
      "step: 2043/2500 loss: 0.76\n",
      "step: 2044/2500 loss: 0.78\n",
      "step: 2045/2500 loss: 0.64\n",
      "step: 2046/2500 loss: 0.69\n",
      "step: 2047/2500 loss: 0.64\n",
      "step: 2048/2500 loss: 0.71\n",
      "step: 2049/2500 loss: 0.66\n",
      "step: 2050/2500 loss: 0.7\n",
      "step: 2051/2500 loss: 0.65\n",
      "step: 2052/2500 loss: 0.71\n",
      "step: 2053/2500 loss: 0.58\n",
      "step: 2054/2500 loss: 0.64\n",
      "step: 2055/2500 loss: 0.75\n",
      "step: 2056/2500 loss: 0.65\n",
      "step: 2057/2500 loss: 0.59\n",
      "step: 2058/2500 loss: 0.76\n",
      "step: 2059/2500 loss: 0.67\n",
      "step: 2060/2500 loss: 0.78\n",
      "step: 2061/2500 loss: 0.73\n",
      "step: 2062/2500 loss: 0.66\n",
      "step: 2063/2500 loss: 0.66\n",
      "step: 2064/2500 loss: 0.71\n",
      "step: 2065/2500 loss: 0.73\n",
      "step: 2066/2500 loss: 0.66\n",
      "step: 2067/2500 loss: 0.59\n",
      "step: 2068/2500 loss: 0.72\n",
      "step: 2069/2500 loss: 0.58\n",
      "step: 2070/2500 loss: 0.64\n",
      "step: 2071/2500 loss: 0.7\n",
      "step: 2072/2500 loss: 0.66\n",
      "step: 2073/2500 loss: 0.77\n",
      "step: 2074/2500 loss: 0.83\n",
      "step: 2075/2500 loss: 0.67\n",
      "step: 2076/2500 loss: 0.61\n",
      "step: 2077/2500 loss: 0.62\n",
      "step: 2078/2500 loss: 0.67\n",
      "step: 2079/2500 loss: 0.73\n",
      "step: 2080/2500 loss: 0.79\n",
      "step: 2081/2500 loss: 0.69\n",
      "step: 2082/2500 loss: 0.68\n",
      "step: 2083/2500 loss: 0.66\n",
      "step: 2084/2500 loss: 0.67\n",
      "step: 2085/2500 loss: 0.61\n",
      "step: 2086/2500 loss: 0.68\n",
      "step: 2087/2500 loss: 0.69\n",
      "step: 2088/2500 loss: 0.58\n",
      "step: 2089/2500 loss: 0.55\n",
      "step: 2090/2500 loss: 0.61\n",
      "step: 2091/2500 loss: 0.69\n",
      "step: 2092/2500 loss: 0.67\n",
      "step: 2093/2500 loss: 0.67\n",
      "step: 2094/2500 loss: 0.64\n",
      "step: 2095/2500 loss: 0.62\n",
      "step: 2096/2500 loss: 0.71\n",
      "step: 2097/2500 loss: 0.7\n",
      "step: 2098/2500 loss: 0.72\n",
      "step: 2099/2500 loss: 0.6\n",
      "step: 2100/2500 loss: 0.57\n",
      "step: 2101/2500 loss: 0.8\n",
      "step: 2102/2500 loss: 0.54\n",
      "step: 2103/2500 loss: 0.71\n",
      "step: 2104/2500 loss: 0.63\n",
      "step: 2105/2500 loss: 0.75\n",
      "step: 2106/2500 loss: 0.84\n",
      "step: 2107/2500 loss: 0.76\n",
      "step: 2108/2500 loss: 0.81\n",
      "step: 2109/2500 loss: 0.76\n",
      "step: 2110/2500 loss: 0.57\n",
      "step: 2111/2500 loss: 0.68\n",
      "step: 2112/2500 loss: 0.71\n",
      "step: 2113/2500 loss: 0.83\n",
      "step: 2114/2500 loss: 0.61\n",
      "step: 2115/2500 loss: 0.69\n",
      "step: 2116/2500 loss: 0.66\n",
      "step: 2117/2500 loss: 0.6\n",
      "step: 2118/2500 loss: 0.7\n",
      "step: 2119/2500 loss: 0.65\n",
      "step: 2120/2500 loss: 0.76\n",
      "step: 2121/2500 loss: 0.57\n",
      "step: 2122/2500 loss: 0.71\n",
      "step: 2123/2500 loss: 0.65\n",
      "step: 2124/2500 loss: 0.69\n",
      "step: 2125/2500 loss: 0.79\n",
      "step: 2126/2500 loss: 0.62\n",
      "step: 2127/2500 loss: 0.61\n",
      "step: 2128/2500 loss: 0.67\n",
      "step: 2129/2500 loss: 0.57\n",
      "step: 2130/2500 loss: 0.66\n",
      "step: 2131/2500 loss: 0.79\n",
      "step: 2132/2500 loss: 0.61\n",
      "step: 2133/2500 loss: 0.55\n",
      "step: 2134/2500 loss: 0.7\n",
      "step: 2135/2500 loss: 0.66\n",
      "step: 2136/2500 loss: 0.8\n",
      "step: 2137/2500 loss: 0.71\n",
      "step: 2138/2500 loss: 0.7\n",
      "step: 2139/2500 loss: 0.68\n",
      "step: 2140/2500 loss: 0.88\n",
      "step: 2141/2500 loss: 0.68\n",
      "step: 2142/2500 loss: 0.67\n",
      "step: 2143/2500 loss: 0.63\n",
      "step: 2144/2500 loss: 0.72\n",
      "step: 2145/2500 loss: 0.81\n",
      "step: 2146/2500 loss: 0.77\n",
      "step: 2147/2500 loss: 0.67\n",
      "step: 2148/2500 loss: 0.66\n",
      "step: 2149/2500 loss: 0.67\n",
      "step: 2150/2500 loss: 0.64\n",
      "step: 2151/2500 loss: 0.64\n",
      "step: 2152/2500 loss: 0.73\n",
      "step: 2153/2500 loss: 0.72\n",
      "step: 2154/2500 loss: 0.63\n",
      "step: 2155/2500 loss: 0.69\n",
      "step: 2156/2500 loss: 0.76\n",
      "step: 2157/2500 loss: 0.62\n",
      "step: 2158/2500 loss: 0.7\n",
      "step: 2159/2500 loss: 0.63\n",
      "step: 2160/2500 loss: 0.6\n",
      "step: 2161/2500 loss: 0.6\n",
      "step: 2162/2500 loss: 0.61\n",
      "step: 2163/2500 loss: 0.68\n",
      "step: 2164/2500 loss: 0.7\n",
      "step: 2165/2500 loss: 0.62\n",
      "step: 2166/2500 loss: 0.76\n",
      "step: 2167/2500 loss: 0.7\n",
      "step: 2168/2500 loss: 0.58\n",
      "step: 2169/2500 loss: 0.57\n",
      "step: 2170/2500 loss: 0.59\n",
      "step: 2171/2500 loss: 0.53\n",
      "step: 2172/2500 loss: 0.71\n",
      "step: 2173/2500 loss: 0.69\n",
      "step: 2174/2500 loss: 0.63\n",
      "step: 2175/2500 loss: 0.81\n",
      "step: 2176/2500 loss: 0.76\n",
      "step: 2177/2500 loss: 0.77\n",
      "step: 2178/2500 loss: 0.7\n",
      "step: 2179/2500 loss: 0.69\n",
      "step: 2180/2500 loss: 0.74\n",
      "step: 2181/2500 loss: 0.59\n",
      "step: 2182/2500 loss: 0.65\n",
      "step: 2183/2500 loss: 0.74\n",
      "step: 2184/2500 loss: 0.6\n",
      "step: 2185/2500 loss: 0.72\n",
      "step: 2186/2500 loss: 0.61\n",
      "step: 2187/2500 loss: 0.74\n",
      "step: 2188/2500 loss: 0.75\n",
      "step: 2189/2500 loss: 0.62\n",
      "step: 2190/2500 loss: 0.73\n",
      "step: 2191/2500 loss: 0.72\n",
      "step: 2192/2500 loss: 0.8\n",
      "step: 2193/2500 loss: 0.72\n",
      "step: 2194/2500 loss: 0.65\n",
      "step: 2195/2500 loss: 0.69\n",
      "step: 2196/2500 loss: 0.66\n",
      "step: 2197/2500 loss: 0.81\n",
      "step: 2198/2500 loss: 0.62\n",
      "step: 2199/2500 loss: 0.68\n",
      "step: 2200/2500 loss: 0.83\n",
      "step: 2201/2500 loss: 0.58\n",
      "step: 2202/2500 loss: 0.63\n",
      "step: 2203/2500 loss: 0.52\n",
      "step: 2204/2500 loss: 0.69\n",
      "step: 2205/2500 loss: 0.72\n",
      "step: 2206/2500 loss: 0.65\n",
      "step: 2207/2500 loss: 0.6\n",
      "step: 2208/2500 loss: 0.65\n",
      "step: 2209/2500 loss: 0.61\n",
      "step: 2210/2500 loss: 0.72\n",
      "step: 2211/2500 loss: 0.73\n",
      "step: 2212/2500 loss: 0.62\n",
      "step: 2213/2500 loss: 0.64\n",
      "step: 2214/2500 loss: 0.71\n",
      "step: 2215/2500 loss: 0.71\n",
      "step: 2216/2500 loss: 0.8\n",
      "step: 2217/2500 loss: 0.73\n",
      "step: 2218/2500 loss: 0.75\n",
      "step: 2219/2500 loss: 0.66\n",
      "step: 2220/2500 loss: 0.59\n",
      "step: 2221/2500 loss: 0.68\n",
      "step: 2222/2500 loss: 0.66\n",
      "step: 2223/2500 loss: 0.72\n",
      "step: 2224/2500 loss: 0.64\n",
      "step: 2225/2500 loss: 0.61\n",
      "step: 2226/2500 loss: 0.76\n",
      "step: 2227/2500 loss: 0.72\n",
      "step: 2228/2500 loss: 0.65\n",
      "step: 2229/2500 loss: 0.63\n",
      "step: 2230/2500 loss: 0.78\n",
      "step: 2231/2500 loss: 0.73\n",
      "step: 2232/2500 loss: 0.63\n",
      "step: 2233/2500 loss: 0.7\n",
      "step: 2234/2500 loss: 0.79\n",
      "step: 2235/2500 loss: 0.62\n",
      "step: 2236/2500 loss: 0.59\n",
      "step: 2237/2500 loss: 0.79\n",
      "step: 2238/2500 loss: 0.75\n",
      "step: 2239/2500 loss: 0.63\n",
      "step: 2240/2500 loss: 0.57\n",
      "step: 2241/2500 loss: 0.61\n",
      "step: 2242/2500 loss: 0.75\n",
      "step: 2243/2500 loss: 0.6\n",
      "step: 2244/2500 loss: 0.62\n",
      "step: 2245/2500 loss: 0.71\n",
      "step: 2246/2500 loss: 0.65\n",
      "step: 2247/2500 loss: 0.8\n",
      "step: 2248/2500 loss: 0.72\n",
      "step: 2249/2500 loss: 0.83\n",
      "eval loss: 0.66\n",
      "step: 2250/2500 loss: 0.68\n",
      "step: 2251/2500 loss: 0.66\n",
      "step: 2252/2500 loss: 0.83\n",
      "step: 2253/2500 loss: 0.59\n",
      "step: 2254/2500 loss: 0.58\n",
      "step: 2255/2500 loss: 0.65\n",
      "step: 2256/2500 loss: 0.68\n",
      "step: 2257/2500 loss: 0.69\n",
      "step: 2258/2500 loss: 0.67\n",
      "step: 2259/2500 loss: 0.75\n",
      "step: 2260/2500 loss: 0.59\n",
      "step: 2261/2500 loss: 0.8\n",
      "step: 2262/2500 loss: 0.62\n",
      "step: 2263/2500 loss: 0.6\n",
      "step: 2264/2500 loss: 0.65\n",
      "step: 2265/2500 loss: 0.74\n",
      "step: 2266/2500 loss: 0.62\n",
      "step: 2267/2500 loss: 0.7\n",
      "step: 2268/2500 loss: 0.61\n",
      "step: 2269/2500 loss: 0.62\n",
      "step: 2270/2500 loss: 0.65\n",
      "step: 2271/2500 loss: 0.72\n",
      "step: 2272/2500 loss: 0.71\n",
      "step: 2273/2500 loss: 0.71\n",
      "step: 2274/2500 loss: 0.57\n",
      "step: 2275/2500 loss: 0.59\n",
      "step: 2276/2500 loss: 0.73\n",
      "step: 2277/2500 loss: 0.72\n",
      "step: 2278/2500 loss: 0.61\n",
      "step: 2279/2500 loss: 0.6\n",
      "step: 2280/2500 loss: 0.78\n",
      "step: 2281/2500 loss: 0.56\n",
      "step: 2282/2500 loss: 0.58\n",
      "step: 2283/2500 loss: 0.7\n",
      "step: 2284/2500 loss: 0.71\n",
      "step: 2285/2500 loss: 0.65\n",
      "step: 2286/2500 loss: 0.68\n",
      "step: 2287/2500 loss: 0.75\n",
      "step: 2288/2500 loss: 0.6\n",
      "step: 2289/2500 loss: 0.63\n",
      "step: 2290/2500 loss: 0.86\n",
      "step: 2291/2500 loss: 0.66\n",
      "step: 2292/2500 loss: 0.69\n",
      "step: 2293/2500 loss: 0.68\n",
      "step: 2294/2500 loss: 0.75\n",
      "step: 2295/2500 loss: 0.68\n",
      "step: 2296/2500 loss: 0.69\n",
      "step: 2297/2500 loss: 0.68\n",
      "step: 2298/2500 loss: 0.65\n",
      "step: 2299/2500 loss: 0.75\n",
      "step: 2300/2500 loss: 0.77\n",
      "step: 2301/2500 loss: 0.68\n",
      "step: 2302/2500 loss: 0.61\n",
      "step: 2303/2500 loss: 0.65\n",
      "step: 2304/2500 loss: 0.68\n",
      "step: 2305/2500 loss: 0.64\n",
      "step: 2306/2500 loss: 0.65\n",
      "step: 2307/2500 loss: 0.66\n",
      "step: 2308/2500 loss: 0.57\n",
      "step: 2309/2500 loss: 0.7\n",
      "step: 2310/2500 loss: 0.71\n",
      "step: 2311/2500 loss: 0.64\n",
      "step: 2312/2500 loss: 0.83\n",
      "step: 2313/2500 loss: 0.57\n",
      "step: 2314/2500 loss: 0.61\n",
      "step: 2315/2500 loss: 0.68\n",
      "step: 2316/2500 loss: 0.68\n",
      "step: 2317/2500 loss: 0.81\n",
      "step: 2318/2500 loss: 0.59\n",
      "step: 2319/2500 loss: 0.72\n",
      "step: 2320/2500 loss: 0.72\n",
      "step: 2321/2500 loss: 0.66\n",
      "step: 2322/2500 loss: 0.73\n",
      "step: 2323/2500 loss: 0.65\n",
      "step: 2324/2500 loss: 0.69\n",
      "step: 2325/2500 loss: 0.6\n",
      "step: 2326/2500 loss: 0.73\n",
      "step: 2327/2500 loss: 0.68\n",
      "step: 2328/2500 loss: 0.64\n",
      "step: 2329/2500 loss: 0.66\n",
      "step: 2330/2500 loss: 0.72\n",
      "step: 2331/2500 loss: 0.71\n",
      "step: 2332/2500 loss: 0.55\n",
      "step: 2333/2500 loss: 0.56\n",
      "step: 2334/2500 loss: 0.61\n",
      "step: 2335/2500 loss: 0.66\n",
      "step: 2336/2500 loss: 0.73\n",
      "step: 2337/2500 loss: 0.77\n",
      "step: 2338/2500 loss: 0.77\n",
      "step: 2339/2500 loss: 0.64\n",
      "step: 2340/2500 loss: 0.62\n",
      "step: 2341/2500 loss: 0.64\n",
      "step: 2342/2500 loss: 0.71\n",
      "step: 2343/2500 loss: 0.71\n",
      "step: 2344/2500 loss: 0.58\n",
      "step: 2345/2500 loss: 0.6\n",
      "step: 2346/2500 loss: 0.69\n",
      "step: 2347/2500 loss: 0.57\n",
      "step: 2348/2500 loss: 0.76\n",
      "step: 2349/2500 loss: 0.63\n",
      "step: 2350/2500 loss: 0.71\n",
      "step: 2351/2500 loss: 0.59\n",
      "step: 2352/2500 loss: 0.64\n",
      "step: 2353/2500 loss: 0.72\n",
      "step: 2354/2500 loss: 0.6\n",
      "step: 2355/2500 loss: 0.66\n",
      "step: 2356/2500 loss: 0.72\n",
      "step: 2357/2500 loss: 0.71\n",
      "step: 2358/2500 loss: 0.64\n",
      "step: 2359/2500 loss: 0.75\n",
      "step: 2360/2500 loss: 0.65\n",
      "step: 2361/2500 loss: 0.63\n",
      "step: 2362/2500 loss: 0.65\n",
      "step: 2363/2500 loss: 0.73\n",
      "step: 2364/2500 loss: 0.72\n",
      "step: 2365/2500 loss: 0.66\n",
      "step: 2366/2500 loss: 0.66\n",
      "step: 2367/2500 loss: 0.79\n",
      "step: 2368/2500 loss: 0.6\n",
      "step: 2369/2500 loss: 0.59\n",
      "step: 2370/2500 loss: 0.58\n",
      "step: 2371/2500 loss: 0.72\n",
      "step: 2372/2500 loss: 0.59\n",
      "step: 2373/2500 loss: 0.53\n",
      "step: 2374/2500 loss: 0.65\n",
      "step: 2375/2500 loss: 0.6\n",
      "step: 2376/2500 loss: 0.62\n",
      "step: 2377/2500 loss: 0.65\n",
      "step: 2378/2500 loss: 0.62\n",
      "step: 2379/2500 loss: 0.62\n",
      "step: 2380/2500 loss: 0.61\n",
      "step: 2381/2500 loss: 0.75\n",
      "step: 2382/2500 loss: 0.55\n",
      "step: 2383/2500 loss: 0.71\n",
      "step: 2384/2500 loss: 0.69\n",
      "step: 2385/2500 loss: 0.64\n",
      "step: 2386/2500 loss: 0.66\n",
      "step: 2387/2500 loss: 0.55\n",
      "step: 2388/2500 loss: 0.82\n",
      "step: 2389/2500 loss: 0.56\n",
      "step: 2390/2500 loss: 0.77\n",
      "step: 2391/2500 loss: 0.66\n",
      "step: 2392/2500 loss: 0.65\n",
      "step: 2393/2500 loss: 0.64\n",
      "step: 2394/2500 loss: 0.65\n",
      "step: 2395/2500 loss: 0.67\n",
      "step: 2396/2500 loss: 0.65\n",
      "step: 2397/2500 loss: 0.76\n",
      "step: 2398/2500 loss: 0.71\n",
      "step: 2399/2500 loss: 0.62\n",
      "step: 2400/2500 loss: 0.68\n",
      "step: 2401/2500 loss: 0.73\n",
      "step: 2402/2500 loss: 0.62\n",
      "step: 2403/2500 loss: 0.7\n",
      "step: 2404/2500 loss: 0.7\n",
      "step: 2405/2500 loss: 0.77\n",
      "step: 2406/2500 loss: 0.69\n",
      "step: 2407/2500 loss: 0.76\n",
      "step: 2408/2500 loss: 0.7\n",
      "step: 2409/2500 loss: 0.66\n",
      "step: 2410/2500 loss: 0.69\n",
      "step: 2411/2500 loss: 0.76\n",
      "step: 2412/2500 loss: 0.68\n",
      "step: 2413/2500 loss: 0.71\n",
      "step: 2414/2500 loss: 0.78\n",
      "step: 2415/2500 loss: 0.71\n",
      "step: 2416/2500 loss: 0.63\n",
      "step: 2417/2500 loss: 0.53\n",
      "step: 2418/2500 loss: 0.64\n",
      "step: 2419/2500 loss: 0.72\n",
      "step: 2420/2500 loss: 0.73\n",
      "step: 2421/2500 loss: 0.69\n",
      "step: 2422/2500 loss: 0.69\n",
      "step: 2423/2500 loss: 0.78\n",
      "step: 2424/2500 loss: 0.68\n",
      "step: 2425/2500 loss: 0.63\n",
      "step: 2426/2500 loss: 0.6\n",
      "step: 2427/2500 loss: 0.69\n",
      "step: 2428/2500 loss: 0.76\n",
      "step: 2429/2500 loss: 0.72\n",
      "step: 2430/2500 loss: 0.61\n",
      "step: 2431/2500 loss: 0.64\n",
      "step: 2432/2500 loss: 0.62\n",
      "step: 2433/2500 loss: 0.66\n",
      "step: 2434/2500 loss: 0.68\n",
      "step: 2435/2500 loss: 0.64\n",
      "step: 2436/2500 loss: 0.63\n",
      "step: 2437/2500 loss: 0.66\n",
      "step: 2438/2500 loss: 0.64\n",
      "step: 2439/2500 loss: 0.63\n",
      "step: 2440/2500 loss: 0.65\n",
      "step: 2441/2500 loss: 0.64\n",
      "step: 2442/2500 loss: 0.72\n",
      "step: 2443/2500 loss: 0.64\n",
      "step: 2444/2500 loss: 0.67\n",
      "step: 2445/2500 loss: 0.63\n",
      "step: 2446/2500 loss: 0.67\n",
      "step: 2447/2500 loss: 0.61\n",
      "step: 2448/2500 loss: 0.75\n",
      "step: 2449/2500 loss: 0.71\n",
      "step: 2450/2500 loss: 0.81\n",
      "step: 2451/2500 loss: 0.69\n",
      "step: 2452/2500 loss: 0.74\n",
      "step: 2453/2500 loss: 0.68\n",
      "step: 2454/2500 loss: 0.71\n",
      "step: 2455/2500 loss: 0.69\n",
      "step: 2456/2500 loss: 0.7\n",
      "step: 2457/2500 loss: 0.79\n",
      "step: 2458/2500 loss: 0.57\n",
      "step: 2459/2500 loss: 0.6\n",
      "step: 2460/2500 loss: 0.66\n",
      "step: 2461/2500 loss: 0.61\n",
      "step: 2462/2500 loss: 0.68\n",
      "step: 2463/2500 loss: 0.61\n",
      "step: 2464/2500 loss: 0.66\n",
      "step: 2465/2500 loss: 0.89\n",
      "step: 2466/2500 loss: 0.62\n",
      "step: 2467/2500 loss: 0.66\n",
      "step: 2468/2500 loss: 0.71\n",
      "step: 2469/2500 loss: 0.61\n",
      "step: 2470/2500 loss: 0.77\n",
      "step: 2471/2500 loss: 0.66\n",
      "step: 2472/2500 loss: 0.73\n",
      "step: 2473/2500 loss: 0.68\n",
      "step: 2474/2500 loss: 0.67\n",
      "step: 2475/2500 loss: 0.56\n",
      "step: 2476/2500 loss: 0.69\n",
      "step: 2477/2500 loss: 0.63\n",
      "step: 2478/2500 loss: 0.69\n",
      "step: 2479/2500 loss: 0.67\n",
      "step: 2480/2500 loss: 0.62\n",
      "step: 2481/2500 loss: 0.76\n",
      "step: 2482/2500 loss: 0.79\n",
      "step: 2483/2500 loss: 0.77\n",
      "step: 2484/2500 loss: 0.61\n",
      "step: 2485/2500 loss: 0.71\n",
      "step: 2486/2500 loss: 0.64\n",
      "step: 2487/2500 loss: 0.76\n",
      "step: 2488/2500 loss: 0.72\n",
      "step: 2489/2500 loss: 0.59\n",
      "step: 2490/2500 loss: 0.66\n",
      "step: 2491/2500 loss: 0.68\n",
      "step: 2492/2500 loss: 0.64\n",
      "step: 2493/2500 loss: 0.64\n",
      "step: 2494/2500 loss: 0.64\n",
      "step: 2495/2500 loss: 0.93\n",
      "step: 2496/2500 loss: 0.7\n",
      "step: 2497/2500 loss: 0.67\n",
      "step: 2498/2500 loss: 0.71\n",
      "step: 2499/2500 loss: 0.86\n"
     ]
    }
   ],
   "source": [
    "model = train_classifier(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "for batch in dataset_test['input_ids']:\n",
    "    logits = model(batch)\n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "    predictions_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_stacked = torch.stack(predictions_list)\n",
    "predictions = predictions_stacked.view(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset_test['labels'].view(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = torch.sum(labels == predictions).float() / labels.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.98999905586243"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.item()*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
