{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, length: int, embed_dim: int, dropout: int):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : int\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    encoder_block : nn.TransformerEncoder\n",
    "        Transformer Encoder.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 27, length: int = 20, embed_dim: int = 768, dropout: int = 0.1, attention_heads: int = 12, layers: int = 2):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        embed_dim : int, optional\n",
    "            Dimensionality of the token and position embeddings. Defaults to 768.\n",
    "        dropout : int, optional\n",
    "            Dropout probability, used for regularisation. Defaults to 0.1.\n",
    "        attention_heads : int, optional\n",
    "            The number of attention heads in the Transformer encoder layer. Defaults to 12.\n",
    "        layers : int, optional\n",
    "            The number of Transformer encoder layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()  # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size, length, embed_dim, dropout) # embedding layer which combines token and position embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, attention_heads, dim_feedforward=embed_dim * 4, dropout=dropout, activation=\"gelu\") # instance of transformer encoder layer\n",
    "        self.encoder_block = nn.TransformerEncoder(encoder_layer, layers) # full transformer encoder consisting of multiple layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "        embeddings = embeddings.transpose(0, 1) # rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\n",
    "\n",
    "        encoder_output = self.encoder_block(embeddings) # pass embeddings through transformer encoder block\n",
    "\n",
    "        logits = self.classifier(encoder_output.transpose(0, 1)) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model : BERT, dataset_test: dict, loss_fn: nn.CrossEntropyLoss, plot_data: dict, step_current: int, step_total: int) -> float:\n",
    "    \"\"\"\n",
    "    Peforms model evaluation by computing the average loss of the entire test dataset. The average loss is printed and 'plot_data' is updated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BERT\n",
    "        An instance of the BERT model to be evaluated.\n",
    "    dataset_test : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    loss_fn : nn.CrossEntropyLoss\n",
    "        The loss function used to compute the loss between the predictions and labels.\n",
    "    plot_data : dict\n",
    "        A dictionary where the key represents the name of the variable and the value is a dictionary of timeline data.\n",
    "        The values are dictionaries structured as so:\n",
    "        - 'x': list\n",
    "            A list of x-coordinate values, representing the given training step.\n",
    "        - 'y': list\n",
    "            A list of y-coordinate values, representing the value of the variable at the given training step.\n",
    "    step_current : int\n",
    "        The current training step during evaluation.\n",
    "    step_total : int\n",
    "        The total number of training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated plot data dictionary with the test loss added.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    batches = len(dataset_test['input_ids']) # number of batches in the test dataset\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        for batch in range(batches):\n",
    "            \n",
    "            logits = model(dataset_test['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_test['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "\n",
    "            loss_batch = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            loss_total += loss_batch.item()\n",
    "\n",
    "    loss_average = loss_total / batches # loss is the average of all batches\n",
    "    model.train() # revert model to training mode\n",
    "\n",
    "    plot_data['test']['x'].append(step_current)\n",
    "    plot_data['test']['y'].append(loss_average)\n",
    "    print(f'step: {step_current}/{step_total} eval loss: {round(loss_average,2)}')\n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(plot_data: dict):\n",
    "    \"\"\"\n",
    "    Displays a plot of the training timeline for various variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    plot_data : dict\n",
    "        A dictionary where the key represents the name of the variable and the value is a dictionary of timeline data.\n",
    "        The values are dictionaries structured as so:\n",
    "        - 'x': list\n",
    "            A list of x-coordinate values, representing the given training step.\n",
    "        - 'y': list\n",
    "            A list of y-coordinate values, representing the value of the variable at the given training step.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(len(plot_data.keys()), 1, figsize=(8, 6), sharex=True) # create subplots\n",
    "\n",
    "    for p,plot in enumerate(plot_data.keys()): # plot x,y of each subplot in plot_data\n",
    "        axs[p].plot(plot_data[plot]['x'],plot_data[plot]['y'])\n",
    "        axs[p].set_title(plot)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(dataset_train: dict, dataset_test: dict, learning_rate: float = 1e-6, epochs: int = 1, warmup_ratio: float = 0.1, eval_every: int = 250, print_train: bool = True, plot: bool = True) -> BERT:\n",
    "    \"\"\"\n",
    "    Creates and trains a BERT model for cumulative frequency classification given a training dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the training data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        Refer to 'dataset_train'.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for the optimiser (magnitiude of weight updates per step). Defaults to 1e-6.\n",
    "    epochs : int, optional\n",
    "        The number of epochs for training. Each epoch corresponds to one full iteration through training data. Defaults to 1.\n",
    "    warmup_ratio : float, optional\n",
    "        The ratio of total training steps that learning rate warmup occurs for. 0 = no warmup, 1 = all warmup. Defaults to 0.1.\n",
    "\n",
    "    print_train : bool, optional\n",
    "        Whether to print the training state at every training step. Defaults to False.\n",
    "    plot : bool, optional\n",
    "        Whether to display a plot of the training timeline once training is finished. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BERT\n",
    "        The trained BERT model.\n",
    "    \"\"\"\n",
    "    plot_data = {key: {'x':[], 'y':[]} for key in ['train','test','lr']} # dict storing x,y plot data for training progress\n",
    "    \n",
    "    model = BERT() # initialise model\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    batches = len(dataset_train['input_ids']) # number of batches in the training dataset\n",
    "    step_total = batches*epochs\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate) # initialise AdamW optimiser\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimiser, lr_lambda=lambda step: lr_scheduler(warmup_ratio, step, step_total)) # create custom learning rate scheduler\n",
    "    loss_fn = nn.CrossEntropyLoss() # initialise cross-entropy loss function for classification\n",
    "\n",
    "    print(\"Beginning Training.\")\n",
    "    print_line()\n",
    "\n",
    "    for epoch in range(epochs): # iterate through epochs\n",
    "        for batch in range(batches): # iterate through batches in epoch\n",
    "            step_current = batch*(epoch+1)\n",
    "            \n",
    "            if batch%eval_every == 0: # perform evaluation on test split at set intervals\n",
    "                plot_data = evaluate(model, dataset_test, loss_fn, plot_data, step_current, step_total)\n",
    "\n",
    "            logits = model(dataset_train['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "            \n",
    "            loss = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            \n",
    "            optimiser.zero_grad() # zero the gradients from previous step (no gradient accumulation)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimiser.step() # update model weights\n",
    "            scheduler.step() # update learning rate\n",
    "\n",
    "            plot_data['train']['x'].append(step_current)\n",
    "            plot_data['train']['y'].append(loss.item())\n",
    "            plot_data['lr']['x'].append(step_current)\n",
    "            plot_data['lr']['y'].append(scheduler.get_last_lr()[0])\n",
    "            if print_train:\n",
    "                print(f'step: {step_current}/{step_total} train loss: {round(loss.item(),2)}, LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    if batch%eval_every != 0: # perform final evaluation (as long as not already performed on this step)\n",
    "        plot_data = evaluate(model, dataset_test, loss_fn, plot_data, step_current, step_total)\n",
    "    print(\"Finishing Training.\")\n",
    "    print_line()\n",
    "    if plot:\n",
    "        plot_train(plot_data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(warmup_ratio: float, step_current: int, step_total: int) -> float:\n",
    "    \"\"\"\n",
    "    Defines a custom learning rate scheduler (warmup and decay) to adjust learning rate based on current training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    warmup_ratio : float\n",
    "        The ratio of total training steps that learning rate warmup occurs for. 0 = no warmup, 1 = all warmup.\n",
    "    step_current : int\n",
    "        The current training step during evaluation.\n",
    "    step_total : int\n",
    "        The total number of training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The ratio that the learning rate will be multiplied by for the given training step.\n",
    "    \"\"\"\n",
    "    warmup_steps = int(step_total*warmup_ratio)\n",
    "    if step_current < warmup_steps: # LR warmup for initial steps\n",
    "        return step_current/max(1,warmup_steps)\n",
    "    else: # linear LR decay for remaining steps\n",
    "        return (step_total-step_current) / max(1,step_total-warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "tokeniser = Tokeniser()\n",
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)\n",
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training.\n",
      "--------------------------------------------------------------------------------\n",
      "step: 0/2500 eval loss: 1.15\n",
      "step: 0/2500 train loss: 1.18, LR: 4.00e-09\n",
      "step: 1/2500 train loss: 1.21, LR: 8.00e-09\n",
      "step: 2/2500 train loss: 1.09, LR: 1.20e-08\n",
      "step: 3/2500 train loss: 1.15, LR: 1.60e-08\n",
      "step: 4/2500 train loss: 1.11, LR: 2.00e-08\n",
      "step: 5/2500 train loss: 1.13, LR: 2.40e-08\n",
      "step: 6/2500 train loss: 1.13, LR: 2.80e-08\n",
      "step: 7/2500 train loss: 1.09, LR: 3.20e-08\n",
      "step: 8/2500 train loss: 1.14, LR: 3.60e-08\n",
      "step: 9/2500 train loss: 1.12, LR: 4.00e-08\n",
      "step: 10/2500 train loss: 1.14, LR: 4.40e-08\n",
      "step: 11/2500 train loss: 1.08, LR: 4.80e-08\n",
      "step: 12/2500 train loss: 1.15, LR: 5.20e-08\n",
      "step: 13/2500 train loss: 1.14, LR: 5.60e-08\n",
      "step: 14/2500 train loss: 1.14, LR: 6.00e-08\n",
      "step: 15/2500 train loss: 1.18, LR: 6.40e-08\n",
      "step: 16/2500 train loss: 1.17, LR: 6.80e-08\n",
      "step: 17/2500 train loss: 1.1, LR: 7.20e-08\n",
      "step: 18/2500 train loss: 1.21, LR: 7.60e-08\n",
      "step: 19/2500 train loss: 1.12, LR: 8.00e-08\n",
      "step: 20/2500 train loss: 1.15, LR: 8.40e-08\n",
      "step: 21/2500 train loss: 1.11, LR: 8.80e-08\n",
      "step: 22/2500 train loss: 1.07, LR: 9.20e-08\n",
      "step: 23/2500 train loss: 1.16, LR: 9.60e-08\n",
      "step: 24/2500 train loss: 1.11, LR: 1.00e-07\n",
      "step: 25/2500 train loss: 1.15, LR: 1.04e-07\n",
      "step: 26/2500 train loss: 1.18, LR: 1.08e-07\n",
      "step: 27/2500 train loss: 1.13, LR: 1.12e-07\n",
      "step: 28/2500 train loss: 1.21, LR: 1.16e-07\n",
      "step: 29/2500 train loss: 1.22, LR: 1.20e-07\n",
      "step: 30/2500 train loss: 1.11, LR: 1.24e-07\n",
      "step: 31/2500 train loss: 1.2, LR: 1.28e-07\n",
      "step: 32/2500 train loss: 1.16, LR: 1.32e-07\n",
      "step: 33/2500 train loss: 1.12, LR: 1.36e-07\n",
      "step: 34/2500 train loss: 1.2, LR: 1.40e-07\n",
      "step: 35/2500 train loss: 1.14, LR: 1.44e-07\n",
      "step: 36/2500 train loss: 1.2, LR: 1.48e-07\n",
      "step: 37/2500 train loss: 1.15, LR: 1.52e-07\n",
      "step: 38/2500 train loss: 1.11, LR: 1.56e-07\n",
      "step: 39/2500 train loss: 1.1, LR: 1.60e-07\n",
      "step: 40/2500 train loss: 1.14, LR: 1.64e-07\n",
      "step: 41/2500 train loss: 1.15, LR: 1.68e-07\n",
      "step: 42/2500 train loss: 1.11, LR: 1.72e-07\n",
      "step: 43/2500 train loss: 1.18, LR: 1.76e-07\n",
      "step: 44/2500 train loss: 1.06, LR: 1.80e-07\n",
      "step: 45/2500 train loss: 1.16, LR: 1.84e-07\n",
      "step: 46/2500 train loss: 1.08, LR: 1.88e-07\n",
      "step: 47/2500 train loss: 1.09, LR: 1.92e-07\n",
      "step: 48/2500 train loss: 1.1, LR: 1.96e-07\n",
      "step: 49/2500 train loss: 1.1, LR: 2.00e-07\n",
      "step: 50/2500 train loss: 1.17, LR: 2.04e-07\n",
      "step: 51/2500 train loss: 1.21, LR: 2.08e-07\n",
      "step: 52/2500 train loss: 1.15, LR: 2.12e-07\n",
      "step: 53/2500 train loss: 1.12, LR: 2.16e-07\n",
      "step: 54/2500 train loss: 1.15, LR: 2.20e-07\n",
      "step: 55/2500 train loss: 1.14, LR: 2.24e-07\n",
      "step: 56/2500 train loss: 1.16, LR: 2.28e-07\n",
      "step: 57/2500 train loss: 1.12, LR: 2.32e-07\n",
      "step: 58/2500 train loss: 1.13, LR: 2.36e-07\n",
      "step: 59/2500 train loss: 1.11, LR: 2.40e-07\n",
      "step: 60/2500 train loss: 1.2, LR: 2.44e-07\n",
      "step: 61/2500 train loss: 1.14, LR: 2.48e-07\n",
      "step: 62/2500 train loss: 1.17, LR: 2.52e-07\n",
      "step: 63/2500 train loss: 1.16, LR: 2.56e-07\n",
      "step: 64/2500 train loss: 1.13, LR: 2.60e-07\n",
      "step: 65/2500 train loss: 1.17, LR: 2.64e-07\n",
      "step: 66/2500 train loss: 1.08, LR: 2.68e-07\n",
      "step: 67/2500 train loss: 1.1, LR: 2.72e-07\n",
      "step: 68/2500 train loss: 1.19, LR: 2.76e-07\n",
      "step: 69/2500 train loss: 1.12, LR: 2.80e-07\n",
      "step: 70/2500 train loss: 1.11, LR: 2.84e-07\n",
      "step: 71/2500 train loss: 1.15, LR: 2.88e-07\n",
      "step: 72/2500 train loss: 1.11, LR: 2.92e-07\n",
      "step: 73/2500 train loss: 1.14, LR: 2.96e-07\n",
      "step: 74/2500 train loss: 1.16, LR: 3.00e-07\n",
      "step: 75/2500 train loss: 1.15, LR: 3.04e-07\n",
      "step: 76/2500 train loss: 1.1, LR: 3.08e-07\n",
      "step: 77/2500 train loss: 1.09, LR: 3.12e-07\n",
      "step: 78/2500 train loss: 1.09, LR: 3.16e-07\n",
      "step: 79/2500 train loss: 1.13, LR: 3.20e-07\n",
      "step: 80/2500 train loss: 1.11, LR: 3.24e-07\n",
      "step: 81/2500 train loss: 1.08, LR: 3.28e-07\n",
      "step: 82/2500 train loss: 1.13, LR: 3.32e-07\n",
      "step: 83/2500 train loss: 1.11, LR: 3.36e-07\n",
      "step: 84/2500 train loss: 1.07, LR: 3.40e-07\n",
      "step: 85/2500 train loss: 1.11, LR: 3.44e-07\n",
      "step: 86/2500 train loss: 1.23, LR: 3.48e-07\n",
      "step: 87/2500 train loss: 1.11, LR: 3.52e-07\n",
      "step: 88/2500 train loss: 1.15, LR: 3.56e-07\n",
      "step: 89/2500 train loss: 1.07, LR: 3.60e-07\n",
      "step: 90/2500 train loss: 1.09, LR: 3.64e-07\n",
      "step: 91/2500 train loss: 1.08, LR: 3.68e-07\n",
      "step: 92/2500 train loss: 1.1, LR: 3.72e-07\n",
      "step: 93/2500 train loss: 1.13, LR: 3.76e-07\n",
      "step: 94/2500 train loss: 1.09, LR: 3.80e-07\n",
      "step: 95/2500 train loss: 1.17, LR: 3.84e-07\n",
      "step: 96/2500 train loss: 1.11, LR: 3.88e-07\n",
      "step: 97/2500 train loss: 1.02, LR: 3.92e-07\n",
      "step: 98/2500 train loss: 1.08, LR: 3.96e-07\n",
      "step: 99/2500 train loss: 1.03, LR: 4.00e-07\n",
      "step: 100/2500 train loss: 1.12, LR: 4.04e-07\n",
      "step: 101/2500 train loss: 1.03, LR: 4.08e-07\n",
      "step: 102/2500 train loss: 1.07, LR: 4.12e-07\n",
      "step: 103/2500 train loss: 1.15, LR: 4.16e-07\n",
      "step: 104/2500 train loss: 1.03, LR: 4.20e-07\n",
      "step: 105/2500 train loss: 1.04, LR: 4.24e-07\n",
      "step: 106/2500 train loss: 1.04, LR: 4.28e-07\n",
      "step: 107/2500 train loss: 1.1, LR: 4.32e-07\n",
      "step: 108/2500 train loss: 1.1, LR: 4.36e-07\n",
      "step: 109/2500 train loss: 1.07, LR: 4.40e-07\n",
      "step: 110/2500 train loss: 1.05, LR: 4.44e-07\n",
      "step: 111/2500 train loss: 1.07, LR: 4.48e-07\n",
      "step: 112/2500 train loss: 1.0, LR: 4.52e-07\n",
      "step: 113/2500 train loss: 1.04, LR: 4.56e-07\n",
      "step: 114/2500 train loss: 1.07, LR: 4.60e-07\n",
      "step: 115/2500 train loss: 1.07, LR: 4.64e-07\n",
      "step: 116/2500 train loss: 1.08, LR: 4.68e-07\n",
      "step: 117/2500 train loss: 1.03, LR: 4.72e-07\n",
      "step: 118/2500 train loss: 1.12, LR: 4.76e-07\n",
      "step: 119/2500 train loss: 1.0, LR: 4.80e-07\n",
      "step: 120/2500 train loss: 1.08, LR: 4.84e-07\n",
      "step: 121/2500 train loss: 1.11, LR: 4.88e-07\n",
      "step: 122/2500 train loss: 1.12, LR: 4.92e-07\n",
      "step: 123/2500 train loss: 1.06, LR: 4.96e-07\n",
      "step: 124/2500 train loss: 0.99, LR: 5.00e-07\n",
      "step: 125/2500 train loss: 0.99, LR: 5.04e-07\n",
      "step: 126/2500 train loss: 1.11, LR: 5.08e-07\n",
      "step: 127/2500 train loss: 1.17, LR: 5.12e-07\n",
      "step: 128/2500 train loss: 1.04, LR: 5.16e-07\n",
      "step: 129/2500 train loss: 1.03, LR: 5.20e-07\n",
      "step: 130/2500 train loss: 1.08, LR: 5.24e-07\n",
      "step: 131/2500 train loss: 1.1, LR: 5.28e-07\n",
      "step: 132/2500 train loss: 1.04, LR: 5.32e-07\n",
      "step: 133/2500 train loss: 1.11, LR: 5.36e-07\n",
      "step: 134/2500 train loss: 1.06, LR: 5.40e-07\n",
      "step: 135/2500 train loss: 1.07, LR: 5.44e-07\n",
      "step: 136/2500 train loss: 1.1, LR: 5.48e-07\n",
      "step: 137/2500 train loss: 1.0, LR: 5.52e-07\n",
      "step: 138/2500 train loss: 1.03, LR: 5.56e-07\n",
      "step: 139/2500 train loss: 1.04, LR: 5.60e-07\n",
      "step: 140/2500 train loss: 1.05, LR: 5.64e-07\n",
      "step: 141/2500 train loss: 0.99, LR: 5.68e-07\n",
      "step: 142/2500 train loss: 1.06, LR: 5.72e-07\n",
      "step: 143/2500 train loss: 0.99, LR: 5.76e-07\n",
      "step: 144/2500 train loss: 1.09, LR: 5.80e-07\n",
      "step: 145/2500 train loss: 1.0, LR: 5.84e-07\n",
      "step: 146/2500 train loss: 1.04, LR: 5.88e-07\n",
      "step: 147/2500 train loss: 0.97, LR: 5.92e-07\n",
      "step: 148/2500 train loss: 1.08, LR: 5.96e-07\n",
      "step: 149/2500 train loss: 0.98, LR: 6.00e-07\n",
      "step: 150/2500 train loss: 0.99, LR: 6.04e-07\n",
      "step: 151/2500 train loss: 1.05, LR: 6.08e-07\n",
      "step: 152/2500 train loss: 1.12, LR: 6.12e-07\n",
      "step: 153/2500 train loss: 1.1, LR: 6.16e-07\n",
      "step: 154/2500 train loss: 1.07, LR: 6.20e-07\n",
      "step: 155/2500 train loss: 1.06, LR: 6.24e-07\n",
      "step: 156/2500 train loss: 0.95, LR: 6.28e-07\n",
      "step: 157/2500 train loss: 1.05, LR: 6.32e-07\n",
      "step: 158/2500 train loss: 0.98, LR: 6.36e-07\n",
      "step: 159/2500 train loss: 1.06, LR: 6.40e-07\n",
      "step: 160/2500 train loss: 1.02, LR: 6.44e-07\n",
      "step: 161/2500 train loss: 0.97, LR: 6.48e-07\n",
      "step: 162/2500 train loss: 0.98, LR: 6.52e-07\n",
      "step: 163/2500 train loss: 1.07, LR: 6.56e-07\n",
      "step: 164/2500 train loss: 0.92, LR: 6.60e-07\n",
      "step: 165/2500 train loss: 0.95, LR: 6.64e-07\n",
      "step: 166/2500 train loss: 0.97, LR: 6.68e-07\n",
      "step: 167/2500 train loss: 0.97, LR: 6.72e-07\n",
      "step: 168/2500 train loss: 0.98, LR: 6.76e-07\n",
      "step: 169/2500 train loss: 1.01, LR: 6.80e-07\n",
      "step: 170/2500 train loss: 1.04, LR: 6.84e-07\n",
      "step: 171/2500 train loss: 0.93, LR: 6.88e-07\n",
      "step: 172/2500 train loss: 1.06, LR: 6.92e-07\n",
      "step: 173/2500 train loss: 0.94, LR: 6.96e-07\n",
      "step: 174/2500 train loss: 1.02, LR: 7.00e-07\n",
      "step: 175/2500 train loss: 1.0, LR: 7.04e-07\n",
      "step: 176/2500 train loss: 0.95, LR: 7.08e-07\n",
      "step: 177/2500 train loss: 0.93, LR: 7.12e-07\n",
      "step: 178/2500 train loss: 1.05, LR: 7.16e-07\n",
      "step: 179/2500 train loss: 0.97, LR: 7.20e-07\n",
      "step: 180/2500 train loss: 0.99, LR: 7.24e-07\n",
      "step: 181/2500 train loss: 1.0, LR: 7.28e-07\n",
      "step: 182/2500 train loss: 0.9, LR: 7.32e-07\n",
      "step: 183/2500 train loss: 0.95, LR: 7.36e-07\n",
      "step: 184/2500 train loss: 0.97, LR: 7.40e-07\n",
      "step: 185/2500 train loss: 0.9, LR: 7.44e-07\n",
      "step: 186/2500 train loss: 0.99, LR: 7.48e-07\n",
      "step: 187/2500 train loss: 1.22, LR: 7.52e-07\n",
      "step: 188/2500 train loss: 1.08, LR: 7.56e-07\n",
      "step: 189/2500 train loss: 0.95, LR: 7.60e-07\n",
      "step: 190/2500 train loss: 0.95, LR: 7.64e-07\n",
      "step: 191/2500 train loss: 0.96, LR: 7.68e-07\n",
      "step: 192/2500 train loss: 0.92, LR: 7.72e-07\n",
      "step: 193/2500 train loss: 0.95, LR: 7.76e-07\n",
      "step: 194/2500 train loss: 0.98, LR: 7.80e-07\n",
      "step: 195/2500 train loss: 0.96, LR: 7.84e-07\n",
      "step: 196/2500 train loss: 0.99, LR: 7.88e-07\n",
      "step: 197/2500 train loss: 1.02, LR: 7.92e-07\n",
      "step: 198/2500 train loss: 1.02, LR: 7.96e-07\n",
      "step: 199/2500 train loss: 0.94, LR: 8.00e-07\n",
      "step: 200/2500 train loss: 1.03, LR: 8.04e-07\n",
      "step: 201/2500 train loss: 0.99, LR: 8.08e-07\n",
      "step: 202/2500 train loss: 0.98, LR: 8.12e-07\n",
      "step: 203/2500 train loss: 0.91, LR: 8.16e-07\n",
      "step: 204/2500 train loss: 1.01, LR: 8.20e-07\n",
      "step: 205/2500 train loss: 0.93, LR: 8.24e-07\n",
      "step: 206/2500 train loss: 0.94, LR: 8.28e-07\n",
      "step: 207/2500 train loss: 0.93, LR: 8.32e-07\n",
      "step: 208/2500 train loss: 1.03, LR: 8.36e-07\n",
      "step: 209/2500 train loss: 1.09, LR: 8.40e-07\n",
      "step: 210/2500 train loss: 0.97, LR: 8.44e-07\n",
      "step: 211/2500 train loss: 1.06, LR: 8.48e-07\n",
      "step: 212/2500 train loss: 1.0, LR: 8.52e-07\n",
      "step: 213/2500 train loss: 0.98, LR: 8.56e-07\n",
      "step: 214/2500 train loss: 0.93, LR: 8.60e-07\n",
      "step: 215/2500 train loss: 0.98, LR: 8.64e-07\n",
      "step: 216/2500 train loss: 0.99, LR: 8.68e-07\n",
      "step: 217/2500 train loss: 0.95, LR: 8.72e-07\n",
      "step: 218/2500 train loss: 0.95, LR: 8.76e-07\n",
      "step: 219/2500 train loss: 0.94, LR: 8.80e-07\n",
      "step: 220/2500 train loss: 1.03, LR: 8.84e-07\n",
      "step: 221/2500 train loss: 0.91, LR: 8.88e-07\n",
      "step: 222/2500 train loss: 0.94, LR: 8.92e-07\n",
      "step: 223/2500 train loss: 0.9, LR: 8.96e-07\n",
      "step: 224/2500 train loss: 0.97, LR: 9.00e-07\n",
      "step: 225/2500 train loss: 0.92, LR: 9.04e-07\n",
      "step: 226/2500 train loss: 0.86, LR: 9.08e-07\n",
      "step: 227/2500 train loss: 0.89, LR: 9.12e-07\n",
      "step: 228/2500 train loss: 0.92, LR: 9.16e-07\n",
      "step: 229/2500 train loss: 0.88, LR: 9.20e-07\n",
      "step: 230/2500 train loss: 0.91, LR: 9.24e-07\n",
      "step: 231/2500 train loss: 1.03, LR: 9.28e-07\n",
      "step: 232/2500 train loss: 0.99, LR: 9.32e-07\n",
      "step: 233/2500 train loss: 0.94, LR: 9.36e-07\n",
      "step: 234/2500 train loss: 0.99, LR: 9.40e-07\n",
      "step: 235/2500 train loss: 0.99, LR: 9.44e-07\n",
      "step: 236/2500 train loss: 0.91, LR: 9.48e-07\n",
      "step: 237/2500 train loss: 1.03, LR: 9.52e-07\n",
      "step: 238/2500 train loss: 0.83, LR: 9.56e-07\n",
      "step: 239/2500 train loss: 0.97, LR: 9.60e-07\n",
      "step: 240/2500 train loss: 0.96, LR: 9.64e-07\n",
      "step: 241/2500 train loss: 0.93, LR: 9.68e-07\n",
      "step: 242/2500 train loss: 0.93, LR: 9.72e-07\n",
      "step: 243/2500 train loss: 0.98, LR: 9.76e-07\n",
      "step: 244/2500 train loss: 0.92, LR: 9.80e-07\n",
      "step: 245/2500 train loss: 0.93, LR: 9.84e-07\n",
      "step: 246/2500 train loss: 0.92, LR: 9.88e-07\n",
      "step: 247/2500 train loss: 0.89, LR: 9.92e-07\n",
      "step: 248/2500 train loss: 0.94, LR: 9.96e-07\n",
      "step: 249/2500 train loss: 0.96, LR: 1.00e-06\n",
      "step: 250/2500 eval loss: 0.92\n",
      "step: 250/2500 train loss: 0.9, LR: 1.00e-06\n",
      "step: 251/2500 train loss: 1.01, LR: 9.99e-07\n",
      "step: 252/2500 train loss: 0.96, LR: 9.99e-07\n",
      "step: 253/2500 train loss: 0.88, LR: 9.98e-07\n",
      "step: 254/2500 train loss: 0.93, LR: 9.98e-07\n",
      "step: 255/2500 train loss: 0.89, LR: 9.97e-07\n",
      "step: 256/2500 train loss: 0.84, LR: 9.97e-07\n",
      "step: 257/2500 train loss: 0.95, LR: 9.96e-07\n",
      "step: 258/2500 train loss: 0.92, LR: 9.96e-07\n",
      "step: 259/2500 train loss: 0.87, LR: 9.96e-07\n",
      "step: 260/2500 train loss: 0.94, LR: 9.95e-07\n",
      "step: 261/2500 train loss: 1.02, LR: 9.95e-07\n",
      "step: 262/2500 train loss: 0.92, LR: 9.94e-07\n",
      "step: 263/2500 train loss: 0.83, LR: 9.94e-07\n",
      "step: 264/2500 train loss: 0.92, LR: 9.93e-07\n",
      "step: 265/2500 train loss: 0.93, LR: 9.93e-07\n",
      "step: 266/2500 train loss: 0.93, LR: 9.92e-07\n",
      "step: 267/2500 train loss: 0.88, LR: 9.92e-07\n",
      "step: 268/2500 train loss: 0.92, LR: 9.92e-07\n",
      "step: 269/2500 train loss: 0.88, LR: 9.91e-07\n",
      "step: 270/2500 train loss: 0.93, LR: 9.91e-07\n",
      "step: 271/2500 train loss: 0.86, LR: 9.90e-07\n",
      "step: 272/2500 train loss: 0.88, LR: 9.90e-07\n",
      "step: 273/2500 train loss: 0.84, LR: 9.89e-07\n",
      "step: 274/2500 train loss: 0.95, LR: 9.89e-07\n",
      "step: 275/2500 train loss: 0.92, LR: 9.88e-07\n",
      "step: 276/2500 train loss: 0.94, LR: 9.88e-07\n",
      "step: 277/2500 train loss: 0.94, LR: 9.88e-07\n",
      "step: 278/2500 train loss: 0.97, LR: 9.87e-07\n",
      "step: 279/2500 train loss: 0.88, LR: 9.87e-07\n",
      "step: 280/2500 train loss: 0.95, LR: 9.86e-07\n",
      "step: 281/2500 train loss: 0.88, LR: 9.86e-07\n",
      "step: 282/2500 train loss: 0.97, LR: 9.85e-07\n",
      "step: 283/2500 train loss: 0.9, LR: 9.85e-07\n",
      "step: 284/2500 train loss: 0.88, LR: 9.84e-07\n",
      "step: 285/2500 train loss: 0.78, LR: 9.84e-07\n",
      "step: 286/2500 train loss: 0.93, LR: 9.84e-07\n",
      "step: 287/2500 train loss: 0.89, LR: 9.83e-07\n",
      "step: 288/2500 train loss: 0.84, LR: 9.83e-07\n",
      "step: 289/2500 train loss: 0.83, LR: 9.82e-07\n",
      "step: 290/2500 train loss: 0.87, LR: 9.82e-07\n",
      "step: 291/2500 train loss: 0.84, LR: 9.81e-07\n",
      "step: 292/2500 train loss: 0.92, LR: 9.81e-07\n",
      "step: 293/2500 train loss: 0.88, LR: 9.80e-07\n",
      "step: 294/2500 train loss: 0.8, LR: 9.80e-07\n",
      "step: 295/2500 train loss: 0.91, LR: 9.80e-07\n",
      "step: 296/2500 train loss: 0.92, LR: 9.79e-07\n",
      "step: 297/2500 train loss: 0.93, LR: 9.79e-07\n",
      "step: 298/2500 train loss: 0.85, LR: 9.78e-07\n",
      "step: 299/2500 train loss: 0.91, LR: 9.78e-07\n",
      "step: 300/2500 train loss: 0.89, LR: 9.77e-07\n",
      "step: 301/2500 train loss: 0.82, LR: 9.77e-07\n",
      "step: 302/2500 train loss: 0.86, LR: 9.76e-07\n",
      "step: 303/2500 train loss: 0.85, LR: 9.76e-07\n",
      "step: 304/2500 train loss: 0.87, LR: 9.76e-07\n",
      "step: 305/2500 train loss: 0.89, LR: 9.75e-07\n",
      "step: 306/2500 train loss: 0.95, LR: 9.75e-07\n",
      "step: 307/2500 train loss: 0.91, LR: 9.74e-07\n",
      "step: 308/2500 train loss: 0.95, LR: 9.74e-07\n",
      "step: 309/2500 train loss: 0.89, LR: 9.73e-07\n",
      "step: 310/2500 train loss: 0.84, LR: 9.73e-07\n",
      "step: 311/2500 train loss: 0.85, LR: 9.72e-07\n",
      "step: 312/2500 train loss: 0.86, LR: 9.72e-07\n",
      "step: 313/2500 train loss: 0.86, LR: 9.72e-07\n",
      "step: 314/2500 train loss: 0.98, LR: 9.71e-07\n",
      "step: 315/2500 train loss: 0.81, LR: 9.71e-07\n",
      "step: 316/2500 train loss: 0.95, LR: 9.70e-07\n",
      "step: 317/2500 train loss: 0.82, LR: 9.70e-07\n",
      "step: 318/2500 train loss: 0.81, LR: 9.69e-07\n",
      "step: 319/2500 train loss: 1.04, LR: 9.69e-07\n",
      "step: 320/2500 train loss: 0.84, LR: 9.68e-07\n",
      "step: 321/2500 train loss: 0.93, LR: 9.68e-07\n",
      "step: 322/2500 train loss: 0.79, LR: 9.68e-07\n",
      "step: 323/2500 train loss: 0.94, LR: 9.67e-07\n",
      "step: 324/2500 train loss: 0.94, LR: 9.67e-07\n",
      "step: 325/2500 train loss: 0.89, LR: 9.66e-07\n",
      "step: 326/2500 train loss: 0.9, LR: 9.66e-07\n",
      "step: 327/2500 train loss: 0.87, LR: 9.65e-07\n",
      "step: 328/2500 train loss: 0.89, LR: 9.65e-07\n",
      "step: 329/2500 train loss: 0.85, LR: 9.64e-07\n",
      "step: 330/2500 train loss: 0.92, LR: 9.64e-07\n",
      "step: 331/2500 train loss: 0.93, LR: 9.64e-07\n",
      "step: 332/2500 train loss: 0.95, LR: 9.63e-07\n",
      "step: 333/2500 train loss: 0.83, LR: 9.63e-07\n",
      "step: 334/2500 train loss: 0.9, LR: 9.62e-07\n",
      "step: 335/2500 train loss: 0.86, LR: 9.62e-07\n",
      "step: 336/2500 train loss: 0.81, LR: 9.61e-07\n",
      "step: 337/2500 train loss: 0.84, LR: 9.61e-07\n",
      "step: 338/2500 train loss: 0.93, LR: 9.60e-07\n",
      "step: 339/2500 train loss: 0.83, LR: 9.60e-07\n",
      "step: 340/2500 train loss: 0.85, LR: 9.60e-07\n",
      "step: 341/2500 train loss: 0.94, LR: 9.59e-07\n",
      "step: 342/2500 train loss: 0.9, LR: 9.59e-07\n",
      "step: 343/2500 train loss: 0.89, LR: 9.58e-07\n",
      "step: 344/2500 train loss: 0.91, LR: 9.58e-07\n",
      "step: 345/2500 train loss: 0.88, LR: 9.57e-07\n",
      "step: 346/2500 train loss: 0.75, LR: 9.57e-07\n",
      "step: 347/2500 train loss: 0.85, LR: 9.56e-07\n",
      "step: 348/2500 train loss: 0.84, LR: 9.56e-07\n",
      "step: 349/2500 train loss: 0.9, LR: 9.56e-07\n",
      "step: 350/2500 train loss: 0.82, LR: 9.55e-07\n",
      "step: 351/2500 train loss: 0.85, LR: 9.55e-07\n",
      "step: 352/2500 train loss: 0.77, LR: 9.54e-07\n",
      "step: 353/2500 train loss: 0.91, LR: 9.54e-07\n",
      "step: 354/2500 train loss: 0.78, LR: 9.53e-07\n",
      "step: 355/2500 train loss: 0.82, LR: 9.53e-07\n",
      "step: 356/2500 train loss: 0.9, LR: 9.52e-07\n",
      "step: 357/2500 train loss: 0.84, LR: 9.52e-07\n",
      "step: 358/2500 train loss: 0.85, LR: 9.52e-07\n",
      "step: 359/2500 train loss: 0.83, LR: 9.51e-07\n",
      "step: 360/2500 train loss: 0.94, LR: 9.51e-07\n",
      "step: 361/2500 train loss: 0.83, LR: 9.50e-07\n",
      "step: 362/2500 train loss: 0.82, LR: 9.50e-07\n",
      "step: 363/2500 train loss: 0.89, LR: 9.49e-07\n",
      "step: 364/2500 train loss: 0.83, LR: 9.49e-07\n",
      "step: 365/2500 train loss: 0.97, LR: 9.48e-07\n",
      "step: 366/2500 train loss: 0.89, LR: 9.48e-07\n",
      "step: 367/2500 train loss: 0.82, LR: 9.48e-07\n",
      "step: 368/2500 train loss: 0.86, LR: 9.47e-07\n",
      "step: 369/2500 train loss: 0.8, LR: 9.47e-07\n",
      "step: 370/2500 train loss: 0.81, LR: 9.46e-07\n",
      "step: 371/2500 train loss: 0.91, LR: 9.46e-07\n",
      "step: 372/2500 train loss: 0.84, LR: 9.45e-07\n",
      "step: 373/2500 train loss: 0.83, LR: 9.45e-07\n",
      "step: 374/2500 train loss: 0.77, LR: 9.44e-07\n",
      "step: 375/2500 train loss: 0.8, LR: 9.44e-07\n",
      "step: 376/2500 train loss: 0.91, LR: 9.44e-07\n",
      "step: 377/2500 train loss: 0.73, LR: 9.43e-07\n",
      "step: 378/2500 train loss: 0.81, LR: 9.43e-07\n",
      "step: 379/2500 train loss: 0.85, LR: 9.42e-07\n",
      "step: 380/2500 train loss: 0.76, LR: 9.42e-07\n",
      "step: 381/2500 train loss: 0.86, LR: 9.41e-07\n",
      "step: 382/2500 train loss: 0.83, LR: 9.41e-07\n",
      "step: 383/2500 train loss: 0.83, LR: 9.40e-07\n",
      "step: 384/2500 train loss: 0.79, LR: 9.40e-07\n",
      "step: 385/2500 train loss: 0.84, LR: 9.40e-07\n",
      "step: 386/2500 train loss: 0.86, LR: 9.39e-07\n",
      "step: 387/2500 train loss: 0.83, LR: 9.39e-07\n",
      "step: 388/2500 train loss: 0.77, LR: 9.38e-07\n",
      "step: 389/2500 train loss: 0.86, LR: 9.38e-07\n",
      "step: 390/2500 train loss: 0.81, LR: 9.37e-07\n",
      "step: 391/2500 train loss: 0.82, LR: 9.37e-07\n",
      "step: 392/2500 train loss: 0.79, LR: 9.36e-07\n",
      "step: 393/2500 train loss: 0.78, LR: 9.36e-07\n",
      "step: 394/2500 train loss: 0.92, LR: 9.36e-07\n",
      "step: 395/2500 train loss: 0.83, LR: 9.35e-07\n",
      "step: 396/2500 train loss: 0.75, LR: 9.35e-07\n",
      "step: 397/2500 train loss: 0.83, LR: 9.34e-07\n",
      "step: 398/2500 train loss: 0.81, LR: 9.34e-07\n",
      "step: 399/2500 train loss: 0.78, LR: 9.33e-07\n",
      "step: 400/2500 train loss: 0.81, LR: 9.33e-07\n",
      "step: 401/2500 train loss: 0.96, LR: 9.32e-07\n",
      "step: 402/2500 train loss: 0.84, LR: 9.32e-07\n",
      "step: 403/2500 train loss: 0.85, LR: 9.32e-07\n",
      "step: 404/2500 train loss: 0.8, LR: 9.31e-07\n",
      "step: 405/2500 train loss: 0.85, LR: 9.31e-07\n",
      "step: 406/2500 train loss: 0.79, LR: 9.30e-07\n",
      "step: 407/2500 train loss: 0.82, LR: 9.30e-07\n",
      "step: 408/2500 train loss: 0.86, LR: 9.29e-07\n",
      "step: 409/2500 train loss: 0.81, LR: 9.29e-07\n",
      "step: 410/2500 train loss: 0.79, LR: 9.28e-07\n",
      "step: 411/2500 train loss: 0.86, LR: 9.28e-07\n",
      "step: 412/2500 train loss: 0.81, LR: 9.28e-07\n",
      "step: 413/2500 train loss: 0.88, LR: 9.27e-07\n",
      "step: 414/2500 train loss: 0.82, LR: 9.27e-07\n",
      "step: 415/2500 train loss: 0.85, LR: 9.26e-07\n",
      "step: 416/2500 train loss: 0.95, LR: 9.26e-07\n",
      "step: 417/2500 train loss: 0.78, LR: 9.25e-07\n",
      "step: 418/2500 train loss: 0.82, LR: 9.25e-07\n",
      "step: 419/2500 train loss: 0.85, LR: 9.24e-07\n",
      "step: 420/2500 train loss: 0.8, LR: 9.24e-07\n",
      "step: 421/2500 train loss: 0.78, LR: 9.24e-07\n",
      "step: 422/2500 train loss: 0.88, LR: 9.23e-07\n",
      "step: 423/2500 train loss: 0.81, LR: 9.23e-07\n",
      "step: 424/2500 train loss: 0.75, LR: 9.22e-07\n",
      "step: 425/2500 train loss: 0.83, LR: 9.22e-07\n",
      "step: 426/2500 train loss: 0.89, LR: 9.21e-07\n",
      "step: 427/2500 train loss: 0.85, LR: 9.21e-07\n",
      "step: 428/2500 train loss: 0.93, LR: 9.20e-07\n",
      "step: 429/2500 train loss: 0.84, LR: 9.20e-07\n",
      "step: 430/2500 train loss: 0.77, LR: 9.20e-07\n",
      "step: 431/2500 train loss: 0.8, LR: 9.19e-07\n",
      "step: 432/2500 train loss: 0.77, LR: 9.19e-07\n",
      "step: 433/2500 train loss: 0.9, LR: 9.18e-07\n",
      "step: 434/2500 train loss: 0.86, LR: 9.18e-07\n",
      "step: 435/2500 train loss: 0.89, LR: 9.17e-07\n",
      "step: 436/2500 train loss: 0.99, LR: 9.17e-07\n",
      "step: 437/2500 train loss: 0.78, LR: 9.16e-07\n",
      "step: 438/2500 train loss: 0.78, LR: 9.16e-07\n",
      "step: 439/2500 train loss: 0.79, LR: 9.16e-07\n",
      "step: 440/2500 train loss: 0.8, LR: 9.15e-07\n",
      "step: 441/2500 train loss: 0.88, LR: 9.15e-07\n",
      "step: 442/2500 train loss: 0.81, LR: 9.14e-07\n",
      "step: 443/2500 train loss: 0.85, LR: 9.14e-07\n",
      "step: 444/2500 train loss: 0.77, LR: 9.13e-07\n",
      "step: 445/2500 train loss: 0.84, LR: 9.13e-07\n",
      "step: 446/2500 train loss: 0.87, LR: 9.12e-07\n",
      "step: 447/2500 train loss: 0.77, LR: 9.12e-07\n",
      "step: 448/2500 train loss: 0.78, LR: 9.12e-07\n",
      "step: 449/2500 train loss: 0.87, LR: 9.11e-07\n",
      "step: 450/2500 train loss: 0.81, LR: 9.11e-07\n",
      "step: 451/2500 train loss: 0.86, LR: 9.10e-07\n",
      "step: 452/2500 train loss: 0.79, LR: 9.10e-07\n",
      "step: 453/2500 train loss: 0.88, LR: 9.09e-07\n",
      "step: 454/2500 train loss: 0.74, LR: 9.09e-07\n",
      "step: 455/2500 train loss: 0.79, LR: 9.08e-07\n",
      "step: 456/2500 train loss: 0.79, LR: 9.08e-07\n",
      "step: 457/2500 train loss: 0.83, LR: 9.08e-07\n",
      "step: 458/2500 train loss: 0.78, LR: 9.07e-07\n",
      "step: 459/2500 train loss: 0.76, LR: 9.07e-07\n",
      "step: 460/2500 train loss: 0.89, LR: 9.06e-07\n",
      "step: 461/2500 train loss: 0.73, LR: 9.06e-07\n",
      "step: 462/2500 train loss: 0.78, LR: 9.05e-07\n",
      "step: 463/2500 train loss: 0.93, LR: 9.05e-07\n",
      "step: 464/2500 train loss: 0.79, LR: 9.04e-07\n",
      "step: 465/2500 train loss: 0.81, LR: 9.04e-07\n",
      "step: 466/2500 train loss: 0.75, LR: 9.04e-07\n",
      "step: 467/2500 train loss: 0.79, LR: 9.03e-07\n",
      "step: 468/2500 train loss: 0.81, LR: 9.03e-07\n",
      "step: 469/2500 train loss: 0.86, LR: 9.02e-07\n",
      "step: 470/2500 train loss: 0.79, LR: 9.02e-07\n",
      "step: 471/2500 train loss: 0.87, LR: 9.01e-07\n",
      "step: 472/2500 train loss: 0.71, LR: 9.01e-07\n",
      "step: 473/2500 train loss: 0.86, LR: 9.00e-07\n",
      "step: 474/2500 train loss: 0.83, LR: 9.00e-07\n",
      "step: 475/2500 train loss: 0.84, LR: 9.00e-07\n",
      "step: 476/2500 train loss: 0.85, LR: 8.99e-07\n",
      "step: 477/2500 train loss: 0.72, LR: 8.99e-07\n",
      "step: 478/2500 train loss: 0.89, LR: 8.98e-07\n",
      "step: 479/2500 train loss: 0.73, LR: 8.98e-07\n",
      "step: 480/2500 train loss: 0.82, LR: 8.97e-07\n",
      "step: 481/2500 train loss: 0.75, LR: 8.97e-07\n",
      "step: 482/2500 train loss: 0.83, LR: 8.96e-07\n",
      "step: 483/2500 train loss: 0.84, LR: 8.96e-07\n",
      "step: 484/2500 train loss: 0.74, LR: 8.96e-07\n",
      "step: 485/2500 train loss: 0.79, LR: 8.95e-07\n",
      "step: 486/2500 train loss: 0.79, LR: 8.95e-07\n",
      "step: 487/2500 train loss: 0.84, LR: 8.94e-07\n",
      "step: 488/2500 train loss: 0.79, LR: 8.94e-07\n",
      "step: 489/2500 train loss: 0.83, LR: 8.93e-07\n",
      "step: 490/2500 train loss: 0.78, LR: 8.93e-07\n",
      "step: 491/2500 train loss: 0.77, LR: 8.92e-07\n",
      "step: 492/2500 train loss: 0.84, LR: 8.92e-07\n",
      "step: 493/2500 train loss: 0.81, LR: 8.92e-07\n",
      "step: 494/2500 train loss: 0.77, LR: 8.91e-07\n",
      "step: 495/2500 train loss: 0.82, LR: 8.91e-07\n",
      "step: 496/2500 train loss: 0.79, LR: 8.90e-07\n",
      "step: 497/2500 train loss: 0.84, LR: 8.90e-07\n",
      "step: 498/2500 train loss: 0.82, LR: 8.89e-07\n",
      "step: 499/2500 train loss: 0.77, LR: 8.89e-07\n",
      "step: 500/2500 eval loss: 0.79\n",
      "step: 500/2500 train loss: 0.76, LR: 8.88e-07\n",
      "step: 501/2500 train loss: 0.83, LR: 8.88e-07\n",
      "step: 502/2500 train loss: 0.81, LR: 8.88e-07\n",
      "step: 503/2500 train loss: 0.73, LR: 8.87e-07\n",
      "step: 504/2500 train loss: 0.81, LR: 8.87e-07\n",
      "step: 505/2500 train loss: 0.79, LR: 8.86e-07\n",
      "step: 506/2500 train loss: 0.84, LR: 8.86e-07\n",
      "step: 507/2500 train loss: 0.77, LR: 8.85e-07\n",
      "step: 508/2500 train loss: 0.83, LR: 8.85e-07\n",
      "step: 509/2500 train loss: 0.78, LR: 8.84e-07\n",
      "step: 510/2500 train loss: 0.74, LR: 8.84e-07\n",
      "step: 511/2500 train loss: 0.87, LR: 8.84e-07\n",
      "step: 512/2500 train loss: 0.74, LR: 8.83e-07\n",
      "step: 513/2500 train loss: 0.81, LR: 8.83e-07\n",
      "step: 514/2500 train loss: 0.74, LR: 8.82e-07\n",
      "step: 515/2500 train loss: 0.82, LR: 8.82e-07\n",
      "step: 516/2500 train loss: 0.8, LR: 8.81e-07\n",
      "step: 517/2500 train loss: 0.96, LR: 8.81e-07\n",
      "step: 518/2500 train loss: 0.82, LR: 8.80e-07\n",
      "step: 519/2500 train loss: 0.81, LR: 8.80e-07\n",
      "step: 520/2500 train loss: 0.81, LR: 8.80e-07\n",
      "step: 521/2500 train loss: 0.75, LR: 8.79e-07\n",
      "step: 522/2500 train loss: 0.81, LR: 8.79e-07\n",
      "step: 523/2500 train loss: 0.8, LR: 8.78e-07\n",
      "step: 524/2500 train loss: 0.75, LR: 8.78e-07\n",
      "step: 525/2500 train loss: 0.84, LR: 8.77e-07\n",
      "step: 526/2500 train loss: 0.76, LR: 8.77e-07\n",
      "step: 527/2500 train loss: 0.76, LR: 8.76e-07\n",
      "step: 528/2500 train loss: 0.78, LR: 8.76e-07\n",
      "step: 529/2500 train loss: 0.76, LR: 8.76e-07\n",
      "step: 530/2500 train loss: 0.82, LR: 8.75e-07\n",
      "step: 531/2500 train loss: 0.74, LR: 8.75e-07\n",
      "step: 532/2500 train loss: 0.79, LR: 8.74e-07\n",
      "step: 533/2500 train loss: 0.78, LR: 8.74e-07\n",
      "step: 534/2500 train loss: 0.82, LR: 8.73e-07\n",
      "step: 535/2500 train loss: 0.73, LR: 8.73e-07\n",
      "step: 536/2500 train loss: 0.79, LR: 8.72e-07\n",
      "step: 537/2500 train loss: 0.72, LR: 8.72e-07\n",
      "step: 538/2500 train loss: 0.74, LR: 8.72e-07\n",
      "step: 539/2500 train loss: 0.86, LR: 8.71e-07\n",
      "step: 540/2500 train loss: 0.78, LR: 8.71e-07\n",
      "step: 541/2500 train loss: 0.73, LR: 8.70e-07\n",
      "step: 542/2500 train loss: 0.73, LR: 8.70e-07\n",
      "step: 543/2500 train loss: 0.85, LR: 8.69e-07\n",
      "step: 544/2500 train loss: 0.8, LR: 8.69e-07\n",
      "step: 545/2500 train loss: 0.8, LR: 8.68e-07\n",
      "step: 546/2500 train loss: 0.82, LR: 8.68e-07\n",
      "step: 547/2500 train loss: 0.75, LR: 8.68e-07\n",
      "step: 548/2500 train loss: 0.77, LR: 8.67e-07\n",
      "step: 549/2500 train loss: 0.87, LR: 8.67e-07\n",
      "step: 550/2500 train loss: 0.82, LR: 8.66e-07\n",
      "step: 551/2500 train loss: 0.83, LR: 8.66e-07\n",
      "step: 552/2500 train loss: 0.74, LR: 8.65e-07\n",
      "step: 553/2500 train loss: 0.76, LR: 8.65e-07\n",
      "step: 554/2500 train loss: 0.81, LR: 8.64e-07\n",
      "step: 555/2500 train loss: 0.7, LR: 8.64e-07\n",
      "step: 556/2500 train loss: 0.75, LR: 8.64e-07\n",
      "step: 557/2500 train loss: 0.72, LR: 8.63e-07\n",
      "step: 558/2500 train loss: 0.72, LR: 8.63e-07\n",
      "step: 559/2500 train loss: 0.78, LR: 8.62e-07\n",
      "step: 560/2500 train loss: 0.77, LR: 8.62e-07\n",
      "step: 561/2500 train loss: 0.71, LR: 8.61e-07\n",
      "step: 562/2500 train loss: 0.8, LR: 8.61e-07\n",
      "step: 563/2500 train loss: 0.74, LR: 8.60e-07\n",
      "step: 564/2500 train loss: 0.77, LR: 8.60e-07\n",
      "step: 565/2500 train loss: 0.75, LR: 8.60e-07\n",
      "step: 566/2500 train loss: 0.77, LR: 8.59e-07\n",
      "step: 567/2500 train loss: 0.79, LR: 8.59e-07\n",
      "step: 568/2500 train loss: 0.87, LR: 8.58e-07\n",
      "step: 569/2500 train loss: 0.76, LR: 8.58e-07\n",
      "step: 570/2500 train loss: 0.68, LR: 8.57e-07\n",
      "step: 571/2500 train loss: 0.78, LR: 8.57e-07\n",
      "step: 572/2500 train loss: 0.81, LR: 8.56e-07\n",
      "step: 573/2500 train loss: 0.84, LR: 8.56e-07\n",
      "step: 574/2500 train loss: 0.81, LR: 8.56e-07\n",
      "step: 575/2500 train loss: 0.79, LR: 8.55e-07\n",
      "step: 576/2500 train loss: 0.73, LR: 8.55e-07\n",
      "step: 577/2500 train loss: 0.82, LR: 8.54e-07\n",
      "step: 578/2500 train loss: 0.75, LR: 8.54e-07\n",
      "step: 579/2500 train loss: 0.79, LR: 8.53e-07\n",
      "step: 580/2500 train loss: 0.9, LR: 8.53e-07\n",
      "step: 581/2500 train loss: 0.81, LR: 8.52e-07\n",
      "step: 582/2500 train loss: 0.83, LR: 8.52e-07\n",
      "step: 583/2500 train loss: 0.89, LR: 8.52e-07\n",
      "step: 584/2500 train loss: 0.82, LR: 8.51e-07\n",
      "step: 585/2500 train loss: 0.82, LR: 8.51e-07\n",
      "step: 586/2500 train loss: 0.88, LR: 8.50e-07\n",
      "step: 587/2500 train loss: 0.8, LR: 8.50e-07\n",
      "step: 588/2500 train loss: 0.82, LR: 8.49e-07\n",
      "step: 589/2500 train loss: 0.8, LR: 8.49e-07\n",
      "step: 590/2500 train loss: 0.95, LR: 8.48e-07\n",
      "step: 591/2500 train loss: 0.69, LR: 8.48e-07\n",
      "step: 592/2500 train loss: 0.76, LR: 8.48e-07\n",
      "step: 593/2500 train loss: 0.78, LR: 8.47e-07\n",
      "step: 594/2500 train loss: 0.74, LR: 8.47e-07\n",
      "step: 595/2500 train loss: 0.84, LR: 8.46e-07\n",
      "step: 596/2500 train loss: 0.93, LR: 8.46e-07\n",
      "step: 597/2500 train loss: 0.71, LR: 8.45e-07\n",
      "step: 598/2500 train loss: 0.71, LR: 8.45e-07\n",
      "step: 599/2500 train loss: 0.81, LR: 8.44e-07\n",
      "step: 600/2500 train loss: 0.72, LR: 8.44e-07\n",
      "step: 601/2500 train loss: 0.9, LR: 8.44e-07\n",
      "step: 602/2500 train loss: 0.77, LR: 8.43e-07\n",
      "step: 603/2500 train loss: 0.77, LR: 8.43e-07\n",
      "step: 604/2500 train loss: 0.78, LR: 8.42e-07\n",
      "step: 605/2500 train loss: 0.77, LR: 8.42e-07\n",
      "step: 606/2500 train loss: 0.81, LR: 8.41e-07\n",
      "step: 607/2500 train loss: 0.8, LR: 8.41e-07\n",
      "step: 608/2500 train loss: 0.85, LR: 8.40e-07\n",
      "step: 609/2500 train loss: 0.84, LR: 8.40e-07\n",
      "step: 610/2500 train loss: 0.79, LR: 8.40e-07\n",
      "step: 611/2500 train loss: 0.73, LR: 8.39e-07\n",
      "step: 612/2500 train loss: 0.75, LR: 8.39e-07\n",
      "step: 613/2500 train loss: 0.85, LR: 8.38e-07\n",
      "step: 614/2500 train loss: 0.77, LR: 8.38e-07\n",
      "step: 615/2500 train loss: 0.82, LR: 8.37e-07\n",
      "step: 616/2500 train loss: 0.77, LR: 8.37e-07\n",
      "step: 617/2500 train loss: 0.72, LR: 8.36e-07\n",
      "step: 618/2500 train loss: 0.77, LR: 8.36e-07\n",
      "step: 619/2500 train loss: 0.78, LR: 8.36e-07\n",
      "step: 620/2500 train loss: 0.8, LR: 8.35e-07\n",
      "step: 621/2500 train loss: 0.8, LR: 8.35e-07\n",
      "step: 622/2500 train loss: 0.81, LR: 8.34e-07\n",
      "step: 623/2500 train loss: 0.79, LR: 8.34e-07\n",
      "step: 624/2500 train loss: 0.84, LR: 8.33e-07\n",
      "step: 625/2500 train loss: 0.7, LR: 8.33e-07\n",
      "step: 626/2500 train loss: 0.81, LR: 8.32e-07\n",
      "step: 627/2500 train loss: 0.79, LR: 8.32e-07\n",
      "step: 628/2500 train loss: 0.74, LR: 8.32e-07\n",
      "step: 629/2500 train loss: 0.81, LR: 8.31e-07\n",
      "step: 630/2500 train loss: 0.81, LR: 8.31e-07\n",
      "step: 631/2500 train loss: 0.76, LR: 8.30e-07\n",
      "step: 632/2500 train loss: 0.77, LR: 8.30e-07\n",
      "step: 633/2500 train loss: 0.74, LR: 8.29e-07\n",
      "step: 634/2500 train loss: 0.76, LR: 8.29e-07\n",
      "step: 635/2500 train loss: 0.77, LR: 8.28e-07\n",
      "step: 636/2500 train loss: 0.72, LR: 8.28e-07\n",
      "step: 637/2500 train loss: 0.79, LR: 8.28e-07\n",
      "step: 638/2500 train loss: 0.79, LR: 8.27e-07\n",
      "step: 639/2500 train loss: 0.74, LR: 8.27e-07\n",
      "step: 640/2500 train loss: 0.8, LR: 8.26e-07\n",
      "step: 641/2500 train loss: 0.74, LR: 8.26e-07\n",
      "step: 642/2500 train loss: 0.76, LR: 8.25e-07\n",
      "step: 643/2500 train loss: 0.75, LR: 8.25e-07\n",
      "step: 644/2500 train loss: 0.76, LR: 8.24e-07\n",
      "step: 645/2500 train loss: 0.76, LR: 8.24e-07\n",
      "step: 646/2500 train loss: 0.74, LR: 8.24e-07\n",
      "step: 647/2500 train loss: 0.8, LR: 8.23e-07\n",
      "step: 648/2500 train loss: 0.75, LR: 8.23e-07\n",
      "step: 649/2500 train loss: 0.71, LR: 8.22e-07\n",
      "step: 650/2500 train loss: 0.74, LR: 8.22e-07\n",
      "step: 651/2500 train loss: 0.77, LR: 8.21e-07\n",
      "step: 652/2500 train loss: 0.74, LR: 8.21e-07\n",
      "step: 653/2500 train loss: 0.88, LR: 8.20e-07\n",
      "step: 654/2500 train loss: 0.82, LR: 8.20e-07\n",
      "step: 655/2500 train loss: 0.7, LR: 8.20e-07\n",
      "step: 656/2500 train loss: 0.82, LR: 8.19e-07\n",
      "step: 657/2500 train loss: 0.79, LR: 8.19e-07\n",
      "step: 658/2500 train loss: 0.74, LR: 8.18e-07\n",
      "step: 659/2500 train loss: 0.75, LR: 8.18e-07\n",
      "step: 660/2500 train loss: 0.72, LR: 8.17e-07\n",
      "step: 661/2500 train loss: 0.71, LR: 8.17e-07\n",
      "step: 662/2500 train loss: 0.8, LR: 8.16e-07\n",
      "step: 663/2500 train loss: 0.79, LR: 8.16e-07\n",
      "step: 664/2500 train loss: 0.71, LR: 8.16e-07\n",
      "step: 665/2500 train loss: 0.76, LR: 8.15e-07\n",
      "step: 666/2500 train loss: 0.76, LR: 8.15e-07\n",
      "step: 667/2500 train loss: 0.8, LR: 8.14e-07\n",
      "step: 668/2500 train loss: 0.71, LR: 8.14e-07\n",
      "step: 669/2500 train loss: 0.65, LR: 8.13e-07\n",
      "step: 670/2500 train loss: 0.81, LR: 8.13e-07\n",
      "step: 671/2500 train loss: 0.72, LR: 8.12e-07\n",
      "step: 672/2500 train loss: 0.78, LR: 8.12e-07\n",
      "step: 673/2500 train loss: 0.83, LR: 8.12e-07\n",
      "step: 674/2500 train loss: 0.77, LR: 8.11e-07\n",
      "step: 675/2500 train loss: 0.67, LR: 8.11e-07\n",
      "step: 676/2500 train loss: 0.85, LR: 8.10e-07\n",
      "step: 677/2500 train loss: 0.76, LR: 8.10e-07\n",
      "step: 678/2500 train loss: 0.72, LR: 8.09e-07\n",
      "step: 679/2500 train loss: 0.76, LR: 8.09e-07\n",
      "step: 680/2500 train loss: 0.75, LR: 8.08e-07\n",
      "step: 681/2500 train loss: 0.82, LR: 8.08e-07\n",
      "step: 682/2500 train loss: 0.77, LR: 8.08e-07\n",
      "step: 683/2500 train loss: 0.77, LR: 8.07e-07\n",
      "step: 684/2500 train loss: 0.74, LR: 8.07e-07\n",
      "step: 685/2500 train loss: 0.77, LR: 8.06e-07\n",
      "step: 686/2500 train loss: 0.88, LR: 8.06e-07\n",
      "step: 687/2500 train loss: 0.81, LR: 8.05e-07\n",
      "step: 688/2500 train loss: 0.73, LR: 8.05e-07\n",
      "step: 689/2500 train loss: 0.8, LR: 8.04e-07\n",
      "step: 690/2500 train loss: 0.8, LR: 8.04e-07\n",
      "step: 691/2500 train loss: 0.87, LR: 8.04e-07\n",
      "step: 692/2500 train loss: 0.8, LR: 8.03e-07\n",
      "step: 693/2500 train loss: 0.71, LR: 8.03e-07\n",
      "step: 694/2500 train loss: 0.81, LR: 8.02e-07\n",
      "step: 695/2500 train loss: 0.75, LR: 8.02e-07\n",
      "step: 696/2500 train loss: 0.79, LR: 8.01e-07\n",
      "step: 697/2500 train loss: 0.72, LR: 8.01e-07\n",
      "step: 698/2500 train loss: 0.76, LR: 8.00e-07\n",
      "step: 699/2500 train loss: 0.74, LR: 8.00e-07\n",
      "step: 700/2500 train loss: 0.78, LR: 8.00e-07\n",
      "step: 701/2500 train loss: 0.76, LR: 7.99e-07\n",
      "step: 702/2500 train loss: 0.81, LR: 7.99e-07\n",
      "step: 703/2500 train loss: 0.76, LR: 7.98e-07\n",
      "step: 704/2500 train loss: 0.72, LR: 7.98e-07\n",
      "step: 705/2500 train loss: 0.81, LR: 7.97e-07\n",
      "step: 706/2500 train loss: 0.74, LR: 7.97e-07\n",
      "step: 707/2500 train loss: 0.78, LR: 7.96e-07\n",
      "step: 708/2500 train loss: 0.79, LR: 7.96e-07\n",
      "step: 709/2500 train loss: 0.76, LR: 7.96e-07\n",
      "step: 710/2500 train loss: 0.71, LR: 7.95e-07\n",
      "step: 711/2500 train loss: 0.74, LR: 7.95e-07\n",
      "step: 712/2500 train loss: 0.78, LR: 7.94e-07\n",
      "step: 713/2500 train loss: 0.78, LR: 7.94e-07\n",
      "step: 714/2500 train loss: 0.66, LR: 7.93e-07\n",
      "step: 715/2500 train loss: 0.78, LR: 7.93e-07\n",
      "step: 716/2500 train loss: 0.78, LR: 7.92e-07\n",
      "step: 717/2500 train loss: 0.76, LR: 7.92e-07\n",
      "step: 718/2500 train loss: 0.77, LR: 7.92e-07\n",
      "step: 719/2500 train loss: 0.73, LR: 7.91e-07\n",
      "step: 720/2500 train loss: 0.76, LR: 7.91e-07\n",
      "step: 721/2500 train loss: 0.79, LR: 7.90e-07\n",
      "step: 722/2500 train loss: 0.8, LR: 7.90e-07\n",
      "step: 723/2500 train loss: 0.73, LR: 7.89e-07\n",
      "step: 724/2500 train loss: 0.7, LR: 7.89e-07\n",
      "step: 725/2500 train loss: 0.73, LR: 7.88e-07\n",
      "step: 726/2500 train loss: 0.75, LR: 7.88e-07\n",
      "step: 727/2500 train loss: 0.73, LR: 7.88e-07\n",
      "step: 728/2500 train loss: 0.69, LR: 7.87e-07\n",
      "step: 729/2500 train loss: 0.68, LR: 7.87e-07\n",
      "step: 730/2500 train loss: 0.82, LR: 7.86e-07\n",
      "step: 731/2500 train loss: 0.79, LR: 7.86e-07\n",
      "step: 732/2500 train loss: 0.77, LR: 7.85e-07\n",
      "step: 733/2500 train loss: 0.65, LR: 7.85e-07\n",
      "step: 734/2500 train loss: 0.83, LR: 7.84e-07\n",
      "step: 735/2500 train loss: 0.72, LR: 7.84e-07\n",
      "step: 736/2500 train loss: 0.77, LR: 7.84e-07\n",
      "step: 737/2500 train loss: 0.77, LR: 7.83e-07\n",
      "step: 738/2500 train loss: 0.71, LR: 7.83e-07\n",
      "step: 739/2500 train loss: 0.81, LR: 7.82e-07\n",
      "step: 740/2500 train loss: 0.77, LR: 7.82e-07\n",
      "step: 741/2500 train loss: 0.74, LR: 7.81e-07\n",
      "step: 742/2500 train loss: 0.75, LR: 7.81e-07\n",
      "step: 743/2500 train loss: 0.71, LR: 7.80e-07\n",
      "step: 744/2500 train loss: 0.79, LR: 7.80e-07\n",
      "step: 745/2500 train loss: 0.69, LR: 7.80e-07\n",
      "step: 746/2500 train loss: 0.75, LR: 7.79e-07\n",
      "step: 747/2500 train loss: 0.63, LR: 7.79e-07\n",
      "step: 748/2500 train loss: 0.72, LR: 7.78e-07\n",
      "step: 749/2500 train loss: 0.82, LR: 7.78e-07\n",
      "step: 750/2500 eval loss: 0.74\n",
      "step: 750/2500 train loss: 0.81, LR: 7.77e-07\n",
      "step: 751/2500 train loss: 0.72, LR: 7.77e-07\n",
      "step: 752/2500 train loss: 0.78, LR: 7.76e-07\n",
      "step: 753/2500 train loss: 0.75, LR: 7.76e-07\n",
      "step: 754/2500 train loss: 0.7, LR: 7.76e-07\n",
      "step: 755/2500 train loss: 0.78, LR: 7.75e-07\n",
      "step: 756/2500 train loss: 0.73, LR: 7.75e-07\n",
      "step: 757/2500 train loss: 0.8, LR: 7.74e-07\n",
      "step: 758/2500 train loss: 0.74, LR: 7.74e-07\n",
      "step: 759/2500 train loss: 0.67, LR: 7.73e-07\n",
      "step: 760/2500 train loss: 0.68, LR: 7.73e-07\n",
      "step: 761/2500 train loss: 0.74, LR: 7.72e-07\n",
      "step: 762/2500 train loss: 0.7, LR: 7.72e-07\n",
      "step: 763/2500 train loss: 0.76, LR: 7.72e-07\n",
      "step: 764/2500 train loss: 0.74, LR: 7.71e-07\n",
      "step: 765/2500 train loss: 0.67, LR: 7.71e-07\n",
      "step: 766/2500 train loss: 0.82, LR: 7.70e-07\n",
      "step: 767/2500 train loss: 0.72, LR: 7.70e-07\n",
      "step: 768/2500 train loss: 0.74, LR: 7.69e-07\n",
      "step: 769/2500 train loss: 0.68, LR: 7.69e-07\n",
      "step: 770/2500 train loss: 0.77, LR: 7.68e-07\n",
      "step: 771/2500 train loss: 0.69, LR: 7.68e-07\n",
      "step: 772/2500 train loss: 0.81, LR: 7.68e-07\n",
      "step: 773/2500 train loss: 0.78, LR: 7.67e-07\n",
      "step: 774/2500 train loss: 0.76, LR: 7.67e-07\n",
      "step: 775/2500 train loss: 0.68, LR: 7.66e-07\n",
      "step: 776/2500 train loss: 0.77, LR: 7.66e-07\n",
      "step: 777/2500 train loss: 0.77, LR: 7.65e-07\n",
      "step: 778/2500 train loss: 0.71, LR: 7.65e-07\n",
      "step: 779/2500 train loss: 0.81, LR: 7.64e-07\n",
      "step: 780/2500 train loss: 0.73, LR: 7.64e-07\n",
      "step: 781/2500 train loss: 0.8, LR: 7.64e-07\n",
      "step: 782/2500 train loss: 0.66, LR: 7.63e-07\n",
      "step: 783/2500 train loss: 0.81, LR: 7.63e-07\n",
      "step: 784/2500 train loss: 0.78, LR: 7.62e-07\n",
      "step: 785/2500 train loss: 0.74, LR: 7.62e-07\n",
      "step: 786/2500 train loss: 0.83, LR: 7.61e-07\n",
      "step: 787/2500 train loss: 0.67, LR: 7.61e-07\n",
      "step: 788/2500 train loss: 0.76, LR: 7.60e-07\n",
      "step: 789/2500 train loss: 0.7, LR: 7.60e-07\n",
      "step: 790/2500 train loss: 0.72, LR: 7.60e-07\n",
      "step: 791/2500 train loss: 0.76, LR: 7.59e-07\n",
      "step: 792/2500 train loss: 0.73, LR: 7.59e-07\n",
      "step: 793/2500 train loss: 0.73, LR: 7.58e-07\n",
      "step: 794/2500 train loss: 0.76, LR: 7.58e-07\n",
      "step: 795/2500 train loss: 0.7, LR: 7.57e-07\n",
      "step: 796/2500 train loss: 0.7, LR: 7.57e-07\n",
      "step: 797/2500 train loss: 0.8, LR: 7.56e-07\n",
      "step: 798/2500 train loss: 0.65, LR: 7.56e-07\n",
      "step: 799/2500 train loss: 0.69, LR: 7.56e-07\n",
      "step: 800/2500 train loss: 0.79, LR: 7.55e-07\n",
      "step: 801/2500 train loss: 0.66, LR: 7.55e-07\n",
      "step: 802/2500 train loss: 0.84, LR: 7.54e-07\n",
      "step: 803/2500 train loss: 0.66, LR: 7.54e-07\n",
      "step: 804/2500 train loss: 0.8, LR: 7.53e-07\n",
      "step: 805/2500 train loss: 0.75, LR: 7.53e-07\n",
      "step: 806/2500 train loss: 0.71, LR: 7.52e-07\n",
      "step: 807/2500 train loss: 0.73, LR: 7.52e-07\n",
      "step: 808/2500 train loss: 0.76, LR: 7.52e-07\n",
      "step: 809/2500 train loss: 0.72, LR: 7.51e-07\n",
      "step: 810/2500 train loss: 0.84, LR: 7.51e-07\n",
      "step: 811/2500 train loss: 0.77, LR: 7.50e-07\n",
      "step: 812/2500 train loss: 0.8, LR: 7.50e-07\n",
      "step: 813/2500 train loss: 0.78, LR: 7.49e-07\n",
      "step: 814/2500 train loss: 0.77, LR: 7.49e-07\n",
      "step: 815/2500 train loss: 0.66, LR: 7.48e-07\n",
      "step: 816/2500 train loss: 0.69, LR: 7.48e-07\n",
      "step: 817/2500 train loss: 0.7, LR: 7.48e-07\n",
      "step: 818/2500 train loss: 0.78, LR: 7.47e-07\n",
      "step: 819/2500 train loss: 0.7, LR: 7.47e-07\n",
      "step: 820/2500 train loss: 0.83, LR: 7.46e-07\n",
      "step: 821/2500 train loss: 0.75, LR: 7.46e-07\n",
      "step: 822/2500 train loss: 0.75, LR: 7.45e-07\n",
      "step: 823/2500 train loss: 0.74, LR: 7.45e-07\n",
      "step: 824/2500 train loss: 0.77, LR: 7.44e-07\n",
      "step: 825/2500 train loss: 0.79, LR: 7.44e-07\n",
      "step: 826/2500 train loss: 0.8, LR: 7.44e-07\n",
      "step: 827/2500 train loss: 0.77, LR: 7.43e-07\n",
      "step: 828/2500 train loss: 0.8, LR: 7.43e-07\n",
      "step: 829/2500 train loss: 0.75, LR: 7.42e-07\n",
      "step: 830/2500 train loss: 0.82, LR: 7.42e-07\n",
      "step: 831/2500 train loss: 0.77, LR: 7.41e-07\n",
      "step: 832/2500 train loss: 0.69, LR: 7.41e-07\n",
      "step: 833/2500 train loss: 0.85, LR: 7.40e-07\n",
      "step: 834/2500 train loss: 0.79, LR: 7.40e-07\n",
      "step: 835/2500 train loss: 0.78, LR: 7.40e-07\n",
      "step: 836/2500 train loss: 0.75, LR: 7.39e-07\n",
      "step: 837/2500 train loss: 0.78, LR: 7.39e-07\n",
      "step: 838/2500 train loss: 0.77, LR: 7.38e-07\n",
      "step: 839/2500 train loss: 0.67, LR: 7.38e-07\n",
      "step: 840/2500 train loss: 0.75, LR: 7.37e-07\n",
      "step: 841/2500 train loss: 0.81, LR: 7.37e-07\n",
      "step: 842/2500 train loss: 0.72, LR: 7.36e-07\n",
      "step: 843/2500 train loss: 0.74, LR: 7.36e-07\n",
      "step: 844/2500 train loss: 0.82, LR: 7.36e-07\n",
      "step: 845/2500 train loss: 0.79, LR: 7.35e-07\n",
      "step: 846/2500 train loss: 0.72, LR: 7.35e-07\n",
      "step: 847/2500 train loss: 0.68, LR: 7.34e-07\n",
      "step: 848/2500 train loss: 0.79, LR: 7.34e-07\n",
      "step: 849/2500 train loss: 0.69, LR: 7.33e-07\n",
      "step: 850/2500 train loss: 0.78, LR: 7.33e-07\n",
      "step: 851/2500 train loss: 0.88, LR: 7.32e-07\n",
      "step: 852/2500 train loss: 0.74, LR: 7.32e-07\n",
      "step: 853/2500 train loss: 0.78, LR: 7.32e-07\n",
      "step: 854/2500 train loss: 0.76, LR: 7.31e-07\n",
      "step: 855/2500 train loss: 0.76, LR: 7.31e-07\n",
      "step: 856/2500 train loss: 0.71, LR: 7.30e-07\n",
      "step: 857/2500 train loss: 0.77, LR: 7.30e-07\n",
      "step: 858/2500 train loss: 0.72, LR: 7.29e-07\n",
      "step: 859/2500 train loss: 0.75, LR: 7.29e-07\n",
      "step: 860/2500 train loss: 0.78, LR: 7.28e-07\n",
      "step: 861/2500 train loss: 0.66, LR: 7.28e-07\n",
      "step: 862/2500 train loss: 0.75, LR: 7.28e-07\n",
      "step: 863/2500 train loss: 0.76, LR: 7.27e-07\n",
      "step: 864/2500 train loss: 0.73, LR: 7.27e-07\n",
      "step: 865/2500 train loss: 0.85, LR: 7.26e-07\n",
      "step: 866/2500 train loss: 0.84, LR: 7.26e-07\n",
      "step: 867/2500 train loss: 0.79, LR: 7.25e-07\n",
      "step: 868/2500 train loss: 0.77, LR: 7.25e-07\n",
      "step: 869/2500 train loss: 0.66, LR: 7.24e-07\n",
      "step: 870/2500 train loss: 0.78, LR: 7.24e-07\n",
      "step: 871/2500 train loss: 0.75, LR: 7.24e-07\n",
      "step: 872/2500 train loss: 0.72, LR: 7.23e-07\n",
      "step: 873/2500 train loss: 0.73, LR: 7.23e-07\n",
      "step: 874/2500 train loss: 0.65, LR: 7.22e-07\n",
      "step: 875/2500 train loss: 0.92, LR: 7.22e-07\n",
      "step: 876/2500 train loss: 0.72, LR: 7.21e-07\n",
      "step: 877/2500 train loss: 0.79, LR: 7.21e-07\n",
      "step: 878/2500 train loss: 0.75, LR: 7.20e-07\n",
      "step: 879/2500 train loss: 0.67, LR: 7.20e-07\n",
      "step: 880/2500 train loss: 0.77, LR: 7.20e-07\n",
      "step: 881/2500 train loss: 0.72, LR: 7.19e-07\n",
      "step: 882/2500 train loss: 0.75, LR: 7.19e-07\n",
      "step: 883/2500 train loss: 0.72, LR: 7.18e-07\n",
      "step: 884/2500 train loss: 0.81, LR: 7.18e-07\n",
      "step: 885/2500 train loss: 0.67, LR: 7.17e-07\n",
      "step: 886/2500 train loss: 0.7, LR: 7.17e-07\n",
      "step: 887/2500 train loss: 0.66, LR: 7.16e-07\n",
      "step: 888/2500 train loss: 0.78, LR: 7.16e-07\n",
      "step: 889/2500 train loss: 0.73, LR: 7.16e-07\n",
      "step: 890/2500 train loss: 0.74, LR: 7.15e-07\n",
      "step: 891/2500 train loss: 0.67, LR: 7.15e-07\n",
      "step: 892/2500 train loss: 0.7, LR: 7.14e-07\n",
      "step: 893/2500 train loss: 0.68, LR: 7.14e-07\n",
      "step: 894/2500 train loss: 0.79, LR: 7.13e-07\n",
      "step: 895/2500 train loss: 0.7, LR: 7.13e-07\n",
      "step: 896/2500 train loss: 0.62, LR: 7.12e-07\n",
      "step: 897/2500 train loss: 0.73, LR: 7.12e-07\n",
      "step: 898/2500 train loss: 0.73, LR: 7.12e-07\n",
      "step: 899/2500 train loss: 0.85, LR: 7.11e-07\n",
      "step: 900/2500 train loss: 0.8, LR: 7.11e-07\n",
      "step: 901/2500 train loss: 0.78, LR: 7.10e-07\n",
      "step: 902/2500 train loss: 0.88, LR: 7.10e-07\n",
      "step: 903/2500 train loss: 0.64, LR: 7.09e-07\n",
      "step: 904/2500 train loss: 0.75, LR: 7.09e-07\n",
      "step: 905/2500 train loss: 0.75, LR: 7.08e-07\n",
      "step: 906/2500 train loss: 0.68, LR: 7.08e-07\n",
      "step: 907/2500 train loss: 0.74, LR: 7.08e-07\n",
      "step: 908/2500 train loss: 0.73, LR: 7.07e-07\n",
      "step: 909/2500 train loss: 0.75, LR: 7.07e-07\n",
      "step: 910/2500 train loss: 0.75, LR: 7.06e-07\n",
      "step: 911/2500 train loss: 0.73, LR: 7.06e-07\n",
      "step: 912/2500 train loss: 0.83, LR: 7.05e-07\n",
      "step: 913/2500 train loss: 0.73, LR: 7.05e-07\n",
      "step: 914/2500 train loss: 0.79, LR: 7.04e-07\n",
      "step: 915/2500 train loss: 0.79, LR: 7.04e-07\n",
      "step: 916/2500 train loss: 0.7, LR: 7.04e-07\n",
      "step: 917/2500 train loss: 0.69, LR: 7.03e-07\n",
      "step: 918/2500 train loss: 0.76, LR: 7.03e-07\n",
      "step: 919/2500 train loss: 0.82, LR: 7.02e-07\n",
      "step: 920/2500 train loss: 0.79, LR: 7.02e-07\n",
      "step: 921/2500 train loss: 0.74, LR: 7.01e-07\n",
      "step: 922/2500 train loss: 0.73, LR: 7.01e-07\n",
      "step: 923/2500 train loss: 0.75, LR: 7.00e-07\n",
      "step: 924/2500 train loss: 0.76, LR: 7.00e-07\n",
      "step: 925/2500 train loss: 0.72, LR: 7.00e-07\n",
      "step: 926/2500 train loss: 0.76, LR: 6.99e-07\n",
      "step: 927/2500 train loss: 0.71, LR: 6.99e-07\n",
      "step: 928/2500 train loss: 0.71, LR: 6.98e-07\n",
      "step: 929/2500 train loss: 0.88, LR: 6.98e-07\n",
      "step: 930/2500 train loss: 0.75, LR: 6.97e-07\n",
      "step: 931/2500 train loss: 0.82, LR: 6.97e-07\n",
      "step: 932/2500 train loss: 0.67, LR: 6.96e-07\n",
      "step: 933/2500 train loss: 0.72, LR: 6.96e-07\n",
      "step: 934/2500 train loss: 0.7, LR: 6.96e-07\n",
      "step: 935/2500 train loss: 0.68, LR: 6.95e-07\n",
      "step: 936/2500 train loss: 0.75, LR: 6.95e-07\n",
      "step: 937/2500 train loss: 0.73, LR: 6.94e-07\n",
      "step: 938/2500 train loss: 0.82, LR: 6.94e-07\n",
      "step: 939/2500 train loss: 0.78, LR: 6.93e-07\n",
      "step: 940/2500 train loss: 0.74, LR: 6.93e-07\n",
      "step: 941/2500 train loss: 0.72, LR: 6.92e-07\n",
      "step: 942/2500 train loss: 0.72, LR: 6.92e-07\n",
      "step: 943/2500 train loss: 0.77, LR: 6.92e-07\n",
      "step: 944/2500 train loss: 0.84, LR: 6.91e-07\n",
      "step: 945/2500 train loss: 0.73, LR: 6.91e-07\n",
      "step: 946/2500 train loss: 0.78, LR: 6.90e-07\n",
      "step: 947/2500 train loss: 0.67, LR: 6.90e-07\n",
      "step: 948/2500 train loss: 0.63, LR: 6.89e-07\n",
      "step: 949/2500 train loss: 0.82, LR: 6.89e-07\n",
      "step: 950/2500 train loss: 0.74, LR: 6.88e-07\n",
      "step: 951/2500 train loss: 0.73, LR: 6.88e-07\n",
      "step: 952/2500 train loss: 0.68, LR: 6.88e-07\n",
      "step: 953/2500 train loss: 0.79, LR: 6.87e-07\n",
      "step: 954/2500 train loss: 0.66, LR: 6.87e-07\n",
      "step: 955/2500 train loss: 0.72, LR: 6.86e-07\n",
      "step: 956/2500 train loss: 0.72, LR: 6.86e-07\n",
      "step: 957/2500 train loss: 0.66, LR: 6.85e-07\n",
      "step: 958/2500 train loss: 0.68, LR: 6.85e-07\n",
      "step: 959/2500 train loss: 0.72, LR: 6.84e-07\n",
      "step: 960/2500 train loss: 0.82, LR: 6.84e-07\n",
      "step: 961/2500 train loss: 0.77, LR: 6.84e-07\n",
      "step: 962/2500 train loss: 0.72, LR: 6.83e-07\n",
      "step: 963/2500 train loss: 0.82, LR: 6.83e-07\n",
      "step: 964/2500 train loss: 0.74, LR: 6.82e-07\n",
      "step: 965/2500 train loss: 0.8, LR: 6.82e-07\n",
      "step: 966/2500 train loss: 0.8, LR: 6.81e-07\n",
      "step: 967/2500 train loss: 0.76, LR: 6.81e-07\n",
      "step: 968/2500 train loss: 0.72, LR: 6.80e-07\n",
      "step: 969/2500 train loss: 0.72, LR: 6.80e-07\n",
      "step: 970/2500 train loss: 0.76, LR: 6.80e-07\n",
      "step: 971/2500 train loss: 0.84, LR: 6.79e-07\n",
      "step: 972/2500 train loss: 0.75, LR: 6.79e-07\n",
      "step: 973/2500 train loss: 0.69, LR: 6.78e-07\n",
      "step: 974/2500 train loss: 0.68, LR: 6.78e-07\n",
      "step: 975/2500 train loss: 0.67, LR: 6.77e-07\n",
      "step: 976/2500 train loss: 0.76, LR: 6.77e-07\n",
      "step: 977/2500 train loss: 0.74, LR: 6.76e-07\n",
      "step: 978/2500 train loss: 0.76, LR: 6.76e-07\n",
      "step: 979/2500 train loss: 0.68, LR: 6.76e-07\n",
      "step: 980/2500 train loss: 0.75, LR: 6.75e-07\n",
      "step: 981/2500 train loss: 0.76, LR: 6.75e-07\n",
      "step: 982/2500 train loss: 0.81, LR: 6.74e-07\n",
      "step: 983/2500 train loss: 0.68, LR: 6.74e-07\n",
      "step: 984/2500 train loss: 0.81, LR: 6.73e-07\n",
      "step: 985/2500 train loss: 0.74, LR: 6.73e-07\n",
      "step: 986/2500 train loss: 0.64, LR: 6.72e-07\n",
      "step: 987/2500 train loss: 0.83, LR: 6.72e-07\n",
      "step: 988/2500 train loss: 0.71, LR: 6.72e-07\n",
      "step: 989/2500 train loss: 0.7, LR: 6.71e-07\n",
      "step: 990/2500 train loss: 0.72, LR: 6.71e-07\n",
      "step: 991/2500 train loss: 0.71, LR: 6.70e-07\n",
      "step: 992/2500 train loss: 0.72, LR: 6.70e-07\n",
      "step: 993/2500 train loss: 0.68, LR: 6.69e-07\n",
      "step: 994/2500 train loss: 0.72, LR: 6.69e-07\n",
      "step: 995/2500 train loss: 0.69, LR: 6.68e-07\n",
      "step: 996/2500 train loss: 0.66, LR: 6.68e-07\n",
      "step: 997/2500 train loss: 0.73, LR: 6.68e-07\n",
      "step: 998/2500 train loss: 0.59, LR: 6.67e-07\n",
      "step: 999/2500 train loss: 0.75, LR: 6.67e-07\n",
      "step: 1000/2500 eval loss: 0.71\n",
      "step: 1000/2500 train loss: 0.69, LR: 6.66e-07\n",
      "step: 1001/2500 train loss: 0.66, LR: 6.66e-07\n",
      "step: 1002/2500 train loss: 0.82, LR: 6.65e-07\n",
      "step: 1003/2500 train loss: 0.72, LR: 6.65e-07\n",
      "step: 1004/2500 train loss: 0.76, LR: 6.64e-07\n",
      "step: 1005/2500 train loss: 0.78, LR: 6.64e-07\n",
      "step: 1006/2500 train loss: 0.76, LR: 6.64e-07\n",
      "step: 1007/2500 train loss: 0.67, LR: 6.63e-07\n",
      "step: 1008/2500 train loss: 0.72, LR: 6.63e-07\n",
      "step: 1009/2500 train loss: 0.69, LR: 6.62e-07\n",
      "step: 1010/2500 train loss: 0.67, LR: 6.62e-07\n",
      "step: 1011/2500 train loss: 0.73, LR: 6.61e-07\n",
      "step: 1012/2500 train loss: 0.74, LR: 6.61e-07\n",
      "step: 1013/2500 train loss: 0.76, LR: 6.60e-07\n",
      "step: 1014/2500 train loss: 0.84, LR: 6.60e-07\n",
      "step: 1015/2500 train loss: 0.83, LR: 6.60e-07\n",
      "step: 1016/2500 train loss: 0.69, LR: 6.59e-07\n",
      "step: 1017/2500 train loss: 0.74, LR: 6.59e-07\n",
      "step: 1018/2500 train loss: 0.83, LR: 6.58e-07\n",
      "step: 1019/2500 train loss: 0.71, LR: 6.58e-07\n",
      "step: 1020/2500 train loss: 0.68, LR: 6.57e-07\n",
      "step: 1021/2500 train loss: 0.69, LR: 6.57e-07\n",
      "step: 1022/2500 train loss: 0.66, LR: 6.56e-07\n",
      "step: 1023/2500 train loss: 0.89, LR: 6.56e-07\n",
      "step: 1024/2500 train loss: 0.69, LR: 6.56e-07\n",
      "step: 1025/2500 train loss: 0.78, LR: 6.55e-07\n",
      "step: 1026/2500 train loss: 0.72, LR: 6.55e-07\n",
      "step: 1027/2500 train loss: 0.67, LR: 6.54e-07\n",
      "step: 1028/2500 train loss: 0.78, LR: 6.54e-07\n",
      "step: 1029/2500 train loss: 0.74, LR: 6.53e-07\n",
      "step: 1030/2500 train loss: 0.67, LR: 6.53e-07\n",
      "step: 1031/2500 train loss: 0.71, LR: 6.52e-07\n",
      "step: 1032/2500 train loss: 0.74, LR: 6.52e-07\n",
      "step: 1033/2500 train loss: 0.77, LR: 6.52e-07\n",
      "step: 1034/2500 train loss: 0.71, LR: 6.51e-07\n",
      "step: 1035/2500 train loss: 0.64, LR: 6.51e-07\n",
      "step: 1036/2500 train loss: 0.74, LR: 6.50e-07\n",
      "step: 1037/2500 train loss: 0.74, LR: 6.50e-07\n",
      "step: 1038/2500 train loss: 0.63, LR: 6.49e-07\n",
      "step: 1039/2500 train loss: 0.74, LR: 6.49e-07\n",
      "step: 1040/2500 train loss: 0.67, LR: 6.48e-07\n",
      "step: 1041/2500 train loss: 0.6, LR: 6.48e-07\n",
      "step: 1042/2500 train loss: 0.85, LR: 6.48e-07\n",
      "step: 1043/2500 train loss: 0.72, LR: 6.47e-07\n",
      "step: 1044/2500 train loss: 0.71, LR: 6.47e-07\n",
      "step: 1045/2500 train loss: 0.75, LR: 6.46e-07\n",
      "step: 1046/2500 train loss: 0.68, LR: 6.46e-07\n",
      "step: 1047/2500 train loss: 0.63, LR: 6.45e-07\n",
      "step: 1048/2500 train loss: 0.7, LR: 6.45e-07\n",
      "step: 1049/2500 train loss: 0.74, LR: 6.44e-07\n",
      "step: 1050/2500 train loss: 0.72, LR: 6.44e-07\n",
      "step: 1051/2500 train loss: 0.68, LR: 6.44e-07\n",
      "step: 1052/2500 train loss: 0.73, LR: 6.43e-07\n",
      "step: 1053/2500 train loss: 0.71, LR: 6.43e-07\n",
      "step: 1054/2500 train loss: 0.66, LR: 6.42e-07\n",
      "step: 1055/2500 train loss: 0.63, LR: 6.42e-07\n",
      "step: 1056/2500 train loss: 0.83, LR: 6.41e-07\n",
      "step: 1057/2500 train loss: 0.7, LR: 6.41e-07\n",
      "step: 1058/2500 train loss: 0.75, LR: 6.40e-07\n",
      "step: 1059/2500 train loss: 0.78, LR: 6.40e-07\n",
      "step: 1060/2500 train loss: 0.7, LR: 6.40e-07\n",
      "step: 1061/2500 train loss: 0.76, LR: 6.39e-07\n",
      "step: 1062/2500 train loss: 0.78, LR: 6.39e-07\n",
      "step: 1063/2500 train loss: 0.7, LR: 6.38e-07\n",
      "step: 1064/2500 train loss: 0.68, LR: 6.38e-07\n",
      "step: 1065/2500 train loss: 0.79, LR: 6.37e-07\n",
      "step: 1066/2500 train loss: 0.69, LR: 6.37e-07\n",
      "step: 1067/2500 train loss: 0.76, LR: 6.36e-07\n",
      "step: 1068/2500 train loss: 0.8, LR: 6.36e-07\n",
      "step: 1069/2500 train loss: 0.68, LR: 6.36e-07\n",
      "step: 1070/2500 train loss: 0.74, LR: 6.35e-07\n",
      "step: 1071/2500 train loss: 0.76, LR: 6.35e-07\n",
      "step: 1072/2500 train loss: 0.84, LR: 6.34e-07\n",
      "step: 1073/2500 train loss: 0.71, LR: 6.34e-07\n",
      "step: 1074/2500 train loss: 0.66, LR: 6.33e-07\n",
      "step: 1075/2500 train loss: 0.74, LR: 6.33e-07\n",
      "step: 1076/2500 train loss: 0.82, LR: 6.32e-07\n",
      "step: 1077/2500 train loss: 0.69, LR: 6.32e-07\n",
      "step: 1078/2500 train loss: 0.81, LR: 6.32e-07\n",
      "step: 1079/2500 train loss: 0.73, LR: 6.31e-07\n",
      "step: 1080/2500 train loss: 0.69, LR: 6.31e-07\n",
      "step: 1081/2500 train loss: 0.77, LR: 6.30e-07\n",
      "step: 1082/2500 train loss: 0.68, LR: 6.30e-07\n",
      "step: 1083/2500 train loss: 0.75, LR: 6.29e-07\n",
      "step: 1084/2500 train loss: 0.7, LR: 6.29e-07\n",
      "step: 1085/2500 train loss: 0.69, LR: 6.28e-07\n",
      "step: 1086/2500 train loss: 0.71, LR: 6.28e-07\n",
      "step: 1087/2500 train loss: 0.64, LR: 6.28e-07\n",
      "step: 1088/2500 train loss: 0.77, LR: 6.27e-07\n",
      "step: 1089/2500 train loss: 0.7, LR: 6.27e-07\n",
      "step: 1090/2500 train loss: 0.68, LR: 6.26e-07\n",
      "step: 1091/2500 train loss: 0.7, LR: 6.26e-07\n",
      "step: 1092/2500 train loss: 0.68, LR: 6.25e-07\n",
      "step: 1093/2500 train loss: 0.72, LR: 6.25e-07\n",
      "step: 1094/2500 train loss: 0.94, LR: 6.24e-07\n",
      "step: 1095/2500 train loss: 0.72, LR: 6.24e-07\n",
      "step: 1096/2500 train loss: 0.73, LR: 6.24e-07\n",
      "step: 1097/2500 train loss: 0.74, LR: 6.23e-07\n",
      "step: 1098/2500 train loss: 0.67, LR: 6.23e-07\n",
      "step: 1099/2500 train loss: 0.74, LR: 6.22e-07\n",
      "step: 1100/2500 train loss: 0.7, LR: 6.22e-07\n",
      "step: 1101/2500 train loss: 0.77, LR: 6.21e-07\n",
      "step: 1102/2500 train loss: 0.68, LR: 6.21e-07\n",
      "step: 1103/2500 train loss: 0.66, LR: 6.20e-07\n",
      "step: 1104/2500 train loss: 0.78, LR: 6.20e-07\n",
      "step: 1105/2500 train loss: 0.73, LR: 6.20e-07\n",
      "step: 1106/2500 train loss: 0.75, LR: 6.19e-07\n",
      "step: 1107/2500 train loss: 0.73, LR: 6.19e-07\n",
      "step: 1108/2500 train loss: 0.65, LR: 6.18e-07\n",
      "step: 1109/2500 train loss: 0.75, LR: 6.18e-07\n",
      "step: 1110/2500 train loss: 0.78, LR: 6.17e-07\n",
      "step: 1111/2500 train loss: 0.64, LR: 6.17e-07\n",
      "step: 1112/2500 train loss: 0.75, LR: 6.16e-07\n",
      "step: 1113/2500 train loss: 0.73, LR: 6.16e-07\n",
      "step: 1114/2500 train loss: 0.72, LR: 6.16e-07\n",
      "step: 1115/2500 train loss: 0.68, LR: 6.15e-07\n",
      "step: 1116/2500 train loss: 0.74, LR: 6.15e-07\n",
      "step: 1117/2500 train loss: 0.65, LR: 6.14e-07\n",
      "step: 1118/2500 train loss: 0.66, LR: 6.14e-07\n",
      "step: 1119/2500 train loss: 0.67, LR: 6.13e-07\n",
      "step: 1120/2500 train loss: 0.72, LR: 6.13e-07\n",
      "step: 1121/2500 train loss: 0.66, LR: 6.12e-07\n",
      "step: 1122/2500 train loss: 0.66, LR: 6.12e-07\n",
      "step: 1123/2500 train loss: 0.68, LR: 6.12e-07\n",
      "step: 1124/2500 train loss: 0.69, LR: 6.11e-07\n",
      "step: 1125/2500 train loss: 0.69, LR: 6.11e-07\n",
      "step: 1126/2500 train loss: 0.59, LR: 6.10e-07\n",
      "step: 1127/2500 train loss: 0.71, LR: 6.10e-07\n",
      "step: 1128/2500 train loss: 0.71, LR: 6.09e-07\n",
      "step: 1129/2500 train loss: 0.7, LR: 6.09e-07\n",
      "step: 1130/2500 train loss: 0.7, LR: 6.08e-07\n",
      "step: 1131/2500 train loss: 0.68, LR: 6.08e-07\n",
      "step: 1132/2500 train loss: 0.72, LR: 6.08e-07\n",
      "step: 1133/2500 train loss: 0.7, LR: 6.07e-07\n",
      "step: 1134/2500 train loss: 0.73, LR: 6.07e-07\n",
      "step: 1135/2500 train loss: 0.69, LR: 6.06e-07\n",
      "step: 1136/2500 train loss: 0.74, LR: 6.06e-07\n",
      "step: 1137/2500 train loss: 0.67, LR: 6.05e-07\n",
      "step: 1138/2500 train loss: 0.68, LR: 6.05e-07\n",
      "step: 1139/2500 train loss: 0.74, LR: 6.04e-07\n",
      "step: 1140/2500 train loss: 0.7, LR: 6.04e-07\n",
      "step: 1141/2500 train loss: 0.77, LR: 6.04e-07\n",
      "step: 1142/2500 train loss: 0.66, LR: 6.03e-07\n",
      "step: 1143/2500 train loss: 0.66, LR: 6.03e-07\n",
      "step: 1144/2500 train loss: 0.6, LR: 6.02e-07\n",
      "step: 1145/2500 train loss: 0.69, LR: 6.02e-07\n",
      "step: 1146/2500 train loss: 0.77, LR: 6.01e-07\n",
      "step: 1147/2500 train loss: 0.64, LR: 6.01e-07\n",
      "step: 1148/2500 train loss: 0.63, LR: 6.00e-07\n",
      "step: 1149/2500 train loss: 0.67, LR: 6.00e-07\n",
      "step: 1150/2500 train loss: 0.82, LR: 6.00e-07\n",
      "step: 1151/2500 train loss: 0.7, LR: 5.99e-07\n",
      "step: 1152/2500 train loss: 0.75, LR: 5.99e-07\n",
      "step: 1153/2500 train loss: 0.74, LR: 5.98e-07\n",
      "step: 1154/2500 train loss: 0.68, LR: 5.98e-07\n",
      "step: 1155/2500 train loss: 0.72, LR: 5.97e-07\n",
      "step: 1156/2500 train loss: 0.72, LR: 5.97e-07\n",
      "step: 1157/2500 train loss: 0.73, LR: 5.96e-07\n",
      "step: 1158/2500 train loss: 0.86, LR: 5.96e-07\n",
      "step: 1159/2500 train loss: 0.8, LR: 5.96e-07\n",
      "step: 1160/2500 train loss: 0.83, LR: 5.95e-07\n",
      "step: 1161/2500 train loss: 0.75, LR: 5.95e-07\n",
      "step: 1162/2500 train loss: 0.68, LR: 5.94e-07\n",
      "step: 1163/2500 train loss: 0.78, LR: 5.94e-07\n",
      "step: 1164/2500 train loss: 0.7, LR: 5.93e-07\n",
      "step: 1165/2500 train loss: 0.63, LR: 5.93e-07\n",
      "step: 1166/2500 train loss: 0.79, LR: 5.92e-07\n",
      "step: 1167/2500 train loss: 0.81, LR: 5.92e-07\n",
      "step: 1168/2500 train loss: 0.7, LR: 5.92e-07\n",
      "step: 1169/2500 train loss: 0.86, LR: 5.91e-07\n",
      "step: 1170/2500 train loss: 0.64, LR: 5.91e-07\n",
      "step: 1171/2500 train loss: 0.68, LR: 5.90e-07\n",
      "step: 1172/2500 train loss: 0.73, LR: 5.90e-07\n",
      "step: 1173/2500 train loss: 0.75, LR: 5.89e-07\n",
      "step: 1174/2500 train loss: 0.61, LR: 5.89e-07\n",
      "step: 1175/2500 train loss: 0.71, LR: 5.88e-07\n",
      "step: 1176/2500 train loss: 0.69, LR: 5.88e-07\n",
      "step: 1177/2500 train loss: 0.69, LR: 5.88e-07\n",
      "step: 1178/2500 train loss: 0.71, LR: 5.87e-07\n",
      "step: 1179/2500 train loss: 0.61, LR: 5.87e-07\n",
      "step: 1180/2500 train loss: 0.75, LR: 5.86e-07\n",
      "step: 1181/2500 train loss: 0.71, LR: 5.86e-07\n",
      "step: 1182/2500 train loss: 0.67, LR: 5.85e-07\n",
      "step: 1183/2500 train loss: 0.73, LR: 5.85e-07\n",
      "step: 1184/2500 train loss: 0.7, LR: 5.84e-07\n",
      "step: 1185/2500 train loss: 0.76, LR: 5.84e-07\n",
      "step: 1186/2500 train loss: 0.81, LR: 5.84e-07\n",
      "step: 1187/2500 train loss: 0.72, LR: 5.83e-07\n",
      "step: 1188/2500 train loss: 0.74, LR: 5.83e-07\n",
      "step: 1189/2500 train loss: 0.78, LR: 5.82e-07\n",
      "step: 1190/2500 train loss: 0.65, LR: 5.82e-07\n",
      "step: 1191/2500 train loss: 0.75, LR: 5.81e-07\n",
      "step: 1192/2500 train loss: 0.71, LR: 5.81e-07\n",
      "step: 1193/2500 train loss: 0.73, LR: 5.80e-07\n",
      "step: 1194/2500 train loss: 0.81, LR: 5.80e-07\n",
      "step: 1195/2500 train loss: 0.71, LR: 5.80e-07\n",
      "step: 1196/2500 train loss: 0.72, LR: 5.79e-07\n",
      "step: 1197/2500 train loss: 0.75, LR: 5.79e-07\n",
      "step: 1198/2500 train loss: 0.68, LR: 5.78e-07\n",
      "step: 1199/2500 train loss: 0.87, LR: 5.78e-07\n",
      "step: 1200/2500 train loss: 0.62, LR: 5.77e-07\n",
      "step: 1201/2500 train loss: 0.87, LR: 5.77e-07\n",
      "step: 1202/2500 train loss: 0.72, LR: 5.76e-07\n",
      "step: 1203/2500 train loss: 0.9, LR: 5.76e-07\n",
      "step: 1204/2500 train loss: 0.68, LR: 5.76e-07\n",
      "step: 1205/2500 train loss: 0.69, LR: 5.75e-07\n",
      "step: 1206/2500 train loss: 0.7, LR: 5.75e-07\n",
      "step: 1207/2500 train loss: 0.69, LR: 5.74e-07\n",
      "step: 1208/2500 train loss: 0.69, LR: 5.74e-07\n",
      "step: 1209/2500 train loss: 0.63, LR: 5.73e-07\n",
      "step: 1210/2500 train loss: 0.79, LR: 5.73e-07\n",
      "step: 1211/2500 train loss: 0.73, LR: 5.72e-07\n",
      "step: 1212/2500 train loss: 0.71, LR: 5.72e-07\n",
      "step: 1213/2500 train loss: 0.69, LR: 5.72e-07\n",
      "step: 1214/2500 train loss: 0.82, LR: 5.71e-07\n",
      "step: 1215/2500 train loss: 0.7, LR: 5.71e-07\n",
      "step: 1216/2500 train loss: 0.74, LR: 5.70e-07\n",
      "step: 1217/2500 train loss: 0.72, LR: 5.70e-07\n",
      "step: 1218/2500 train loss: 0.66, LR: 5.69e-07\n",
      "step: 1219/2500 train loss: 0.73, LR: 5.69e-07\n",
      "step: 1220/2500 train loss: 0.68, LR: 5.68e-07\n",
      "step: 1221/2500 train loss: 0.73, LR: 5.68e-07\n",
      "step: 1222/2500 train loss: 0.75, LR: 5.68e-07\n",
      "step: 1223/2500 train loss: 0.72, LR: 5.67e-07\n",
      "step: 1224/2500 train loss: 0.74, LR: 5.67e-07\n",
      "step: 1225/2500 train loss: 0.7, LR: 5.66e-07\n",
      "step: 1226/2500 train loss: 0.71, LR: 5.66e-07\n",
      "step: 1227/2500 train loss: 0.72, LR: 5.65e-07\n",
      "step: 1228/2500 train loss: 0.72, LR: 5.65e-07\n",
      "step: 1229/2500 train loss: 0.78, LR: 5.64e-07\n",
      "step: 1230/2500 train loss: 0.74, LR: 5.64e-07\n",
      "step: 1231/2500 train loss: 0.77, LR: 5.64e-07\n",
      "step: 1232/2500 train loss: 0.68, LR: 5.63e-07\n",
      "step: 1233/2500 train loss: 0.73, LR: 5.63e-07\n",
      "step: 1234/2500 train loss: 0.78, LR: 5.62e-07\n",
      "step: 1235/2500 train loss: 0.72, LR: 5.62e-07\n",
      "step: 1236/2500 train loss: 0.67, LR: 5.61e-07\n",
      "step: 1237/2500 train loss: 0.66, LR: 5.61e-07\n",
      "step: 1238/2500 train loss: 0.69, LR: 5.60e-07\n",
      "step: 1239/2500 train loss: 0.71, LR: 5.60e-07\n",
      "step: 1240/2500 train loss: 0.82, LR: 5.60e-07\n",
      "step: 1241/2500 train loss: 0.7, LR: 5.59e-07\n",
      "step: 1242/2500 train loss: 0.74, LR: 5.59e-07\n",
      "step: 1243/2500 train loss: 0.66, LR: 5.58e-07\n",
      "step: 1244/2500 train loss: 0.71, LR: 5.58e-07\n",
      "step: 1245/2500 train loss: 0.64, LR: 5.57e-07\n",
      "step: 1246/2500 train loss: 0.84, LR: 5.57e-07\n",
      "step: 1247/2500 train loss: 0.66, LR: 5.56e-07\n",
      "step: 1248/2500 train loss: 0.66, LR: 5.56e-07\n",
      "step: 1249/2500 train loss: 0.61, LR: 5.56e-07\n",
      "step: 1250/2500 eval loss: 0.7\n",
      "step: 1250/2500 train loss: 0.72, LR: 5.55e-07\n",
      "step: 1251/2500 train loss: 0.66, LR: 5.55e-07\n",
      "step: 1252/2500 train loss: 0.69, LR: 5.54e-07\n",
      "step: 1253/2500 train loss: 0.7, LR: 5.54e-07\n",
      "step: 1254/2500 train loss: 0.75, LR: 5.53e-07\n",
      "step: 1255/2500 train loss: 0.63, LR: 5.53e-07\n",
      "step: 1256/2500 train loss: 0.76, LR: 5.52e-07\n",
      "step: 1257/2500 train loss: 0.77, LR: 5.52e-07\n",
      "step: 1258/2500 train loss: 0.72, LR: 5.52e-07\n",
      "step: 1259/2500 train loss: 0.64, LR: 5.51e-07\n",
      "step: 1260/2500 train loss: 0.73, LR: 5.51e-07\n",
      "step: 1261/2500 train loss: 0.68, LR: 5.50e-07\n",
      "step: 1262/2500 train loss: 0.76, LR: 5.50e-07\n",
      "step: 1263/2500 train loss: 0.67, LR: 5.49e-07\n",
      "step: 1264/2500 train loss: 0.71, LR: 5.49e-07\n",
      "step: 1265/2500 train loss: 0.65, LR: 5.48e-07\n",
      "step: 1266/2500 train loss: 0.75, LR: 5.48e-07\n",
      "step: 1267/2500 train loss: 0.68, LR: 5.48e-07\n",
      "step: 1268/2500 train loss: 0.77, LR: 5.47e-07\n",
      "step: 1269/2500 train loss: 0.66, LR: 5.47e-07\n",
      "step: 1270/2500 train loss: 0.71, LR: 5.46e-07\n",
      "step: 1271/2500 train loss: 0.58, LR: 5.46e-07\n",
      "step: 1272/2500 train loss: 0.87, LR: 5.45e-07\n",
      "step: 1273/2500 train loss: 0.77, LR: 5.45e-07\n",
      "step: 1274/2500 train loss: 0.71, LR: 5.44e-07\n",
      "step: 1275/2500 train loss: 0.78, LR: 5.44e-07\n",
      "step: 1276/2500 train loss: 0.7, LR: 5.44e-07\n",
      "step: 1277/2500 train loss: 0.72, LR: 5.43e-07\n",
      "step: 1278/2500 train loss: 0.73, LR: 5.43e-07\n",
      "step: 1279/2500 train loss: 0.7, LR: 5.42e-07\n",
      "step: 1280/2500 train loss: 0.65, LR: 5.42e-07\n",
      "step: 1281/2500 train loss: 0.69, LR: 5.41e-07\n",
      "step: 1282/2500 train loss: 0.68, LR: 5.41e-07\n",
      "step: 1283/2500 train loss: 0.8, LR: 5.40e-07\n",
      "step: 1284/2500 train loss: 0.8, LR: 5.40e-07\n",
      "step: 1285/2500 train loss: 0.74, LR: 5.40e-07\n",
      "step: 1286/2500 train loss: 0.71, LR: 5.39e-07\n",
      "step: 1287/2500 train loss: 0.61, LR: 5.39e-07\n",
      "step: 1288/2500 train loss: 0.73, LR: 5.38e-07\n",
      "step: 1289/2500 train loss: 0.73, LR: 5.38e-07\n",
      "step: 1290/2500 train loss: 0.75, LR: 5.37e-07\n",
      "step: 1291/2500 train loss: 0.79, LR: 5.37e-07\n",
      "step: 1292/2500 train loss: 0.73, LR: 5.36e-07\n",
      "step: 1293/2500 train loss: 0.69, LR: 5.36e-07\n",
      "step: 1294/2500 train loss: 0.68, LR: 5.36e-07\n",
      "step: 1295/2500 train loss: 0.69, LR: 5.35e-07\n",
      "step: 1296/2500 train loss: 0.7, LR: 5.35e-07\n",
      "step: 1297/2500 train loss: 0.69, LR: 5.34e-07\n",
      "step: 1298/2500 train loss: 0.67, LR: 5.34e-07\n",
      "step: 1299/2500 train loss: 0.68, LR: 5.33e-07\n",
      "step: 1300/2500 train loss: 0.65, LR: 5.33e-07\n",
      "step: 1301/2500 train loss: 0.73, LR: 5.32e-07\n",
      "step: 1302/2500 train loss: 0.65, LR: 5.32e-07\n",
      "step: 1303/2500 train loss: 0.62, LR: 5.32e-07\n",
      "step: 1304/2500 train loss: 0.68, LR: 5.31e-07\n",
      "step: 1305/2500 train loss: 0.65, LR: 5.31e-07\n",
      "step: 1306/2500 train loss: 0.77, LR: 5.30e-07\n",
      "step: 1307/2500 train loss: 0.7, LR: 5.30e-07\n",
      "step: 1308/2500 train loss: 0.62, LR: 5.29e-07\n",
      "step: 1309/2500 train loss: 0.75, LR: 5.29e-07\n",
      "step: 1310/2500 train loss: 0.62, LR: 5.28e-07\n",
      "step: 1311/2500 train loss: 0.76, LR: 5.28e-07\n",
      "step: 1312/2500 train loss: 0.68, LR: 5.28e-07\n",
      "step: 1313/2500 train loss: 0.73, LR: 5.27e-07\n",
      "step: 1314/2500 train loss: 0.69, LR: 5.27e-07\n",
      "step: 1315/2500 train loss: 0.77, LR: 5.26e-07\n",
      "step: 1316/2500 train loss: 0.67, LR: 5.26e-07\n",
      "step: 1317/2500 train loss: 0.68, LR: 5.25e-07\n",
      "step: 1318/2500 train loss: 0.76, LR: 5.25e-07\n",
      "step: 1319/2500 train loss: 0.58, LR: 5.24e-07\n",
      "step: 1320/2500 train loss: 0.67, LR: 5.24e-07\n",
      "step: 1321/2500 train loss: 0.74, LR: 5.24e-07\n",
      "step: 1322/2500 train loss: 0.7, LR: 5.23e-07\n",
      "step: 1323/2500 train loss: 0.7, LR: 5.23e-07\n",
      "step: 1324/2500 train loss: 0.69, LR: 5.22e-07\n",
      "step: 1325/2500 train loss: 0.73, LR: 5.22e-07\n",
      "step: 1326/2500 train loss: 0.63, LR: 5.21e-07\n",
      "step: 1327/2500 train loss: 0.63, LR: 5.21e-07\n",
      "step: 1328/2500 train loss: 0.7, LR: 5.20e-07\n",
      "step: 1329/2500 train loss: 0.79, LR: 5.20e-07\n",
      "step: 1330/2500 train loss: 0.66, LR: 5.20e-07\n",
      "step: 1331/2500 train loss: 0.72, LR: 5.19e-07\n",
      "step: 1332/2500 train loss: 0.69, LR: 5.19e-07\n",
      "step: 1333/2500 train loss: 0.79, LR: 5.18e-07\n",
      "step: 1334/2500 train loss: 0.68, LR: 5.18e-07\n",
      "step: 1335/2500 train loss: 0.74, LR: 5.17e-07\n",
      "step: 1336/2500 train loss: 0.69, LR: 5.17e-07\n",
      "step: 1337/2500 train loss: 0.72, LR: 5.16e-07\n",
      "step: 1338/2500 train loss: 0.61, LR: 5.16e-07\n",
      "step: 1339/2500 train loss: 0.81, LR: 5.16e-07\n",
      "step: 1340/2500 train loss: 0.8, LR: 5.15e-07\n",
      "step: 1341/2500 train loss: 0.7, LR: 5.15e-07\n",
      "step: 1342/2500 train loss: 0.67, LR: 5.14e-07\n",
      "step: 1343/2500 train loss: 0.71, LR: 5.14e-07\n",
      "step: 1344/2500 train loss: 0.67, LR: 5.13e-07\n",
      "step: 1345/2500 train loss: 0.74, LR: 5.13e-07\n",
      "step: 1346/2500 train loss: 0.73, LR: 5.12e-07\n",
      "step: 1347/2500 train loss: 0.72, LR: 5.12e-07\n",
      "step: 1348/2500 train loss: 0.71, LR: 5.12e-07\n",
      "step: 1349/2500 train loss: 0.68, LR: 5.11e-07\n",
      "step: 1350/2500 train loss: 0.8, LR: 5.11e-07\n",
      "step: 1351/2500 train loss: 0.72, LR: 5.10e-07\n",
      "step: 1352/2500 train loss: 0.66, LR: 5.10e-07\n",
      "step: 1353/2500 train loss: 0.83, LR: 5.09e-07\n",
      "step: 1354/2500 train loss: 0.66, LR: 5.09e-07\n",
      "step: 1355/2500 train loss: 0.67, LR: 5.08e-07\n",
      "step: 1356/2500 train loss: 0.8, LR: 5.08e-07\n",
      "step: 1357/2500 train loss: 0.85, LR: 5.08e-07\n",
      "step: 1358/2500 train loss: 0.67, LR: 5.07e-07\n",
      "step: 1359/2500 train loss: 0.64, LR: 5.07e-07\n",
      "step: 1360/2500 train loss: 0.82, LR: 5.06e-07\n",
      "step: 1361/2500 train loss: 0.7, LR: 5.06e-07\n",
      "step: 1362/2500 train loss: 0.66, LR: 5.05e-07\n",
      "step: 1363/2500 train loss: 0.68, LR: 5.05e-07\n",
      "step: 1364/2500 train loss: 0.63, LR: 5.04e-07\n",
      "step: 1365/2500 train loss: 0.8, LR: 5.04e-07\n",
      "step: 1366/2500 train loss: 0.76, LR: 5.04e-07\n",
      "step: 1367/2500 train loss: 0.64, LR: 5.03e-07\n",
      "step: 1368/2500 train loss: 0.76, LR: 5.03e-07\n",
      "step: 1369/2500 train loss: 0.7, LR: 5.02e-07\n",
      "step: 1370/2500 train loss: 0.71, LR: 5.02e-07\n",
      "step: 1371/2500 train loss: 0.73, LR: 5.01e-07\n",
      "step: 1372/2500 train loss: 0.8, LR: 5.01e-07\n",
      "step: 1373/2500 train loss: 0.73, LR: 5.00e-07\n",
      "step: 1374/2500 train loss: 0.7, LR: 5.00e-07\n",
      "step: 1375/2500 train loss: 0.77, LR: 5.00e-07\n",
      "step: 1376/2500 train loss: 0.78, LR: 4.99e-07\n",
      "step: 1377/2500 train loss: 0.66, LR: 4.99e-07\n",
      "step: 1378/2500 train loss: 0.75, LR: 4.98e-07\n",
      "step: 1379/2500 train loss: 0.69, LR: 4.98e-07\n",
      "step: 1380/2500 train loss: 0.68, LR: 4.97e-07\n",
      "step: 1381/2500 train loss: 0.67, LR: 4.97e-07\n",
      "step: 1382/2500 train loss: 0.64, LR: 4.96e-07\n",
      "step: 1383/2500 train loss: 0.78, LR: 4.96e-07\n",
      "step: 1384/2500 train loss: 0.67, LR: 4.96e-07\n",
      "step: 1385/2500 train loss: 0.67, LR: 4.95e-07\n",
      "step: 1386/2500 train loss: 0.82, LR: 4.95e-07\n",
      "step: 1387/2500 train loss: 0.67, LR: 4.94e-07\n",
      "step: 1388/2500 train loss: 0.69, LR: 4.94e-07\n",
      "step: 1389/2500 train loss: 0.76, LR: 4.93e-07\n",
      "step: 1390/2500 train loss: 0.71, LR: 4.93e-07\n",
      "step: 1391/2500 train loss: 0.67, LR: 4.92e-07\n",
      "step: 1392/2500 train loss: 0.71, LR: 4.92e-07\n",
      "step: 1393/2500 train loss: 0.89, LR: 4.92e-07\n",
      "step: 1394/2500 train loss: 0.93, LR: 4.91e-07\n",
      "step: 1395/2500 train loss: 0.67, LR: 4.91e-07\n",
      "step: 1396/2500 train loss: 0.84, LR: 4.90e-07\n",
      "step: 1397/2500 train loss: 0.68, LR: 4.90e-07\n",
      "step: 1398/2500 train loss: 0.63, LR: 4.89e-07\n",
      "step: 1399/2500 train loss: 0.67, LR: 4.89e-07\n",
      "step: 1400/2500 train loss: 0.78, LR: 4.88e-07\n",
      "step: 1401/2500 train loss: 0.76, LR: 4.88e-07\n",
      "step: 1402/2500 train loss: 0.74, LR: 4.88e-07\n",
      "step: 1403/2500 train loss: 0.66, LR: 4.87e-07\n",
      "step: 1404/2500 train loss: 0.63, LR: 4.87e-07\n",
      "step: 1405/2500 train loss: 0.66, LR: 4.86e-07\n",
      "step: 1406/2500 train loss: 0.67, LR: 4.86e-07\n",
      "step: 1407/2500 train loss: 0.71, LR: 4.85e-07\n",
      "step: 1408/2500 train loss: 0.68, LR: 4.85e-07\n",
      "step: 1409/2500 train loss: 0.64, LR: 4.84e-07\n",
      "step: 1410/2500 train loss: 0.74, LR: 4.84e-07\n",
      "step: 1411/2500 train loss: 0.82, LR: 4.84e-07\n",
      "step: 1412/2500 train loss: 0.74, LR: 4.83e-07\n",
      "step: 1413/2500 train loss: 0.79, LR: 4.83e-07\n",
      "step: 1414/2500 train loss: 0.72, LR: 4.82e-07\n",
      "step: 1415/2500 train loss: 0.69, LR: 4.82e-07\n",
      "step: 1416/2500 train loss: 0.7, LR: 4.81e-07\n",
      "step: 1417/2500 train loss: 0.76, LR: 4.81e-07\n",
      "step: 1418/2500 train loss: 0.68, LR: 4.80e-07\n",
      "step: 1419/2500 train loss: 0.76, LR: 4.80e-07\n",
      "step: 1420/2500 train loss: 0.7, LR: 4.80e-07\n",
      "step: 1421/2500 train loss: 0.71, LR: 4.79e-07\n",
      "step: 1422/2500 train loss: 0.75, LR: 4.79e-07\n",
      "step: 1423/2500 train loss: 0.74, LR: 4.78e-07\n",
      "step: 1424/2500 train loss: 0.67, LR: 4.78e-07\n",
      "step: 1425/2500 train loss: 0.7, LR: 4.77e-07\n",
      "step: 1426/2500 train loss: 0.67, LR: 4.77e-07\n",
      "step: 1427/2500 train loss: 0.74, LR: 4.76e-07\n",
      "step: 1428/2500 train loss: 0.67, LR: 4.76e-07\n",
      "step: 1429/2500 train loss: 0.7, LR: 4.76e-07\n",
      "step: 1430/2500 train loss: 0.75, LR: 4.75e-07\n",
      "step: 1431/2500 train loss: 0.72, LR: 4.75e-07\n",
      "step: 1432/2500 train loss: 0.63, LR: 4.74e-07\n",
      "step: 1433/2500 train loss: 0.74, LR: 4.74e-07\n",
      "step: 1434/2500 train loss: 0.8, LR: 4.73e-07\n",
      "step: 1435/2500 train loss: 0.82, LR: 4.73e-07\n",
      "step: 1436/2500 train loss: 0.86, LR: 4.72e-07\n",
      "step: 1437/2500 train loss: 0.66, LR: 4.72e-07\n",
      "step: 1438/2500 train loss: 0.66, LR: 4.72e-07\n",
      "step: 1439/2500 train loss: 0.64, LR: 4.71e-07\n",
      "step: 1440/2500 train loss: 0.67, LR: 4.71e-07\n",
      "step: 1441/2500 train loss: 0.75, LR: 4.70e-07\n",
      "step: 1442/2500 train loss: 0.7, LR: 4.70e-07\n",
      "step: 1443/2500 train loss: 0.81, LR: 4.69e-07\n",
      "step: 1444/2500 train loss: 0.76, LR: 4.69e-07\n",
      "step: 1445/2500 train loss: 0.69, LR: 4.68e-07\n",
      "step: 1446/2500 train loss: 0.72, LR: 4.68e-07\n",
      "step: 1447/2500 train loss: 0.64, LR: 4.68e-07\n",
      "step: 1448/2500 train loss: 0.75, LR: 4.67e-07\n",
      "step: 1449/2500 train loss: 0.76, LR: 4.67e-07\n",
      "step: 1450/2500 train loss: 0.8, LR: 4.66e-07\n",
      "step: 1451/2500 train loss: 0.76, LR: 4.66e-07\n",
      "step: 1452/2500 train loss: 0.69, LR: 4.65e-07\n",
      "step: 1453/2500 train loss: 0.72, LR: 4.65e-07\n",
      "step: 1454/2500 train loss: 0.8, LR: 4.64e-07\n",
      "step: 1455/2500 train loss: 0.76, LR: 4.64e-07\n",
      "step: 1456/2500 train loss: 0.75, LR: 4.64e-07\n",
      "step: 1457/2500 train loss: 0.58, LR: 4.63e-07\n",
      "step: 1458/2500 train loss: 0.75, LR: 4.63e-07\n",
      "step: 1459/2500 train loss: 0.67, LR: 4.62e-07\n",
      "step: 1460/2500 train loss: 0.69, LR: 4.62e-07\n",
      "step: 1461/2500 train loss: 0.7, LR: 4.61e-07\n",
      "step: 1462/2500 train loss: 0.8, LR: 4.61e-07\n",
      "step: 1463/2500 train loss: 0.72, LR: 4.60e-07\n",
      "step: 1464/2500 train loss: 0.73, LR: 4.60e-07\n",
      "step: 1465/2500 train loss: 0.64, LR: 4.60e-07\n",
      "step: 1466/2500 train loss: 0.68, LR: 4.59e-07\n",
      "step: 1467/2500 train loss: 0.74, LR: 4.59e-07\n",
      "step: 1468/2500 train loss: 0.65, LR: 4.58e-07\n",
      "step: 1469/2500 train loss: 0.69, LR: 4.58e-07\n",
      "step: 1470/2500 train loss: 0.66, LR: 4.57e-07\n",
      "step: 1471/2500 train loss: 0.79, LR: 4.57e-07\n",
      "step: 1472/2500 train loss: 0.8, LR: 4.56e-07\n",
      "step: 1473/2500 train loss: 0.66, LR: 4.56e-07\n",
      "step: 1474/2500 train loss: 0.61, LR: 4.56e-07\n",
      "step: 1475/2500 train loss: 0.76, LR: 4.55e-07\n",
      "step: 1476/2500 train loss: 0.7, LR: 4.55e-07\n",
      "step: 1477/2500 train loss: 0.77, LR: 4.54e-07\n",
      "step: 1478/2500 train loss: 0.75, LR: 4.54e-07\n",
      "step: 1479/2500 train loss: 0.59, LR: 4.53e-07\n",
      "step: 1480/2500 train loss: 0.72, LR: 4.53e-07\n",
      "step: 1481/2500 train loss: 0.61, LR: 4.52e-07\n",
      "step: 1482/2500 train loss: 0.65, LR: 4.52e-07\n",
      "step: 1483/2500 train loss: 0.65, LR: 4.52e-07\n",
      "step: 1484/2500 train loss: 0.69, LR: 4.51e-07\n",
      "step: 1485/2500 train loss: 0.66, LR: 4.51e-07\n",
      "step: 1486/2500 train loss: 0.66, LR: 4.50e-07\n",
      "step: 1487/2500 train loss: 0.8, LR: 4.50e-07\n",
      "step: 1488/2500 train loss: 0.68, LR: 4.49e-07\n",
      "step: 1489/2500 train loss: 0.69, LR: 4.49e-07\n",
      "step: 1490/2500 train loss: 0.79, LR: 4.48e-07\n",
      "step: 1491/2500 train loss: 0.69, LR: 4.48e-07\n",
      "step: 1492/2500 train loss: 0.76, LR: 4.48e-07\n",
      "step: 1493/2500 train loss: 0.68, LR: 4.47e-07\n",
      "step: 1494/2500 train loss: 0.7, LR: 4.47e-07\n",
      "step: 1495/2500 train loss: 0.74, LR: 4.46e-07\n",
      "step: 1496/2500 train loss: 0.7, LR: 4.46e-07\n",
      "step: 1497/2500 train loss: 0.67, LR: 4.45e-07\n",
      "step: 1498/2500 train loss: 0.72, LR: 4.45e-07\n",
      "step: 1499/2500 train loss: 0.78, LR: 4.44e-07\n",
      "step: 1500/2500 eval loss: 0.69\n",
      "step: 1500/2500 train loss: 0.69, LR: 4.44e-07\n",
      "step: 1501/2500 train loss: 0.84, LR: 4.44e-07\n",
      "step: 1502/2500 train loss: 0.75, LR: 4.43e-07\n",
      "step: 1503/2500 train loss: 0.7, LR: 4.43e-07\n",
      "step: 1504/2500 train loss: 0.59, LR: 4.42e-07\n",
      "step: 1505/2500 train loss: 0.72, LR: 4.42e-07\n",
      "step: 1506/2500 train loss: 0.76, LR: 4.41e-07\n",
      "step: 1507/2500 train loss: 0.72, LR: 4.41e-07\n",
      "step: 1508/2500 train loss: 0.6, LR: 4.40e-07\n",
      "step: 1509/2500 train loss: 0.63, LR: 4.40e-07\n",
      "step: 1510/2500 train loss: 0.63, LR: 4.40e-07\n",
      "step: 1511/2500 train loss: 0.81, LR: 4.39e-07\n",
      "step: 1512/2500 train loss: 0.74, LR: 4.39e-07\n",
      "step: 1513/2500 train loss: 0.73, LR: 4.38e-07\n",
      "step: 1514/2500 train loss: 0.86, LR: 4.38e-07\n",
      "step: 1515/2500 train loss: 0.76, LR: 4.37e-07\n",
      "step: 1516/2500 train loss: 0.64, LR: 4.37e-07\n",
      "step: 1517/2500 train loss: 0.65, LR: 4.36e-07\n",
      "step: 1518/2500 train loss: 0.85, LR: 4.36e-07\n",
      "step: 1519/2500 train loss: 0.66, LR: 4.36e-07\n",
      "step: 1520/2500 train loss: 0.83, LR: 4.35e-07\n",
      "step: 1521/2500 train loss: 0.71, LR: 4.35e-07\n",
      "step: 1522/2500 train loss: 0.74, LR: 4.34e-07\n",
      "step: 1523/2500 train loss: 0.68, LR: 4.34e-07\n",
      "step: 1524/2500 train loss: 0.88, LR: 4.33e-07\n",
      "step: 1525/2500 train loss: 0.68, LR: 4.33e-07\n",
      "step: 1526/2500 train loss: 0.72, LR: 4.32e-07\n",
      "step: 1527/2500 train loss: 0.66, LR: 4.32e-07\n",
      "step: 1528/2500 train loss: 0.69, LR: 4.32e-07\n",
      "step: 1529/2500 train loss: 0.68, LR: 4.31e-07\n",
      "step: 1530/2500 train loss: 0.7, LR: 4.31e-07\n",
      "step: 1531/2500 train loss: 0.71, LR: 4.30e-07\n",
      "step: 1532/2500 train loss: 0.64, LR: 4.30e-07\n",
      "step: 1533/2500 train loss: 0.77, LR: 4.29e-07\n",
      "step: 1534/2500 train loss: 0.79, LR: 4.29e-07\n",
      "step: 1535/2500 train loss: 0.68, LR: 4.28e-07\n",
      "step: 1536/2500 train loss: 0.57, LR: 4.28e-07\n",
      "step: 1537/2500 train loss: 0.69, LR: 4.28e-07\n",
      "step: 1538/2500 train loss: 0.73, LR: 4.27e-07\n",
      "step: 1539/2500 train loss: 0.72, LR: 4.27e-07\n",
      "step: 1540/2500 train loss: 0.68, LR: 4.26e-07\n",
      "step: 1541/2500 train loss: 0.65, LR: 4.26e-07\n",
      "step: 1542/2500 train loss: 0.72, LR: 4.25e-07\n",
      "step: 1543/2500 train loss: 0.76, LR: 4.25e-07\n",
      "step: 1544/2500 train loss: 0.7, LR: 4.24e-07\n",
      "step: 1545/2500 train loss: 0.65, LR: 4.24e-07\n",
      "step: 1546/2500 train loss: 0.69, LR: 4.24e-07\n",
      "step: 1547/2500 train loss: 0.75, LR: 4.23e-07\n",
      "step: 1548/2500 train loss: 0.71, LR: 4.23e-07\n",
      "step: 1549/2500 train loss: 0.69, LR: 4.22e-07\n",
      "step: 1550/2500 train loss: 0.68, LR: 4.22e-07\n",
      "step: 1551/2500 train loss: 0.7, LR: 4.21e-07\n",
      "step: 1552/2500 train loss: 0.69, LR: 4.21e-07\n",
      "step: 1553/2500 train loss: 0.72, LR: 4.20e-07\n",
      "step: 1554/2500 train loss: 0.8, LR: 4.20e-07\n",
      "step: 1555/2500 train loss: 0.73, LR: 4.20e-07\n",
      "step: 1556/2500 train loss: 0.57, LR: 4.19e-07\n",
      "step: 1557/2500 train loss: 0.75, LR: 4.19e-07\n",
      "step: 1558/2500 train loss: 0.73, LR: 4.18e-07\n",
      "step: 1559/2500 train loss: 0.73, LR: 4.18e-07\n",
      "step: 1560/2500 train loss: 0.71, LR: 4.17e-07\n",
      "step: 1561/2500 train loss: 0.7, LR: 4.17e-07\n",
      "step: 1562/2500 train loss: 0.76, LR: 4.16e-07\n",
      "step: 1563/2500 train loss: 0.75, LR: 4.16e-07\n",
      "step: 1564/2500 train loss: 0.75, LR: 4.16e-07\n",
      "step: 1565/2500 train loss: 0.73, LR: 4.15e-07\n",
      "step: 1566/2500 train loss: 0.63, LR: 4.15e-07\n",
      "step: 1567/2500 train loss: 0.64, LR: 4.14e-07\n",
      "step: 1568/2500 train loss: 0.7, LR: 4.14e-07\n",
      "step: 1569/2500 train loss: 0.69, LR: 4.13e-07\n",
      "step: 1570/2500 train loss: 0.69, LR: 4.13e-07\n",
      "step: 1571/2500 train loss: 0.79, LR: 4.12e-07\n",
      "step: 1572/2500 train loss: 0.63, LR: 4.12e-07\n",
      "step: 1573/2500 train loss: 0.62, LR: 4.12e-07\n",
      "step: 1574/2500 train loss: 0.65, LR: 4.11e-07\n",
      "step: 1575/2500 train loss: 0.77, LR: 4.11e-07\n",
      "step: 1576/2500 train loss: 0.67, LR: 4.10e-07\n",
      "step: 1577/2500 train loss: 0.65, LR: 4.10e-07\n",
      "step: 1578/2500 train loss: 0.75, LR: 4.09e-07\n",
      "step: 1579/2500 train loss: 0.67, LR: 4.09e-07\n",
      "step: 1580/2500 train loss: 0.79, LR: 4.08e-07\n",
      "step: 1581/2500 train loss: 0.65, LR: 4.08e-07\n",
      "step: 1582/2500 train loss: 0.61, LR: 4.08e-07\n",
      "step: 1583/2500 train loss: 0.72, LR: 4.07e-07\n",
      "step: 1584/2500 train loss: 0.73, LR: 4.07e-07\n",
      "step: 1585/2500 train loss: 0.66, LR: 4.06e-07\n",
      "step: 1586/2500 train loss: 0.73, LR: 4.06e-07\n",
      "step: 1587/2500 train loss: 0.69, LR: 4.05e-07\n",
      "step: 1588/2500 train loss: 0.73, LR: 4.05e-07\n",
      "step: 1589/2500 train loss: 0.6, LR: 4.04e-07\n",
      "step: 1590/2500 train loss: 0.74, LR: 4.04e-07\n",
      "step: 1591/2500 train loss: 0.76, LR: 4.04e-07\n",
      "step: 1592/2500 train loss: 0.75, LR: 4.03e-07\n",
      "step: 1593/2500 train loss: 0.6, LR: 4.03e-07\n",
      "step: 1594/2500 train loss: 0.61, LR: 4.02e-07\n",
      "step: 1595/2500 train loss: 0.76, LR: 4.02e-07\n",
      "step: 1596/2500 train loss: 0.68, LR: 4.01e-07\n",
      "step: 1597/2500 train loss: 0.66, LR: 4.01e-07\n",
      "step: 1598/2500 train loss: 0.7, LR: 4.00e-07\n",
      "step: 1599/2500 train loss: 0.61, LR: 4.00e-07\n",
      "step: 1600/2500 train loss: 0.76, LR: 4.00e-07\n",
      "step: 1601/2500 train loss: 0.75, LR: 3.99e-07\n",
      "step: 1602/2500 train loss: 0.81, LR: 3.99e-07\n",
      "step: 1603/2500 train loss: 0.82, LR: 3.98e-07\n",
      "step: 1604/2500 train loss: 0.74, LR: 3.98e-07\n",
      "step: 1605/2500 train loss: 0.84, LR: 3.97e-07\n",
      "step: 1606/2500 train loss: 0.79, LR: 3.97e-07\n",
      "step: 1607/2500 train loss: 0.69, LR: 3.96e-07\n",
      "step: 1608/2500 train loss: 0.71, LR: 3.96e-07\n",
      "step: 1609/2500 train loss: 0.64, LR: 3.96e-07\n",
      "step: 1610/2500 train loss: 0.72, LR: 3.95e-07\n",
      "step: 1611/2500 train loss: 0.74, LR: 3.95e-07\n",
      "step: 1612/2500 train loss: 0.58, LR: 3.94e-07\n",
      "step: 1613/2500 train loss: 0.68, LR: 3.94e-07\n",
      "step: 1614/2500 train loss: 0.67, LR: 3.93e-07\n",
      "step: 1615/2500 train loss: 0.67, LR: 3.93e-07\n",
      "step: 1616/2500 train loss: 0.67, LR: 3.92e-07\n",
      "step: 1617/2500 train loss: 0.6, LR: 3.92e-07\n",
      "step: 1618/2500 train loss: 0.61, LR: 3.92e-07\n",
      "step: 1619/2500 train loss: 0.7, LR: 3.91e-07\n",
      "step: 1620/2500 train loss: 0.6, LR: 3.91e-07\n",
      "step: 1621/2500 train loss: 0.72, LR: 3.90e-07\n",
      "step: 1622/2500 train loss: 0.76, LR: 3.90e-07\n",
      "step: 1623/2500 train loss: 0.74, LR: 3.89e-07\n",
      "step: 1624/2500 train loss: 0.69, LR: 3.89e-07\n",
      "step: 1625/2500 train loss: 0.67, LR: 3.88e-07\n",
      "step: 1626/2500 train loss: 0.7, LR: 3.88e-07\n",
      "step: 1627/2500 train loss: 0.81, LR: 3.88e-07\n",
      "step: 1628/2500 train loss: 0.7, LR: 3.87e-07\n",
      "step: 1629/2500 train loss: 0.7, LR: 3.87e-07\n",
      "step: 1630/2500 train loss: 0.76, LR: 3.86e-07\n",
      "step: 1631/2500 train loss: 0.74, LR: 3.86e-07\n",
      "step: 1632/2500 train loss: 0.8, LR: 3.85e-07\n",
      "step: 1633/2500 train loss: 0.7, LR: 3.85e-07\n",
      "step: 1634/2500 train loss: 0.73, LR: 3.84e-07\n",
      "step: 1635/2500 train loss: 0.69, LR: 3.84e-07\n",
      "step: 1636/2500 train loss: 0.69, LR: 3.84e-07\n",
      "step: 1637/2500 train loss: 0.79, LR: 3.83e-07\n",
      "step: 1638/2500 train loss: 0.78, LR: 3.83e-07\n",
      "step: 1639/2500 train loss: 0.7, LR: 3.82e-07\n",
      "step: 1640/2500 train loss: 0.74, LR: 3.82e-07\n",
      "step: 1641/2500 train loss: 0.72, LR: 3.81e-07\n",
      "step: 1642/2500 train loss: 0.64, LR: 3.81e-07\n",
      "step: 1643/2500 train loss: 0.67, LR: 3.80e-07\n",
      "step: 1644/2500 train loss: 0.63, LR: 3.80e-07\n",
      "step: 1645/2500 train loss: 0.64, LR: 3.80e-07\n",
      "step: 1646/2500 train loss: 0.61, LR: 3.79e-07\n",
      "step: 1647/2500 train loss: 0.67, LR: 3.79e-07\n",
      "step: 1648/2500 train loss: 0.68, LR: 3.78e-07\n",
      "step: 1649/2500 train loss: 0.7, LR: 3.78e-07\n",
      "step: 1650/2500 train loss: 0.69, LR: 3.77e-07\n",
      "step: 1651/2500 train loss: 0.64, LR: 3.77e-07\n",
      "step: 1652/2500 train loss: 0.7, LR: 3.76e-07\n",
      "step: 1653/2500 train loss: 0.78, LR: 3.76e-07\n",
      "step: 1654/2500 train loss: 0.72, LR: 3.76e-07\n",
      "step: 1655/2500 train loss: 0.68, LR: 3.75e-07\n",
      "step: 1656/2500 train loss: 0.6, LR: 3.75e-07\n",
      "step: 1657/2500 train loss: 0.71, LR: 3.74e-07\n",
      "step: 1658/2500 train loss: 0.82, LR: 3.74e-07\n",
      "step: 1659/2500 train loss: 0.82, LR: 3.73e-07\n",
      "step: 1660/2500 train loss: 0.62, LR: 3.73e-07\n",
      "step: 1661/2500 train loss: 0.69, LR: 3.72e-07\n",
      "step: 1662/2500 train loss: 0.67, LR: 3.72e-07\n",
      "step: 1663/2500 train loss: 0.66, LR: 3.72e-07\n",
      "step: 1664/2500 train loss: 0.65, LR: 3.71e-07\n",
      "step: 1665/2500 train loss: 0.79, LR: 3.71e-07\n",
      "step: 1666/2500 train loss: 0.63, LR: 3.70e-07\n",
      "step: 1667/2500 train loss: 0.79, LR: 3.70e-07\n",
      "step: 1668/2500 train loss: 0.68, LR: 3.69e-07\n",
      "step: 1669/2500 train loss: 0.69, LR: 3.69e-07\n",
      "step: 1670/2500 train loss: 0.75, LR: 3.68e-07\n",
      "step: 1671/2500 train loss: 0.67, LR: 3.68e-07\n",
      "step: 1672/2500 train loss: 0.71, LR: 3.68e-07\n",
      "step: 1673/2500 train loss: 0.72, LR: 3.67e-07\n",
      "step: 1674/2500 train loss: 0.69, LR: 3.67e-07\n",
      "step: 1675/2500 train loss: 0.64, LR: 3.66e-07\n",
      "step: 1676/2500 train loss: 0.66, LR: 3.66e-07\n",
      "step: 1677/2500 train loss: 0.7, LR: 3.65e-07\n",
      "step: 1678/2500 train loss: 0.63, LR: 3.65e-07\n",
      "step: 1679/2500 train loss: 0.75, LR: 3.64e-07\n",
      "step: 1680/2500 train loss: 0.67, LR: 3.64e-07\n",
      "step: 1681/2500 train loss: 0.77, LR: 3.64e-07\n",
      "step: 1682/2500 train loss: 0.71, LR: 3.63e-07\n",
      "step: 1683/2500 train loss: 0.69, LR: 3.63e-07\n",
      "step: 1684/2500 train loss: 0.81, LR: 3.62e-07\n",
      "step: 1685/2500 train loss: 0.72, LR: 3.62e-07\n",
      "step: 1686/2500 train loss: 0.67, LR: 3.61e-07\n",
      "step: 1687/2500 train loss: 0.69, LR: 3.61e-07\n",
      "step: 1688/2500 train loss: 0.74, LR: 3.60e-07\n",
      "step: 1689/2500 train loss: 0.64, LR: 3.60e-07\n",
      "step: 1690/2500 train loss: 0.67, LR: 3.60e-07\n",
      "step: 1691/2500 train loss: 0.62, LR: 3.59e-07\n",
      "step: 1692/2500 train loss: 0.73, LR: 3.59e-07\n",
      "step: 1693/2500 train loss: 0.68, LR: 3.58e-07\n",
      "step: 1694/2500 train loss: 0.68, LR: 3.58e-07\n",
      "step: 1695/2500 train loss: 0.75, LR: 3.57e-07\n",
      "step: 1696/2500 train loss: 0.69, LR: 3.57e-07\n",
      "step: 1697/2500 train loss: 0.8, LR: 3.56e-07\n",
      "step: 1698/2500 train loss: 0.69, LR: 3.56e-07\n",
      "step: 1699/2500 train loss: 0.65, LR: 3.56e-07\n",
      "step: 1700/2500 train loss: 0.67, LR: 3.55e-07\n",
      "step: 1701/2500 train loss: 0.63, LR: 3.55e-07\n",
      "step: 1702/2500 train loss: 0.62, LR: 3.54e-07\n",
      "step: 1703/2500 train loss: 0.66, LR: 3.54e-07\n",
      "step: 1704/2500 train loss: 0.66, LR: 3.53e-07\n",
      "step: 1705/2500 train loss: 0.63, LR: 3.53e-07\n",
      "step: 1706/2500 train loss: 0.58, LR: 3.52e-07\n",
      "step: 1707/2500 train loss: 0.7, LR: 3.52e-07\n",
      "step: 1708/2500 train loss: 0.76, LR: 3.52e-07\n",
      "step: 1709/2500 train loss: 0.77, LR: 3.51e-07\n",
      "step: 1710/2500 train loss: 0.63, LR: 3.51e-07\n",
      "step: 1711/2500 train loss: 0.72, LR: 3.50e-07\n",
      "step: 1712/2500 train loss: 0.67, LR: 3.50e-07\n",
      "step: 1713/2500 train loss: 0.7, LR: 3.49e-07\n",
      "step: 1714/2500 train loss: 0.72, LR: 3.49e-07\n",
      "step: 1715/2500 train loss: 0.72, LR: 3.48e-07\n",
      "step: 1716/2500 train loss: 0.71, LR: 3.48e-07\n",
      "step: 1717/2500 train loss: 0.62, LR: 3.48e-07\n",
      "step: 1718/2500 train loss: 0.71, LR: 3.47e-07\n",
      "step: 1719/2500 train loss: 0.72, LR: 3.47e-07\n",
      "step: 1720/2500 train loss: 0.59, LR: 3.46e-07\n",
      "step: 1721/2500 train loss: 0.65, LR: 3.46e-07\n",
      "step: 1722/2500 train loss: 0.64, LR: 3.45e-07\n",
      "step: 1723/2500 train loss: 0.66, LR: 3.45e-07\n",
      "step: 1724/2500 train loss: 0.74, LR: 3.44e-07\n",
      "step: 1725/2500 train loss: 0.75, LR: 3.44e-07\n",
      "step: 1726/2500 train loss: 0.71, LR: 3.44e-07\n",
      "step: 1727/2500 train loss: 0.68, LR: 3.43e-07\n",
      "step: 1728/2500 train loss: 0.66, LR: 3.43e-07\n",
      "step: 1729/2500 train loss: 0.72, LR: 3.42e-07\n",
      "step: 1730/2500 train loss: 0.74, LR: 3.42e-07\n",
      "step: 1731/2500 train loss: 0.66, LR: 3.41e-07\n",
      "step: 1732/2500 train loss: 0.76, LR: 3.41e-07\n",
      "step: 1733/2500 train loss: 0.63, LR: 3.40e-07\n",
      "step: 1734/2500 train loss: 0.61, LR: 3.40e-07\n",
      "step: 1735/2500 train loss: 0.67, LR: 3.40e-07\n",
      "step: 1736/2500 train loss: 0.68, LR: 3.39e-07\n",
      "step: 1737/2500 train loss: 0.71, LR: 3.39e-07\n",
      "step: 1738/2500 train loss: 0.68, LR: 3.38e-07\n",
      "step: 1739/2500 train loss: 0.7, LR: 3.38e-07\n",
      "step: 1740/2500 train loss: 0.73, LR: 3.37e-07\n",
      "step: 1741/2500 train loss: 0.75, LR: 3.37e-07\n",
      "step: 1742/2500 train loss: 0.62, LR: 3.36e-07\n",
      "step: 1743/2500 train loss: 0.79, LR: 3.36e-07\n",
      "step: 1744/2500 train loss: 0.67, LR: 3.36e-07\n",
      "step: 1745/2500 train loss: 0.73, LR: 3.35e-07\n",
      "step: 1746/2500 train loss: 0.68, LR: 3.35e-07\n",
      "step: 1747/2500 train loss: 0.72, LR: 3.34e-07\n",
      "step: 1748/2500 train loss: 0.7, LR: 3.34e-07\n",
      "step: 1749/2500 train loss: 0.69, LR: 3.33e-07\n",
      "step: 1750/2500 eval loss: 0.69\n",
      "step: 1750/2500 train loss: 0.73, LR: 3.33e-07\n",
      "step: 1751/2500 train loss: 0.69, LR: 3.32e-07\n",
      "step: 1752/2500 train loss: 0.68, LR: 3.32e-07\n",
      "step: 1753/2500 train loss: 0.75, LR: 3.32e-07\n",
      "step: 1754/2500 train loss: 0.55, LR: 3.31e-07\n",
      "step: 1755/2500 train loss: 0.71, LR: 3.31e-07\n",
      "step: 1756/2500 train loss: 0.73, LR: 3.30e-07\n",
      "step: 1757/2500 train loss: 0.64, LR: 3.30e-07\n",
      "step: 1758/2500 train loss: 0.77, LR: 3.29e-07\n",
      "step: 1759/2500 train loss: 0.71, LR: 3.29e-07\n",
      "step: 1760/2500 train loss: 0.74, LR: 3.28e-07\n",
      "step: 1761/2500 train loss: 0.73, LR: 3.28e-07\n",
      "step: 1762/2500 train loss: 0.7, LR: 3.28e-07\n",
      "step: 1763/2500 train loss: 0.82, LR: 3.27e-07\n",
      "step: 1764/2500 train loss: 0.72, LR: 3.27e-07\n",
      "step: 1765/2500 train loss: 0.63, LR: 3.26e-07\n",
      "step: 1766/2500 train loss: 0.73, LR: 3.26e-07\n",
      "step: 1767/2500 train loss: 0.65, LR: 3.25e-07\n",
      "step: 1768/2500 train loss: 0.66, LR: 3.25e-07\n",
      "step: 1769/2500 train loss: 0.69, LR: 3.24e-07\n",
      "step: 1770/2500 train loss: 0.75, LR: 3.24e-07\n",
      "step: 1771/2500 train loss: 0.78, LR: 3.24e-07\n",
      "step: 1772/2500 train loss: 0.59, LR: 3.23e-07\n",
      "step: 1773/2500 train loss: 0.71, LR: 3.23e-07\n",
      "step: 1774/2500 train loss: 0.87, LR: 3.22e-07\n",
      "step: 1775/2500 train loss: 0.74, LR: 3.22e-07\n",
      "step: 1776/2500 train loss: 0.69, LR: 3.21e-07\n",
      "step: 1777/2500 train loss: 0.76, LR: 3.21e-07\n",
      "step: 1778/2500 train loss: 0.63, LR: 3.20e-07\n",
      "step: 1779/2500 train loss: 0.8, LR: 3.20e-07\n",
      "step: 1780/2500 train loss: 0.68, LR: 3.20e-07\n",
      "step: 1781/2500 train loss: 0.66, LR: 3.19e-07\n",
      "step: 1782/2500 train loss: 0.68, LR: 3.19e-07\n",
      "step: 1783/2500 train loss: 0.68, LR: 3.18e-07\n",
      "step: 1784/2500 train loss: 0.66, LR: 3.18e-07\n",
      "step: 1785/2500 train loss: 0.63, LR: 3.17e-07\n",
      "step: 1786/2500 train loss: 0.7, LR: 3.17e-07\n",
      "step: 1787/2500 train loss: 0.72, LR: 3.16e-07\n",
      "step: 1788/2500 train loss: 0.7, LR: 3.16e-07\n",
      "step: 1789/2500 train loss: 0.65, LR: 3.16e-07\n",
      "step: 1790/2500 train loss: 0.68, LR: 3.15e-07\n",
      "step: 1791/2500 train loss: 0.68, LR: 3.15e-07\n",
      "step: 1792/2500 train loss: 0.62, LR: 3.14e-07\n",
      "step: 1793/2500 train loss: 0.71, LR: 3.14e-07\n",
      "step: 1794/2500 train loss: 0.61, LR: 3.13e-07\n",
      "step: 1795/2500 train loss: 0.64, LR: 3.13e-07\n",
      "step: 1796/2500 train loss: 0.73, LR: 3.12e-07\n",
      "step: 1797/2500 train loss: 0.61, LR: 3.12e-07\n",
      "step: 1798/2500 train loss: 0.71, LR: 3.12e-07\n",
      "step: 1799/2500 train loss: 0.61, LR: 3.11e-07\n",
      "step: 1800/2500 train loss: 0.65, LR: 3.11e-07\n",
      "step: 1801/2500 train loss: 0.74, LR: 3.10e-07\n",
      "step: 1802/2500 train loss: 0.59, LR: 3.10e-07\n",
      "step: 1803/2500 train loss: 0.6, LR: 3.09e-07\n",
      "step: 1804/2500 train loss: 0.75, LR: 3.09e-07\n",
      "step: 1805/2500 train loss: 0.54, LR: 3.08e-07\n",
      "step: 1806/2500 train loss: 0.74, LR: 3.08e-07\n",
      "step: 1807/2500 train loss: 0.6, LR: 3.08e-07\n",
      "step: 1808/2500 train loss: 0.72, LR: 3.07e-07\n",
      "step: 1809/2500 train loss: 0.79, LR: 3.07e-07\n",
      "step: 1810/2500 train loss: 0.76, LR: 3.06e-07\n",
      "step: 1811/2500 train loss: 0.6, LR: 3.06e-07\n",
      "step: 1812/2500 train loss: 0.58, LR: 3.05e-07\n",
      "step: 1813/2500 train loss: 0.72, LR: 3.05e-07\n",
      "step: 1814/2500 train loss: 0.74, LR: 3.04e-07\n",
      "step: 1815/2500 train loss: 0.62, LR: 3.04e-07\n",
      "step: 1816/2500 train loss: 0.64, LR: 3.04e-07\n",
      "step: 1817/2500 train loss: 0.64, LR: 3.03e-07\n",
      "step: 1818/2500 train loss: 0.56, LR: 3.03e-07\n",
      "step: 1819/2500 train loss: 0.68, LR: 3.02e-07\n",
      "step: 1820/2500 train loss: 0.65, LR: 3.02e-07\n",
      "step: 1821/2500 train loss: 0.71, LR: 3.01e-07\n",
      "step: 1822/2500 train loss: 0.66, LR: 3.01e-07\n",
      "step: 1823/2500 train loss: 0.69, LR: 3.00e-07\n",
      "step: 1824/2500 train loss: 0.67, LR: 3.00e-07\n",
      "step: 1825/2500 train loss: 0.76, LR: 3.00e-07\n",
      "step: 1826/2500 train loss: 0.7, LR: 2.99e-07\n",
      "step: 1827/2500 train loss: 0.76, LR: 2.99e-07\n",
      "step: 1828/2500 train loss: 0.65, LR: 2.98e-07\n",
      "step: 1829/2500 train loss: 0.68, LR: 2.98e-07\n",
      "step: 1830/2500 train loss: 0.65, LR: 2.97e-07\n",
      "step: 1831/2500 train loss: 0.72, LR: 2.97e-07\n",
      "step: 1832/2500 train loss: 0.75, LR: 2.96e-07\n",
      "step: 1833/2500 train loss: 0.85, LR: 2.96e-07\n",
      "step: 1834/2500 train loss: 0.61, LR: 2.96e-07\n",
      "step: 1835/2500 train loss: 0.64, LR: 2.95e-07\n",
      "step: 1836/2500 train loss: 0.8, LR: 2.95e-07\n",
      "step: 1837/2500 train loss: 0.64, LR: 2.94e-07\n",
      "step: 1838/2500 train loss: 0.7, LR: 2.94e-07\n",
      "step: 1839/2500 train loss: 0.71, LR: 2.93e-07\n",
      "step: 1840/2500 train loss: 0.78, LR: 2.93e-07\n",
      "step: 1841/2500 train loss: 0.67, LR: 2.92e-07\n",
      "step: 1842/2500 train loss: 0.78, LR: 2.92e-07\n",
      "step: 1843/2500 train loss: 0.68, LR: 2.92e-07\n",
      "step: 1844/2500 train loss: 0.77, LR: 2.91e-07\n",
      "step: 1845/2500 train loss: 0.76, LR: 2.91e-07\n",
      "step: 1846/2500 train loss: 0.67, LR: 2.90e-07\n",
      "step: 1847/2500 train loss: 0.59, LR: 2.90e-07\n",
      "step: 1848/2500 train loss: 0.86, LR: 2.89e-07\n",
      "step: 1849/2500 train loss: 0.79, LR: 2.89e-07\n",
      "step: 1850/2500 train loss: 0.71, LR: 2.88e-07\n",
      "step: 1851/2500 train loss: 0.7, LR: 2.88e-07\n",
      "step: 1852/2500 train loss: 0.62, LR: 2.88e-07\n",
      "step: 1853/2500 train loss: 0.73, LR: 2.87e-07\n",
      "step: 1854/2500 train loss: 0.71, LR: 2.87e-07\n",
      "step: 1855/2500 train loss: 0.68, LR: 2.86e-07\n",
      "step: 1856/2500 train loss: 0.64, LR: 2.86e-07\n",
      "step: 1857/2500 train loss: 0.75, LR: 2.85e-07\n",
      "step: 1858/2500 train loss: 0.7, LR: 2.85e-07\n",
      "step: 1859/2500 train loss: 0.84, LR: 2.84e-07\n",
      "step: 1860/2500 train loss: 0.79, LR: 2.84e-07\n",
      "step: 1861/2500 train loss: 0.73, LR: 2.84e-07\n",
      "step: 1862/2500 train loss: 0.67, LR: 2.83e-07\n",
      "step: 1863/2500 train loss: 0.67, LR: 2.83e-07\n",
      "step: 1864/2500 train loss: 0.71, LR: 2.82e-07\n",
      "step: 1865/2500 train loss: 0.77, LR: 2.82e-07\n",
      "step: 1866/2500 train loss: 0.68, LR: 2.81e-07\n",
      "step: 1867/2500 train loss: 0.57, LR: 2.81e-07\n",
      "step: 1868/2500 train loss: 0.68, LR: 2.80e-07\n",
      "step: 1869/2500 train loss: 0.75, LR: 2.80e-07\n",
      "step: 1870/2500 train loss: 0.65, LR: 2.80e-07\n",
      "step: 1871/2500 train loss: 0.78, LR: 2.79e-07\n",
      "step: 1872/2500 train loss: 0.67, LR: 2.79e-07\n",
      "step: 1873/2500 train loss: 0.75, LR: 2.78e-07\n",
      "step: 1874/2500 train loss: 0.65, LR: 2.78e-07\n",
      "step: 1875/2500 train loss: 0.65, LR: 2.77e-07\n",
      "step: 1876/2500 train loss: 0.77, LR: 2.77e-07\n",
      "step: 1877/2500 train loss: 0.83, LR: 2.76e-07\n",
      "step: 1878/2500 train loss: 0.75, LR: 2.76e-07\n",
      "step: 1879/2500 train loss: 0.61, LR: 2.76e-07\n",
      "step: 1880/2500 train loss: 0.64, LR: 2.75e-07\n",
      "step: 1881/2500 train loss: 0.64, LR: 2.75e-07\n",
      "step: 1882/2500 train loss: 0.56, LR: 2.74e-07\n",
      "step: 1883/2500 train loss: 0.59, LR: 2.74e-07\n",
      "step: 1884/2500 train loss: 0.64, LR: 2.73e-07\n",
      "step: 1885/2500 train loss: 0.69, LR: 2.73e-07\n",
      "step: 1886/2500 train loss: 0.7, LR: 2.72e-07\n",
      "step: 1887/2500 train loss: 0.63, LR: 2.72e-07\n",
      "step: 1888/2500 train loss: 0.74, LR: 2.72e-07\n",
      "step: 1889/2500 train loss: 0.66, LR: 2.71e-07\n",
      "step: 1890/2500 train loss: 0.69, LR: 2.71e-07\n",
      "step: 1891/2500 train loss: 0.57, LR: 2.70e-07\n",
      "step: 1892/2500 train loss: 0.63, LR: 2.70e-07\n",
      "step: 1893/2500 train loss: 0.83, LR: 2.69e-07\n",
      "step: 1894/2500 train loss: 0.62, LR: 2.69e-07\n",
      "step: 1895/2500 train loss: 0.77, LR: 2.68e-07\n",
      "step: 1896/2500 train loss: 0.73, LR: 2.68e-07\n",
      "step: 1897/2500 train loss: 0.72, LR: 2.68e-07\n",
      "step: 1898/2500 train loss: 0.75, LR: 2.67e-07\n",
      "step: 1899/2500 train loss: 0.67, LR: 2.67e-07\n",
      "step: 1900/2500 train loss: 0.67, LR: 2.66e-07\n",
      "step: 1901/2500 train loss: 0.79, LR: 2.66e-07\n",
      "step: 1902/2500 train loss: 0.6, LR: 2.65e-07\n",
      "step: 1903/2500 train loss: 0.71, LR: 2.65e-07\n",
      "step: 1904/2500 train loss: 0.66, LR: 2.64e-07\n",
      "step: 1905/2500 train loss: 0.73, LR: 2.64e-07\n",
      "step: 1906/2500 train loss: 0.67, LR: 2.64e-07\n",
      "step: 1907/2500 train loss: 0.81, LR: 2.63e-07\n",
      "step: 1908/2500 train loss: 0.8, LR: 2.63e-07\n",
      "step: 1909/2500 train loss: 0.73, LR: 2.62e-07\n",
      "step: 1910/2500 train loss: 0.65, LR: 2.62e-07\n",
      "step: 1911/2500 train loss: 0.67, LR: 2.61e-07\n",
      "step: 1912/2500 train loss: 0.83, LR: 2.61e-07\n",
      "step: 1913/2500 train loss: 0.71, LR: 2.60e-07\n",
      "step: 1914/2500 train loss: 0.71, LR: 2.60e-07\n",
      "step: 1915/2500 train loss: 0.76, LR: 2.60e-07\n",
      "step: 1916/2500 train loss: 0.66, LR: 2.59e-07\n",
      "step: 1917/2500 train loss: 0.68, LR: 2.59e-07\n",
      "step: 1918/2500 train loss: 0.61, LR: 2.58e-07\n",
      "step: 1919/2500 train loss: 0.85, LR: 2.58e-07\n",
      "step: 1920/2500 train loss: 0.78, LR: 2.57e-07\n",
      "step: 1921/2500 train loss: 0.64, LR: 2.57e-07\n",
      "step: 1922/2500 train loss: 0.64, LR: 2.56e-07\n",
      "step: 1923/2500 train loss: 0.81, LR: 2.56e-07\n",
      "step: 1924/2500 train loss: 0.68, LR: 2.56e-07\n",
      "step: 1925/2500 train loss: 0.67, LR: 2.55e-07\n",
      "step: 1926/2500 train loss: 0.7, LR: 2.55e-07\n",
      "step: 1927/2500 train loss: 0.72, LR: 2.54e-07\n",
      "step: 1928/2500 train loss: 0.78, LR: 2.54e-07\n",
      "step: 1929/2500 train loss: 0.64, LR: 2.53e-07\n",
      "step: 1930/2500 train loss: 0.62, LR: 2.53e-07\n",
      "step: 1931/2500 train loss: 0.64, LR: 2.52e-07\n",
      "step: 1932/2500 train loss: 0.65, LR: 2.52e-07\n",
      "step: 1933/2500 train loss: 0.73, LR: 2.52e-07\n",
      "step: 1934/2500 train loss: 0.66, LR: 2.51e-07\n",
      "step: 1935/2500 train loss: 0.69, LR: 2.51e-07\n",
      "step: 1936/2500 train loss: 0.73, LR: 2.50e-07\n",
      "step: 1937/2500 train loss: 0.78, LR: 2.50e-07\n",
      "step: 1938/2500 train loss: 0.68, LR: 2.49e-07\n",
      "step: 1939/2500 train loss: 0.7, LR: 2.49e-07\n",
      "step: 1940/2500 train loss: 0.77, LR: 2.48e-07\n",
      "step: 1941/2500 train loss: 0.6, LR: 2.48e-07\n",
      "step: 1942/2500 train loss: 0.74, LR: 2.48e-07\n",
      "step: 1943/2500 train loss: 0.74, LR: 2.47e-07\n",
      "step: 1944/2500 train loss: 0.71, LR: 2.47e-07\n",
      "step: 1945/2500 train loss: 0.82, LR: 2.46e-07\n",
      "step: 1946/2500 train loss: 0.71, LR: 2.46e-07\n",
      "step: 1947/2500 train loss: 0.78, LR: 2.45e-07\n",
      "step: 1948/2500 train loss: 0.75, LR: 2.45e-07\n",
      "step: 1949/2500 train loss: 0.84, LR: 2.44e-07\n",
      "step: 1950/2500 train loss: 0.72, LR: 2.44e-07\n",
      "step: 1951/2500 train loss: 0.69, LR: 2.44e-07\n",
      "step: 1952/2500 train loss: 0.71, LR: 2.43e-07\n",
      "step: 1953/2500 train loss: 0.77, LR: 2.43e-07\n",
      "step: 1954/2500 train loss: 0.66, LR: 2.42e-07\n",
      "step: 1955/2500 train loss: 0.69, LR: 2.42e-07\n",
      "step: 1956/2500 train loss: 0.72, LR: 2.41e-07\n",
      "step: 1957/2500 train loss: 0.58, LR: 2.41e-07\n",
      "step: 1958/2500 train loss: 0.68, LR: 2.40e-07\n",
      "step: 1959/2500 train loss: 0.72, LR: 2.40e-07\n",
      "step: 1960/2500 train loss: 0.57, LR: 2.40e-07\n",
      "step: 1961/2500 train loss: 0.61, LR: 2.39e-07\n",
      "step: 1962/2500 train loss: 0.62, LR: 2.39e-07\n",
      "step: 1963/2500 train loss: 0.78, LR: 2.38e-07\n",
      "step: 1964/2500 train loss: 0.63, LR: 2.38e-07\n",
      "step: 1965/2500 train loss: 0.72, LR: 2.37e-07\n",
      "step: 1966/2500 train loss: 0.66, LR: 2.37e-07\n",
      "step: 1967/2500 train loss: 0.77, LR: 2.36e-07\n",
      "step: 1968/2500 train loss: 0.66, LR: 2.36e-07\n",
      "step: 1969/2500 train loss: 0.71, LR: 2.36e-07\n",
      "step: 1970/2500 train loss: 0.66, LR: 2.35e-07\n",
      "step: 1971/2500 train loss: 0.67, LR: 2.35e-07\n",
      "step: 1972/2500 train loss: 0.81, LR: 2.34e-07\n",
      "step: 1973/2500 train loss: 0.71, LR: 2.34e-07\n",
      "step: 1974/2500 train loss: 0.7, LR: 2.33e-07\n",
      "step: 1975/2500 train loss: 0.67, LR: 2.33e-07\n",
      "step: 1976/2500 train loss: 0.74, LR: 2.32e-07\n",
      "step: 1977/2500 train loss: 0.68, LR: 2.32e-07\n",
      "step: 1978/2500 train loss: 0.71, LR: 2.32e-07\n",
      "step: 1979/2500 train loss: 0.66, LR: 2.31e-07\n",
      "step: 1980/2500 train loss: 0.7, LR: 2.31e-07\n",
      "step: 1981/2500 train loss: 0.73, LR: 2.30e-07\n",
      "step: 1982/2500 train loss: 0.74, LR: 2.30e-07\n",
      "step: 1983/2500 train loss: 0.77, LR: 2.29e-07\n",
      "step: 1984/2500 train loss: 0.72, LR: 2.29e-07\n",
      "step: 1985/2500 train loss: 0.73, LR: 2.28e-07\n",
      "step: 1986/2500 train loss: 0.73, LR: 2.28e-07\n",
      "step: 1987/2500 train loss: 0.61, LR: 2.28e-07\n",
      "step: 1988/2500 train loss: 0.66, LR: 2.27e-07\n",
      "step: 1989/2500 train loss: 0.67, LR: 2.27e-07\n",
      "step: 1990/2500 train loss: 0.67, LR: 2.26e-07\n",
      "step: 1991/2500 train loss: 0.61, LR: 2.26e-07\n",
      "step: 1992/2500 train loss: 0.66, LR: 2.25e-07\n",
      "step: 1993/2500 train loss: 0.84, LR: 2.25e-07\n",
      "step: 1994/2500 train loss: 0.59, LR: 2.24e-07\n",
      "step: 1995/2500 train loss: 0.73, LR: 2.24e-07\n",
      "step: 1996/2500 train loss: 0.73, LR: 2.24e-07\n",
      "step: 1997/2500 train loss: 0.66, LR: 2.23e-07\n",
      "step: 1998/2500 train loss: 0.76, LR: 2.23e-07\n",
      "step: 1999/2500 train loss: 0.74, LR: 2.22e-07\n",
      "step: 2000/2500 eval loss: 0.68\n",
      "step: 2000/2500 train loss: 0.69, LR: 2.22e-07\n",
      "step: 2001/2500 train loss: 0.79, LR: 2.21e-07\n",
      "step: 2002/2500 train loss: 0.66, LR: 2.21e-07\n",
      "step: 2003/2500 train loss: 0.7, LR: 2.20e-07\n",
      "step: 2004/2500 train loss: 0.74, LR: 2.20e-07\n",
      "step: 2005/2500 train loss: 0.66, LR: 2.20e-07\n",
      "step: 2006/2500 train loss: 0.67, LR: 2.19e-07\n",
      "step: 2007/2500 train loss: 0.65, LR: 2.19e-07\n",
      "step: 2008/2500 train loss: 0.65, LR: 2.18e-07\n",
      "step: 2009/2500 train loss: 0.75, LR: 2.18e-07\n",
      "step: 2010/2500 train loss: 0.72, LR: 2.17e-07\n",
      "step: 2011/2500 train loss: 0.66, LR: 2.17e-07\n",
      "step: 2012/2500 train loss: 0.74, LR: 2.16e-07\n",
      "step: 2013/2500 train loss: 0.72, LR: 2.16e-07\n",
      "step: 2014/2500 train loss: 0.71, LR: 2.16e-07\n",
      "step: 2015/2500 train loss: 0.58, LR: 2.15e-07\n",
      "step: 2016/2500 train loss: 0.57, LR: 2.15e-07\n",
      "step: 2017/2500 train loss: 0.65, LR: 2.14e-07\n",
      "step: 2018/2500 train loss: 0.74, LR: 2.14e-07\n",
      "step: 2019/2500 train loss: 0.68, LR: 2.13e-07\n",
      "step: 2020/2500 train loss: 0.66, LR: 2.13e-07\n",
      "step: 2021/2500 train loss: 0.81, LR: 2.12e-07\n",
      "step: 2022/2500 train loss: 0.7, LR: 2.12e-07\n",
      "step: 2023/2500 train loss: 0.62, LR: 2.12e-07\n",
      "step: 2024/2500 train loss: 0.59, LR: 2.11e-07\n",
      "step: 2025/2500 train loss: 0.64, LR: 2.11e-07\n",
      "step: 2026/2500 train loss: 0.72, LR: 2.10e-07\n",
      "step: 2027/2500 train loss: 0.72, LR: 2.10e-07\n",
      "step: 2028/2500 train loss: 0.63, LR: 2.09e-07\n",
      "step: 2029/2500 train loss: 0.77, LR: 2.09e-07\n",
      "step: 2030/2500 train loss: 0.73, LR: 2.08e-07\n",
      "step: 2031/2500 train loss: 0.74, LR: 2.08e-07\n",
      "step: 2032/2500 train loss: 0.58, LR: 2.08e-07\n",
      "step: 2033/2500 train loss: 0.69, LR: 2.07e-07\n",
      "step: 2034/2500 train loss: 0.71, LR: 2.07e-07\n",
      "step: 2035/2500 train loss: 0.71, LR: 2.06e-07\n",
      "step: 2036/2500 train loss: 0.67, LR: 2.06e-07\n",
      "step: 2037/2500 train loss: 0.65, LR: 2.05e-07\n",
      "step: 2038/2500 train loss: 0.65, LR: 2.05e-07\n",
      "step: 2039/2500 train loss: 0.9, LR: 2.04e-07\n",
      "step: 2040/2500 train loss: 0.73, LR: 2.04e-07\n",
      "step: 2041/2500 train loss: 0.72, LR: 2.04e-07\n",
      "step: 2042/2500 train loss: 0.79, LR: 2.03e-07\n",
      "step: 2043/2500 train loss: 0.84, LR: 2.03e-07\n",
      "step: 2044/2500 train loss: 0.8, LR: 2.02e-07\n",
      "step: 2045/2500 train loss: 0.66, LR: 2.02e-07\n",
      "step: 2046/2500 train loss: 0.73, LR: 2.01e-07\n",
      "step: 2047/2500 train loss: 0.63, LR: 2.01e-07\n",
      "step: 2048/2500 train loss: 0.68, LR: 2.00e-07\n",
      "step: 2049/2500 train loss: 0.67, LR: 2.00e-07\n",
      "step: 2050/2500 train loss: 0.76, LR: 2.00e-07\n",
      "step: 2051/2500 train loss: 0.66, LR: 1.99e-07\n",
      "step: 2052/2500 train loss: 0.74, LR: 1.99e-07\n",
      "step: 2053/2500 train loss: 0.67, LR: 1.98e-07\n",
      "step: 2054/2500 train loss: 0.66, LR: 1.98e-07\n",
      "step: 2055/2500 train loss: 0.76, LR: 1.97e-07\n",
      "step: 2056/2500 train loss: 0.7, LR: 1.97e-07\n",
      "step: 2057/2500 train loss: 0.62, LR: 1.96e-07\n",
      "step: 2058/2500 train loss: 0.75, LR: 1.96e-07\n",
      "step: 2059/2500 train loss: 0.67, LR: 1.96e-07\n",
      "step: 2060/2500 train loss: 0.76, LR: 1.95e-07\n",
      "step: 2061/2500 train loss: 0.72, LR: 1.95e-07\n",
      "step: 2062/2500 train loss: 0.67, LR: 1.94e-07\n",
      "step: 2063/2500 train loss: 0.65, LR: 1.94e-07\n",
      "step: 2064/2500 train loss: 0.69, LR: 1.93e-07\n",
      "step: 2065/2500 train loss: 0.72, LR: 1.93e-07\n",
      "step: 2066/2500 train loss: 0.71, LR: 1.92e-07\n",
      "step: 2067/2500 train loss: 0.59, LR: 1.92e-07\n",
      "step: 2068/2500 train loss: 0.72, LR: 1.92e-07\n",
      "step: 2069/2500 train loss: 0.64, LR: 1.91e-07\n",
      "step: 2070/2500 train loss: 0.69, LR: 1.91e-07\n",
      "step: 2071/2500 train loss: 0.69, LR: 1.90e-07\n",
      "step: 2072/2500 train loss: 0.69, LR: 1.90e-07\n",
      "step: 2073/2500 train loss: 0.78, LR: 1.89e-07\n",
      "step: 2074/2500 train loss: 0.81, LR: 1.89e-07\n",
      "step: 2075/2500 train loss: 0.71, LR: 1.88e-07\n",
      "step: 2076/2500 train loss: 0.64, LR: 1.88e-07\n",
      "step: 2077/2500 train loss: 0.65, LR: 1.88e-07\n",
      "step: 2078/2500 train loss: 0.69, LR: 1.87e-07\n",
      "step: 2079/2500 train loss: 0.7, LR: 1.87e-07\n",
      "step: 2080/2500 train loss: 0.83, LR: 1.86e-07\n",
      "step: 2081/2500 train loss: 0.74, LR: 1.86e-07\n",
      "step: 2082/2500 train loss: 0.64, LR: 1.85e-07\n",
      "step: 2083/2500 train loss: 0.71, LR: 1.85e-07\n",
      "step: 2084/2500 train loss: 0.72, LR: 1.84e-07\n",
      "step: 2085/2500 train loss: 0.63, LR: 1.84e-07\n",
      "step: 2086/2500 train loss: 0.74, LR: 1.84e-07\n",
      "step: 2087/2500 train loss: 0.72, LR: 1.83e-07\n",
      "step: 2088/2500 train loss: 0.62, LR: 1.83e-07\n",
      "step: 2089/2500 train loss: 0.56, LR: 1.82e-07\n",
      "step: 2090/2500 train loss: 0.64, LR: 1.82e-07\n",
      "step: 2091/2500 train loss: 0.72, LR: 1.81e-07\n",
      "step: 2092/2500 train loss: 0.67, LR: 1.81e-07\n",
      "step: 2093/2500 train loss: 0.72, LR: 1.80e-07\n",
      "step: 2094/2500 train loss: 0.65, LR: 1.80e-07\n",
      "step: 2095/2500 train loss: 0.62, LR: 1.80e-07\n",
      "step: 2096/2500 train loss: 0.71, LR: 1.79e-07\n",
      "step: 2097/2500 train loss: 0.7, LR: 1.79e-07\n",
      "step: 2098/2500 train loss: 0.72, LR: 1.78e-07\n",
      "step: 2099/2500 train loss: 0.62, LR: 1.78e-07\n",
      "step: 2100/2500 train loss: 0.64, LR: 1.77e-07\n",
      "step: 2101/2500 train loss: 0.75, LR: 1.77e-07\n",
      "step: 2102/2500 train loss: 0.59, LR: 1.76e-07\n",
      "step: 2103/2500 train loss: 0.74, LR: 1.76e-07\n",
      "step: 2104/2500 train loss: 0.63, LR: 1.76e-07\n",
      "step: 2105/2500 train loss: 0.76, LR: 1.75e-07\n",
      "step: 2106/2500 train loss: 0.81, LR: 1.75e-07\n",
      "step: 2107/2500 train loss: 0.79, LR: 1.74e-07\n",
      "step: 2108/2500 train loss: 0.85, LR: 1.74e-07\n",
      "step: 2109/2500 train loss: 0.72, LR: 1.73e-07\n",
      "step: 2110/2500 train loss: 0.61, LR: 1.73e-07\n",
      "step: 2111/2500 train loss: 0.69, LR: 1.72e-07\n",
      "step: 2112/2500 train loss: 0.76, LR: 1.72e-07\n",
      "step: 2113/2500 train loss: 0.79, LR: 1.72e-07\n",
      "step: 2114/2500 train loss: 0.64, LR: 1.71e-07\n",
      "step: 2115/2500 train loss: 0.71, LR: 1.71e-07\n",
      "step: 2116/2500 train loss: 0.7, LR: 1.70e-07\n",
      "step: 2117/2500 train loss: 0.61, LR: 1.70e-07\n",
      "step: 2118/2500 train loss: 0.7, LR: 1.69e-07\n",
      "step: 2119/2500 train loss: 0.65, LR: 1.69e-07\n",
      "step: 2120/2500 train loss: 0.8, LR: 1.68e-07\n",
      "step: 2121/2500 train loss: 0.6, LR: 1.68e-07\n",
      "step: 2122/2500 train loss: 0.68, LR: 1.68e-07\n",
      "step: 2123/2500 train loss: 0.73, LR: 1.67e-07\n",
      "step: 2124/2500 train loss: 0.72, LR: 1.67e-07\n",
      "step: 2125/2500 train loss: 0.78, LR: 1.66e-07\n",
      "step: 2126/2500 train loss: 0.63, LR: 1.66e-07\n",
      "step: 2127/2500 train loss: 0.66, LR: 1.65e-07\n",
      "step: 2128/2500 train loss: 0.66, LR: 1.65e-07\n",
      "step: 2129/2500 train loss: 0.6, LR: 1.64e-07\n",
      "step: 2130/2500 train loss: 0.66, LR: 1.64e-07\n",
      "step: 2131/2500 train loss: 0.8, LR: 1.64e-07\n",
      "step: 2132/2500 train loss: 0.62, LR: 1.63e-07\n",
      "step: 2133/2500 train loss: 0.57, LR: 1.63e-07\n",
      "step: 2134/2500 train loss: 0.73, LR: 1.62e-07\n",
      "step: 2135/2500 train loss: 0.73, LR: 1.62e-07\n",
      "step: 2136/2500 train loss: 0.77, LR: 1.61e-07\n",
      "step: 2137/2500 train loss: 0.79, LR: 1.61e-07\n",
      "step: 2138/2500 train loss: 0.73, LR: 1.60e-07\n",
      "step: 2139/2500 train loss: 0.71, LR: 1.60e-07\n",
      "step: 2140/2500 train loss: 0.84, LR: 1.60e-07\n",
      "step: 2141/2500 train loss: 0.68, LR: 1.59e-07\n",
      "step: 2142/2500 train loss: 0.73, LR: 1.59e-07\n",
      "step: 2143/2500 train loss: 0.64, LR: 1.58e-07\n",
      "step: 2144/2500 train loss: 0.69, LR: 1.58e-07\n",
      "step: 2145/2500 train loss: 0.8, LR: 1.57e-07\n",
      "step: 2146/2500 train loss: 0.77, LR: 1.57e-07\n",
      "step: 2147/2500 train loss: 0.65, LR: 1.56e-07\n",
      "step: 2148/2500 train loss: 0.72, LR: 1.56e-07\n",
      "step: 2149/2500 train loss: 0.72, LR: 1.56e-07\n",
      "step: 2150/2500 train loss: 0.68, LR: 1.55e-07\n",
      "step: 2151/2500 train loss: 0.7, LR: 1.55e-07\n",
      "step: 2152/2500 train loss: 0.69, LR: 1.54e-07\n",
      "step: 2153/2500 train loss: 0.69, LR: 1.54e-07\n",
      "step: 2154/2500 train loss: 0.64, LR: 1.53e-07\n",
      "step: 2155/2500 train loss: 0.69, LR: 1.53e-07\n",
      "step: 2156/2500 train loss: 0.74, LR: 1.52e-07\n",
      "step: 2157/2500 train loss: 0.65, LR: 1.52e-07\n",
      "step: 2158/2500 train loss: 0.7, LR: 1.52e-07\n",
      "step: 2159/2500 train loss: 0.7, LR: 1.51e-07\n",
      "step: 2160/2500 train loss: 0.63, LR: 1.51e-07\n",
      "step: 2161/2500 train loss: 0.66, LR: 1.50e-07\n",
      "step: 2162/2500 train loss: 0.61, LR: 1.50e-07\n",
      "step: 2163/2500 train loss: 0.68, LR: 1.49e-07\n",
      "step: 2164/2500 train loss: 0.7, LR: 1.49e-07\n",
      "step: 2165/2500 train loss: 0.65, LR: 1.48e-07\n",
      "step: 2166/2500 train loss: 0.78, LR: 1.48e-07\n",
      "step: 2167/2500 train loss: 0.7, LR: 1.48e-07\n",
      "step: 2168/2500 train loss: 0.62, LR: 1.47e-07\n",
      "step: 2169/2500 train loss: 0.56, LR: 1.47e-07\n",
      "step: 2170/2500 train loss: 0.59, LR: 1.46e-07\n",
      "step: 2171/2500 train loss: 0.57, LR: 1.46e-07\n",
      "step: 2172/2500 train loss: 0.77, LR: 1.45e-07\n",
      "step: 2173/2500 train loss: 0.73, LR: 1.45e-07\n",
      "step: 2174/2500 train loss: 0.64, LR: 1.44e-07\n",
      "step: 2175/2500 train loss: 0.79, LR: 1.44e-07\n",
      "step: 2176/2500 train loss: 0.75, LR: 1.44e-07\n",
      "step: 2177/2500 train loss: 0.75, LR: 1.43e-07\n",
      "step: 2178/2500 train loss: 0.74, LR: 1.43e-07\n",
      "step: 2179/2500 train loss: 0.69, LR: 1.42e-07\n",
      "step: 2180/2500 train loss: 0.76, LR: 1.42e-07\n",
      "step: 2181/2500 train loss: 0.6, LR: 1.41e-07\n",
      "step: 2182/2500 train loss: 0.68, LR: 1.41e-07\n",
      "step: 2183/2500 train loss: 0.74, LR: 1.40e-07\n",
      "step: 2184/2500 train loss: 0.64, LR: 1.40e-07\n",
      "step: 2185/2500 train loss: 0.73, LR: 1.40e-07\n",
      "step: 2186/2500 train loss: 0.63, LR: 1.39e-07\n",
      "step: 2187/2500 train loss: 0.78, LR: 1.39e-07\n",
      "step: 2188/2500 train loss: 0.77, LR: 1.38e-07\n",
      "step: 2189/2500 train loss: 0.7, LR: 1.38e-07\n",
      "step: 2190/2500 train loss: 0.7, LR: 1.37e-07\n",
      "step: 2191/2500 train loss: 0.75, LR: 1.37e-07\n",
      "step: 2192/2500 train loss: 0.82, LR: 1.36e-07\n",
      "step: 2193/2500 train loss: 0.71, LR: 1.36e-07\n",
      "step: 2194/2500 train loss: 0.68, LR: 1.36e-07\n",
      "step: 2195/2500 train loss: 0.75, LR: 1.35e-07\n",
      "step: 2196/2500 train loss: 0.66, LR: 1.35e-07\n",
      "step: 2197/2500 train loss: 0.79, LR: 1.34e-07\n",
      "step: 2198/2500 train loss: 0.68, LR: 1.34e-07\n",
      "step: 2199/2500 train loss: 0.7, LR: 1.33e-07\n",
      "step: 2200/2500 train loss: 0.85, LR: 1.33e-07\n",
      "step: 2201/2500 train loss: 0.6, LR: 1.32e-07\n",
      "step: 2202/2500 train loss: 0.66, LR: 1.32e-07\n",
      "step: 2203/2500 train loss: 0.59, LR: 1.32e-07\n",
      "step: 2204/2500 train loss: 0.67, LR: 1.31e-07\n",
      "step: 2205/2500 train loss: 0.74, LR: 1.31e-07\n",
      "step: 2206/2500 train loss: 0.68, LR: 1.30e-07\n",
      "step: 2207/2500 train loss: 0.64, LR: 1.30e-07\n",
      "step: 2208/2500 train loss: 0.68, LR: 1.29e-07\n",
      "step: 2209/2500 train loss: 0.61, LR: 1.29e-07\n",
      "step: 2210/2500 train loss: 0.72, LR: 1.28e-07\n",
      "step: 2211/2500 train loss: 0.75, LR: 1.28e-07\n",
      "step: 2212/2500 train loss: 0.62, LR: 1.28e-07\n",
      "step: 2213/2500 train loss: 0.65, LR: 1.27e-07\n",
      "step: 2214/2500 train loss: 0.7, LR: 1.27e-07\n",
      "step: 2215/2500 train loss: 0.71, LR: 1.26e-07\n",
      "step: 2216/2500 train loss: 0.82, LR: 1.26e-07\n",
      "step: 2217/2500 train loss: 0.82, LR: 1.25e-07\n",
      "step: 2218/2500 train loss: 0.77, LR: 1.25e-07\n",
      "step: 2219/2500 train loss: 0.67, LR: 1.24e-07\n",
      "step: 2220/2500 train loss: 0.63, LR: 1.24e-07\n",
      "step: 2221/2500 train loss: 0.71, LR: 1.24e-07\n",
      "step: 2222/2500 train loss: 0.7, LR: 1.23e-07\n",
      "step: 2223/2500 train loss: 0.75, LR: 1.23e-07\n",
      "step: 2224/2500 train loss: 0.69, LR: 1.22e-07\n",
      "step: 2225/2500 train loss: 0.63, LR: 1.22e-07\n",
      "step: 2226/2500 train loss: 0.77, LR: 1.21e-07\n",
      "step: 2227/2500 train loss: 0.78, LR: 1.21e-07\n",
      "step: 2228/2500 train loss: 0.7, LR: 1.20e-07\n",
      "step: 2229/2500 train loss: 0.66, LR: 1.20e-07\n",
      "step: 2230/2500 train loss: 0.75, LR: 1.20e-07\n",
      "step: 2231/2500 train loss: 0.77, LR: 1.19e-07\n",
      "step: 2232/2500 train loss: 0.67, LR: 1.19e-07\n",
      "step: 2233/2500 train loss: 0.68, LR: 1.18e-07\n",
      "step: 2234/2500 train loss: 0.8, LR: 1.18e-07\n",
      "step: 2235/2500 train loss: 0.63, LR: 1.17e-07\n",
      "step: 2236/2500 train loss: 0.65, LR: 1.17e-07\n",
      "step: 2237/2500 train loss: 0.78, LR: 1.16e-07\n",
      "step: 2238/2500 train loss: 0.75, LR: 1.16e-07\n",
      "step: 2239/2500 train loss: 0.68, LR: 1.16e-07\n",
      "step: 2240/2500 train loss: 0.64, LR: 1.15e-07\n",
      "step: 2241/2500 train loss: 0.6, LR: 1.15e-07\n",
      "step: 2242/2500 train loss: 0.74, LR: 1.14e-07\n",
      "step: 2243/2500 train loss: 0.67, LR: 1.14e-07\n",
      "step: 2244/2500 train loss: 0.59, LR: 1.13e-07\n",
      "step: 2245/2500 train loss: 0.71, LR: 1.13e-07\n",
      "step: 2246/2500 train loss: 0.65, LR: 1.12e-07\n",
      "step: 2247/2500 train loss: 0.81, LR: 1.12e-07\n",
      "step: 2248/2500 train loss: 0.7, LR: 1.12e-07\n",
      "step: 2249/2500 train loss: 0.82, LR: 1.11e-07\n",
      "step: 2250/2500 eval loss: 0.68\n",
      "step: 2250/2500 train loss: 0.7, LR: 1.11e-07\n",
      "step: 2251/2500 train loss: 0.68, LR: 1.10e-07\n",
      "step: 2252/2500 train loss: 0.87, LR: 1.10e-07\n",
      "step: 2253/2500 train loss: 0.63, LR: 1.09e-07\n",
      "step: 2254/2500 train loss: 0.6, LR: 1.09e-07\n",
      "step: 2255/2500 train loss: 0.7, LR: 1.08e-07\n",
      "step: 2256/2500 train loss: 0.68, LR: 1.08e-07\n",
      "step: 2257/2500 train loss: 0.7, LR: 1.08e-07\n",
      "step: 2258/2500 train loss: 0.72, LR: 1.07e-07\n",
      "step: 2259/2500 train loss: 0.71, LR: 1.07e-07\n",
      "step: 2260/2500 train loss: 0.65, LR: 1.06e-07\n",
      "step: 2261/2500 train loss: 0.77, LR: 1.06e-07\n",
      "step: 2262/2500 train loss: 0.64, LR: 1.05e-07\n",
      "step: 2263/2500 train loss: 0.59, LR: 1.05e-07\n",
      "step: 2264/2500 train loss: 0.69, LR: 1.04e-07\n",
      "step: 2265/2500 train loss: 0.72, LR: 1.04e-07\n",
      "step: 2266/2500 train loss: 0.65, LR: 1.04e-07\n",
      "step: 2267/2500 train loss: 0.71, LR: 1.03e-07\n",
      "step: 2268/2500 train loss: 0.64, LR: 1.03e-07\n",
      "step: 2269/2500 train loss: 0.64, LR: 1.02e-07\n",
      "step: 2270/2500 train loss: 0.69, LR: 1.02e-07\n",
      "step: 2271/2500 train loss: 0.71, LR: 1.01e-07\n",
      "step: 2272/2500 train loss: 0.72, LR: 1.01e-07\n",
      "step: 2273/2500 train loss: 0.77, LR: 1.00e-07\n",
      "step: 2274/2500 train loss: 0.6, LR: 1.00e-07\n",
      "step: 2275/2500 train loss: 0.63, LR: 9.96e-08\n",
      "step: 2276/2500 train loss: 0.69, LR: 9.91e-08\n",
      "step: 2277/2500 train loss: 0.72, LR: 9.87e-08\n",
      "step: 2278/2500 train loss: 0.65, LR: 9.82e-08\n",
      "step: 2279/2500 train loss: 0.65, LR: 9.78e-08\n",
      "step: 2280/2500 train loss: 0.76, LR: 9.73e-08\n",
      "step: 2281/2500 train loss: 0.65, LR: 9.69e-08\n",
      "step: 2282/2500 train loss: 0.63, LR: 9.64e-08\n",
      "step: 2283/2500 train loss: 0.74, LR: 9.60e-08\n",
      "step: 2284/2500 train loss: 0.69, LR: 9.56e-08\n",
      "step: 2285/2500 train loss: 0.66, LR: 9.51e-08\n",
      "step: 2286/2500 train loss: 0.67, LR: 9.47e-08\n",
      "step: 2287/2500 train loss: 0.75, LR: 9.42e-08\n",
      "step: 2288/2500 train loss: 0.63, LR: 9.38e-08\n",
      "step: 2289/2500 train loss: 0.69, LR: 9.33e-08\n",
      "step: 2290/2500 train loss: 0.86, LR: 9.29e-08\n",
      "step: 2291/2500 train loss: 0.72, LR: 9.24e-08\n",
      "step: 2292/2500 train loss: 0.74, LR: 9.20e-08\n",
      "step: 2293/2500 train loss: 0.71, LR: 9.16e-08\n",
      "step: 2294/2500 train loss: 0.72, LR: 9.11e-08\n",
      "step: 2295/2500 train loss: 0.7, LR: 9.07e-08\n",
      "step: 2296/2500 train loss: 0.71, LR: 9.02e-08\n",
      "step: 2297/2500 train loss: 0.7, LR: 8.98e-08\n",
      "step: 2298/2500 train loss: 0.67, LR: 8.93e-08\n",
      "step: 2299/2500 train loss: 0.74, LR: 8.89e-08\n",
      "step: 2300/2500 train loss: 0.74, LR: 8.84e-08\n",
      "step: 2301/2500 train loss: 0.71, LR: 8.80e-08\n",
      "step: 2302/2500 train loss: 0.64, LR: 8.76e-08\n",
      "step: 2303/2500 train loss: 0.69, LR: 8.71e-08\n",
      "step: 2304/2500 train loss: 0.74, LR: 8.67e-08\n",
      "step: 2305/2500 train loss: 0.66, LR: 8.62e-08\n",
      "step: 2306/2500 train loss: 0.68, LR: 8.58e-08\n",
      "step: 2307/2500 train loss: 0.69, LR: 8.53e-08\n",
      "step: 2308/2500 train loss: 0.63, LR: 8.49e-08\n",
      "step: 2309/2500 train loss: 0.71, LR: 8.44e-08\n",
      "step: 2310/2500 train loss: 0.75, LR: 8.40e-08\n",
      "step: 2311/2500 train loss: 0.72, LR: 8.36e-08\n",
      "step: 2312/2500 train loss: 0.85, LR: 8.31e-08\n",
      "step: 2313/2500 train loss: 0.63, LR: 8.27e-08\n",
      "step: 2314/2500 train loss: 0.63, LR: 8.22e-08\n",
      "step: 2315/2500 train loss: 0.73, LR: 8.18e-08\n",
      "step: 2316/2500 train loss: 0.71, LR: 8.13e-08\n",
      "step: 2317/2500 train loss: 0.8, LR: 8.09e-08\n",
      "step: 2318/2500 train loss: 0.62, LR: 8.04e-08\n",
      "step: 2319/2500 train loss: 0.71, LR: 8.00e-08\n",
      "step: 2320/2500 train loss: 0.71, LR: 7.96e-08\n",
      "step: 2321/2500 train loss: 0.69, LR: 7.91e-08\n",
      "step: 2322/2500 train loss: 0.78, LR: 7.87e-08\n",
      "step: 2323/2500 train loss: 0.68, LR: 7.82e-08\n",
      "step: 2324/2500 train loss: 0.71, LR: 7.78e-08\n",
      "step: 2325/2500 train loss: 0.64, LR: 7.73e-08\n",
      "step: 2326/2500 train loss: 0.76, LR: 7.69e-08\n",
      "step: 2327/2500 train loss: 0.69, LR: 7.64e-08\n",
      "step: 2328/2500 train loss: 0.65, LR: 7.60e-08\n",
      "step: 2329/2500 train loss: 0.71, LR: 7.56e-08\n",
      "step: 2330/2500 train loss: 0.7, LR: 7.51e-08\n",
      "step: 2331/2500 train loss: 0.7, LR: 7.47e-08\n",
      "step: 2332/2500 train loss: 0.6, LR: 7.42e-08\n",
      "step: 2333/2500 train loss: 0.62, LR: 7.38e-08\n",
      "step: 2334/2500 train loss: 0.69, LR: 7.33e-08\n",
      "step: 2335/2500 train loss: 0.68, LR: 7.29e-08\n",
      "step: 2336/2500 train loss: 0.77, LR: 7.24e-08\n",
      "step: 2337/2500 train loss: 0.81, LR: 7.20e-08\n",
      "step: 2338/2500 train loss: 0.78, LR: 7.16e-08\n",
      "step: 2339/2500 train loss: 0.65, LR: 7.11e-08\n",
      "step: 2340/2500 train loss: 0.62, LR: 7.07e-08\n",
      "step: 2341/2500 train loss: 0.67, LR: 7.02e-08\n",
      "step: 2342/2500 train loss: 0.72, LR: 6.98e-08\n",
      "step: 2343/2500 train loss: 0.72, LR: 6.93e-08\n",
      "step: 2344/2500 train loss: 0.62, LR: 6.89e-08\n",
      "step: 2345/2500 train loss: 0.65, LR: 6.84e-08\n",
      "step: 2346/2500 train loss: 0.71, LR: 6.80e-08\n",
      "step: 2347/2500 train loss: 0.58, LR: 6.76e-08\n",
      "step: 2348/2500 train loss: 0.77, LR: 6.71e-08\n",
      "step: 2349/2500 train loss: 0.63, LR: 6.67e-08\n",
      "step: 2350/2500 train loss: 0.71, LR: 6.62e-08\n",
      "step: 2351/2500 train loss: 0.66, LR: 6.58e-08\n",
      "step: 2352/2500 train loss: 0.7, LR: 6.53e-08\n",
      "step: 2353/2500 train loss: 0.78, LR: 6.49e-08\n",
      "step: 2354/2500 train loss: 0.67, LR: 6.44e-08\n",
      "step: 2355/2500 train loss: 0.68, LR: 6.40e-08\n",
      "step: 2356/2500 train loss: 0.72, LR: 6.36e-08\n",
      "step: 2357/2500 train loss: 0.72, LR: 6.31e-08\n",
      "step: 2358/2500 train loss: 0.68, LR: 6.27e-08\n",
      "step: 2359/2500 train loss: 0.7, LR: 6.22e-08\n",
      "step: 2360/2500 train loss: 0.66, LR: 6.18e-08\n",
      "step: 2361/2500 train loss: 0.7, LR: 6.13e-08\n",
      "step: 2362/2500 train loss: 0.61, LR: 6.09e-08\n",
      "step: 2363/2500 train loss: 0.74, LR: 6.04e-08\n",
      "step: 2364/2500 train loss: 0.74, LR: 6.00e-08\n",
      "step: 2365/2500 train loss: 0.63, LR: 5.96e-08\n",
      "step: 2366/2500 train loss: 0.7, LR: 5.91e-08\n",
      "step: 2367/2500 train loss: 0.75, LR: 5.87e-08\n",
      "step: 2368/2500 train loss: 0.59, LR: 5.82e-08\n",
      "step: 2369/2500 train loss: 0.62, LR: 5.78e-08\n",
      "step: 2370/2500 train loss: 0.6, LR: 5.73e-08\n",
      "step: 2371/2500 train loss: 0.74, LR: 5.69e-08\n",
      "step: 2372/2500 train loss: 0.63, LR: 5.64e-08\n",
      "step: 2373/2500 train loss: 0.6, LR: 5.60e-08\n",
      "step: 2374/2500 train loss: 0.68, LR: 5.56e-08\n",
      "step: 2375/2500 train loss: 0.63, LR: 5.51e-08\n",
      "step: 2376/2500 train loss: 0.65, LR: 5.47e-08\n",
      "step: 2377/2500 train loss: 0.69, LR: 5.42e-08\n",
      "step: 2378/2500 train loss: 0.66, LR: 5.38e-08\n",
      "step: 2379/2500 train loss: 0.66, LR: 5.33e-08\n",
      "step: 2380/2500 train loss: 0.64, LR: 5.29e-08\n",
      "step: 2381/2500 train loss: 0.78, LR: 5.24e-08\n",
      "step: 2382/2500 train loss: 0.6, LR: 5.20e-08\n",
      "step: 2383/2500 train loss: 0.74, LR: 5.16e-08\n",
      "step: 2384/2500 train loss: 0.68, LR: 5.11e-08\n",
      "step: 2385/2500 train loss: 0.7, LR: 5.07e-08\n",
      "step: 2386/2500 train loss: 0.68, LR: 5.02e-08\n",
      "step: 2387/2500 train loss: 0.57, LR: 4.98e-08\n",
      "step: 2388/2500 train loss: 0.79, LR: 4.93e-08\n",
      "step: 2389/2500 train loss: 0.65, LR: 4.89e-08\n",
      "step: 2390/2500 train loss: 0.72, LR: 4.84e-08\n",
      "step: 2391/2500 train loss: 0.7, LR: 4.80e-08\n",
      "step: 2392/2500 train loss: 0.64, LR: 4.76e-08\n",
      "step: 2393/2500 train loss: 0.66, LR: 4.71e-08\n",
      "step: 2394/2500 train loss: 0.66, LR: 4.67e-08\n",
      "step: 2395/2500 train loss: 0.64, LR: 4.62e-08\n",
      "step: 2396/2500 train loss: 0.69, LR: 4.58e-08\n",
      "step: 2397/2500 train loss: 0.7, LR: 4.53e-08\n",
      "step: 2398/2500 train loss: 0.77, LR: 4.49e-08\n",
      "step: 2399/2500 train loss: 0.65, LR: 4.44e-08\n",
      "step: 2400/2500 train loss: 0.7, LR: 4.40e-08\n",
      "step: 2401/2500 train loss: 0.77, LR: 4.36e-08\n",
      "step: 2402/2500 train loss: 0.64, LR: 4.31e-08\n",
      "step: 2403/2500 train loss: 0.67, LR: 4.27e-08\n",
      "step: 2404/2500 train loss: 0.69, LR: 4.22e-08\n",
      "step: 2405/2500 train loss: 0.77, LR: 4.18e-08\n",
      "step: 2406/2500 train loss: 0.67, LR: 4.13e-08\n",
      "step: 2407/2500 train loss: 0.79, LR: 4.09e-08\n",
      "step: 2408/2500 train loss: 0.7, LR: 4.04e-08\n",
      "step: 2409/2500 train loss: 0.69, LR: 4.00e-08\n",
      "step: 2410/2500 train loss: 0.74, LR: 3.96e-08\n",
      "step: 2411/2500 train loss: 0.76, LR: 3.91e-08\n",
      "step: 2412/2500 train loss: 0.67, LR: 3.87e-08\n",
      "step: 2413/2500 train loss: 0.73, LR: 3.82e-08\n",
      "step: 2414/2500 train loss: 0.72, LR: 3.78e-08\n",
      "step: 2415/2500 train loss: 0.72, LR: 3.73e-08\n",
      "step: 2416/2500 train loss: 0.64, LR: 3.69e-08\n",
      "step: 2417/2500 train loss: 0.6, LR: 3.64e-08\n",
      "step: 2418/2500 train loss: 0.64, LR: 3.60e-08\n",
      "step: 2419/2500 train loss: 0.73, LR: 3.56e-08\n",
      "step: 2420/2500 train loss: 0.73, LR: 3.51e-08\n",
      "step: 2421/2500 train loss: 0.69, LR: 3.47e-08\n",
      "step: 2422/2500 train loss: 0.73, LR: 3.42e-08\n",
      "step: 2423/2500 train loss: 0.77, LR: 3.38e-08\n",
      "step: 2424/2500 train loss: 0.71, LR: 3.33e-08\n",
      "step: 2425/2500 train loss: 0.67, LR: 3.29e-08\n",
      "step: 2426/2500 train loss: 0.6, LR: 3.24e-08\n",
      "step: 2427/2500 train loss: 0.73, LR: 3.20e-08\n",
      "step: 2428/2500 train loss: 0.79, LR: 3.16e-08\n",
      "step: 2429/2500 train loss: 0.77, LR: 3.11e-08\n",
      "step: 2430/2500 train loss: 0.64, LR: 3.07e-08\n",
      "step: 2431/2500 train loss: 0.69, LR: 3.02e-08\n",
      "step: 2432/2500 train loss: 0.63, LR: 2.98e-08\n",
      "step: 2433/2500 train loss: 0.68, LR: 2.93e-08\n",
      "step: 2434/2500 train loss: 0.68, LR: 2.89e-08\n",
      "step: 2435/2500 train loss: 0.64, LR: 2.84e-08\n",
      "step: 2436/2500 train loss: 0.68, LR: 2.80e-08\n",
      "step: 2437/2500 train loss: 0.69, LR: 2.76e-08\n",
      "step: 2438/2500 train loss: 0.67, LR: 2.71e-08\n",
      "step: 2439/2500 train loss: 0.67, LR: 2.67e-08\n",
      "step: 2440/2500 train loss: 0.68, LR: 2.62e-08\n",
      "step: 2441/2500 train loss: 0.7, LR: 2.58e-08\n",
      "step: 2442/2500 train loss: 0.75, LR: 2.53e-08\n",
      "step: 2443/2500 train loss: 0.68, LR: 2.49e-08\n",
      "step: 2444/2500 train loss: 0.72, LR: 2.44e-08\n",
      "step: 2445/2500 train loss: 0.63, LR: 2.40e-08\n",
      "step: 2446/2500 train loss: 0.69, LR: 2.36e-08\n",
      "step: 2447/2500 train loss: 0.63, LR: 2.31e-08\n",
      "step: 2448/2500 train loss: 0.79, LR: 2.27e-08\n",
      "step: 2449/2500 train loss: 0.73, LR: 2.22e-08\n",
      "step: 2450/2500 train loss: 0.8, LR: 2.18e-08\n",
      "step: 2451/2500 train loss: 0.74, LR: 2.13e-08\n",
      "step: 2452/2500 train loss: 0.76, LR: 2.09e-08\n",
      "step: 2453/2500 train loss: 0.66, LR: 2.04e-08\n",
      "step: 2454/2500 train loss: 0.7, LR: 2.00e-08\n",
      "step: 2455/2500 train loss: 0.66, LR: 1.96e-08\n",
      "step: 2456/2500 train loss: 0.71, LR: 1.91e-08\n",
      "step: 2457/2500 train loss: 0.77, LR: 1.87e-08\n",
      "step: 2458/2500 train loss: 0.59, LR: 1.82e-08\n",
      "step: 2459/2500 train loss: 0.64, LR: 1.78e-08\n",
      "step: 2460/2500 train loss: 0.67, LR: 1.73e-08\n",
      "step: 2461/2500 train loss: 0.59, LR: 1.69e-08\n",
      "step: 2462/2500 train loss: 0.73, LR: 1.64e-08\n",
      "step: 2463/2500 train loss: 0.66, LR: 1.60e-08\n",
      "step: 2464/2500 train loss: 0.66, LR: 1.56e-08\n",
      "step: 2465/2500 train loss: 0.89, LR: 1.51e-08\n",
      "step: 2466/2500 train loss: 0.69, LR: 1.47e-08\n",
      "step: 2467/2500 train loss: 0.71, LR: 1.42e-08\n",
      "step: 2468/2500 train loss: 0.73, LR: 1.38e-08\n",
      "step: 2469/2500 train loss: 0.69, LR: 1.33e-08\n",
      "step: 2470/2500 train loss: 0.75, LR: 1.29e-08\n",
      "step: 2471/2500 train loss: 0.69, LR: 1.24e-08\n",
      "step: 2472/2500 train loss: 0.74, LR: 1.20e-08\n",
      "step: 2473/2500 train loss: 0.75, LR: 1.16e-08\n",
      "step: 2474/2500 train loss: 0.73, LR: 1.11e-08\n",
      "step: 2475/2500 train loss: 0.57, LR: 1.07e-08\n",
      "step: 2476/2500 train loss: 0.74, LR: 1.02e-08\n",
      "step: 2477/2500 train loss: 0.66, LR: 9.78e-09\n",
      "step: 2478/2500 train loss: 0.69, LR: 9.33e-09\n",
      "step: 2479/2500 train loss: 0.68, LR: 8.89e-09\n",
      "step: 2480/2500 train loss: 0.69, LR: 8.44e-09\n",
      "step: 2481/2500 train loss: 0.8, LR: 8.00e-09\n",
      "step: 2482/2500 train loss: 0.8, LR: 7.56e-09\n",
      "step: 2483/2500 train loss: 0.79, LR: 7.11e-09\n",
      "step: 2484/2500 train loss: 0.67, LR: 6.67e-09\n",
      "step: 2485/2500 train loss: 0.72, LR: 6.22e-09\n",
      "step: 2486/2500 train loss: 0.69, LR: 5.78e-09\n",
      "step: 2487/2500 train loss: 0.76, LR: 5.33e-09\n",
      "step: 2488/2500 train loss: 0.73, LR: 4.89e-09\n",
      "step: 2489/2500 train loss: 0.61, LR: 4.44e-09\n",
      "step: 2490/2500 train loss: 0.67, LR: 4.00e-09\n",
      "step: 2491/2500 train loss: 0.69, LR: 3.56e-09\n",
      "step: 2492/2500 train loss: 0.63, LR: 3.11e-09\n",
      "step: 2493/2500 train loss: 0.66, LR: 2.67e-09\n",
      "step: 2494/2500 train loss: 0.69, LR: 2.22e-09\n",
      "step: 2495/2500 train loss: 0.94, LR: 1.78e-09\n",
      "step: 2496/2500 train loss: 0.74, LR: 1.33e-09\n",
      "step: 2497/2500 train loss: 0.72, LR: 8.89e-10\n",
      "step: 2498/2500 train loss: 0.68, LR: 4.44e-10\n",
      "step: 2499/2500 train loss: 0.86, LR: 0.00e+00\n",
      "step: 2499/2500 eval loss: 0.68\n",
      "Finishing Training.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_classifier(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, dataset_test):\n",
    "    predictions_list = [] # list to store every batch of predictions\n",
    "    for batch in dataset_test['input_ids']:\n",
    "        logits = model(batch) # derive the logits of one batch of inputs\n",
    "        prediction = torch.argmax(logits, dim=-1) # prediction is the highest value logit for each item in sequence\n",
    "        predictions_list.append(prediction)\n",
    "    \n",
    "    predictions = torch.stack(predictions_list).view(1000, 20) # convert list to tensor and flatten batch dimension\n",
    "    labels = dataset_test['labels'].view(1000, 20) # flatten batch dimension of labels\n",
    "    \n",
    "    print(f\"Test Accuracy: {100.0 * score(predictions, labels):.2f}%\") # calculate score\n",
    "    print_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.71%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(model, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
