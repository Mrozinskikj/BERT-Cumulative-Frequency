{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    random.shuffle(inputs) # shuffle incase inputs are ordered\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        dropout: float,\n",
    "        vocab_size: int,\n",
    "        length: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : float\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for Layer Normalisation, used to normalise input tensors such that the embedding dimension (-1) has zero mean and unit variance.\n",
    "    Learnable gain and bias parameters for each embedding element allow for increased flexibility for downstream tasks.\n",
    "    Helps to stabilise learning by keeping weights within a controlled range.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        Small constant preventing division by zero.\n",
    "    gain : nn.Parameter\n",
    "        Learnable gain (multiplier) parameters for each element in embedding dimension. Applied after normalisation.\n",
    "    bias : nn.Parameter\n",
    "        Learnable bias (addition) parameters for each element in embedding dimension. Applied after normalisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Normalises and scales the embedding dimension of the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        epsilon: float = 1e-5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the LayerNorm module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            The size of the embedding dimension. Used to correctly initialise gain and bias parameters.\n",
    "        epsilon : float, optional\n",
    "            Small constant preventing division by zero. Defaults to 1e-5.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.epsilon = epsilon # small constant prevents division by zero\n",
    "        self.gain = nn.Parameter(torch.ones(embed_dim)) # learnable gain (multiplier) parameters for each element in embed_dim\n",
    "        self.bias = nn.Parameter(torch.zeros(embed_dim)) # learnable bias (addition) parameters for each element in embed_dim\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalises and scales the embedding dimension of the input tensor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The input tensor to be normalised across 'embed_dim'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The normalised and scaled input tensor. Prior to scaling, 'embed_dim' has zero mean and unit variance.\n",
    "        \"\"\"\n",
    "        mean = inputs.mean(dim=-1, keepdim=True) # compute the mean across the embedding dimension (-1)\n",
    "        variance = ((inputs - mean) ** 2).mean(dim=-1, keepdim=True) # compute the variance (average of squared deviations from mean) across the embedding dimension (-1)\n",
    "        std = torch.sqrt(variance + self.epsilon) # calculate standard deviation\n",
    "\n",
    "        normalised = (inputs - mean) / std # normalise inputs to mean 0 and standard deviation 1\n",
    "        scaled = normalised * self.gain + self.bias # normalised tensor is shifted and scaled by learnable parameters. increased flexibility\n",
    "        return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for an individual attention head within a MultiHeadAttention module.\n",
    "    Projects input embeddings into keys, values, and queries, then computes attention scores.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    head_size : int\n",
    "        Dimension of each of key, value, and query. Calculated by MultiHeadAttention module based on embed_dim and num_heads.\n",
    "    key : nn.Linear\n",
    "        Linear transformation for projecting input tensor into key space.\n",
    "    query : nn.Linear\n",
    "        Linear transformation for projecting input tensor into query space.\n",
    "    value : nn.Linear\n",
    "        Linear transformation for projecting input tensor into value space.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_tensor: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        head_size: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the AttentionHead module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            The size of the embedding dimension.\n",
    "        head_size : int\n",
    "            Dimension of each of key, value, and query. Calculated by MultiHeadAttention module based on embed_dim and num_heads.\n",
    "        dropout : float\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # keys queries and values are projected from embedding dimension to 'head_size'\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # dropout for regularisation\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the attention scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The transformer input tensor for which attention needs to be calculated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [batch_size, length, head_size])\n",
    "            The attention scores.\n",
    "        \"\"\"\n",
    "        # project input tensor to 'head_size' for keys, queries, and values ([batch_size, length, embed_dim] -> [batch_size, length, head_size])\n",
    "        key = self.key(input_tensor)\n",
    "        query = self.query(input_tensor)\n",
    "        value = self.value(input_tensor)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) # attention scores are dot product between query and key ([batch_size, length, head_size] x [batch_size, head_size, length ] -> [batch_size, length, length])\n",
    "        scores = scores / (self.head_size**0.5) # divide by sqrt of head size to normalise to unit variance. increases stability- high variance would make softmax sharp\n",
    "\n",
    "        attn_weights = nn.functional.softmax(scores, dim=-1) # convert scores into probability distribution\n",
    "        attn_weights = self.dropout(attn_weights) # apply dropout for regularisation\n",
    "\n",
    "        output = torch.matmul(attn_weights, value) # output is the weighted sum of values\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for processing an input tensor with multiple attention heads in parallel.\n",
    "    Attention head output is recombined with a linear transformation.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_heads : int\n",
    "        Number of attention heads. Must be a factor of 'embed_dim'.\n",
    "    heads : nn.ModuleList\n",
    "        List containing all 'num_heads' intances of 'AttentionHead'.\n",
    "    linear : nn.Linear\n",
    "        Linear transformation to re-integrate attenion head outputs into a unified representation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_tensor: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, processing the input_tensor through multiple attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the MultiHeadAttention module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            The size of the embedding dimension.\n",
    "        num_heads : int\n",
    "            Number of attention heads. Must be a factor of 'embed_dim'.\n",
    "        dropout : float\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "\n",
    "        if embed_dim % num_heads != 0: # ensure each attention head gets an equal distribution of the input tensor\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads.\")\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_size = embed_dim // num_heads # size of each attention head is such that the concatenation of all attention heads is embed_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(embed_dim, head_size, dropout) for _ in range(num_heads)\n",
    "        ]) # list of all attention heads\n",
    "\n",
    "        self.linear = nn.Linear(embed_dim, embed_dim) # linear transformation\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, processing the input_tensor through multiple attention heads.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The input tensor to be processed by the attention mechanism.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The final output tensor after attention computation and reintegration.\n",
    "        \"\"\"\n",
    "        head_outputs = [head(input_tensor) for head in self.heads] # compute the attention scores of each head in parallel\n",
    "        concatenated = torch.cat(head_outputs, dim=-1) # concatenate all attention head outputs back into single tensor\n",
    "        output = self.linear(concatenated) # re-integrate attention heads into unified representation with final linear transformation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a single Transformer layer composed of multi-head attention, normalisation, and feed-forward layers.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    attention : MultiHeadAttention\n",
    "        Attention mechanism capturing the relationships between each item in the input sequence.\n",
    "    layer_norm1 : LayerNorm\n",
    "        Normalisation of the attention sub-layer, for stability.\n",
    "    feedforward : nn.Sequential\n",
    "        Two layer deep feed-forward network to process the attention sub-layer. Uses GELU activation as per BERT paper.\n",
    "    layer_norm2 : LayerNorm\n",
    "        Normalisation of the feed-forward sub-layer, for stability.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the intermediate transformer output representation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        attention_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            Dimensionality of the embeddings.\n",
    "        dropout : float\n",
    "            Dropout probability, used for regularisation.\n",
    "        attention_heads : int\n",
    "            The number of attention heads in the Transformer encoder layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_dim, attention_heads, dropout) # attention mechanism capturing relationships between each item in input\n",
    "        self.layer_norm1 = LayerNorm(embed_dim) # normalisation for stability\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        ) # 2 layer deep feed-forward network\n",
    "        self.layer_norm2 = LayerNorm(embed_dim) # normalisation for stability\n",
    "        self.dropout = nn.Dropout(dropout) # dropout for regularisation\n",
    "    \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the intermediate transformer output representation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The transformer input tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [batch_size, length, embed_dim])\n",
    "            The transformer output tensor.\n",
    "        \"\"\"\n",
    "        attn_output = self.attention(input_tensor) # compute the attention scores\n",
    "        attn_output = input_tensor + self.dropout(attn_output) # residual connection and dropout\n",
    "        attn_output = self.layer_norm1(attn_output) # layer normalisation\n",
    "\n",
    "        ffwd_output = self.feedforward(attn_output) # process through feed-forward network\n",
    "        ffwd_output = attn_output + self.dropout(ffwd_output) # residual connection and dropout\n",
    "        output_tensor = self.layer_norm2(ffwd_output) # layer normalisation\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    transformer_layers : nn.ModuleList\n",
    "        A list of TransformerLayer modules. Input is fed through each layer in sequence.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        dropout: float,\n",
    "        attention_heads: int,\n",
    "        layers: int,\n",
    "        vocab_size: int = 27,\n",
    "        length: int = 20,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : float\n",
    "            Dropout probability, used for regularisation.\n",
    "        attention_heads : int\n",
    "            The number of attention heads in the Transformer encoder layer.\n",
    "        layers : int\n",
    "            The number of Transformer encoder layers.\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(embed_dim, dropout, vocab_size, length) # embedding layer which combines token and position embeddings\n",
    "        \n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(embed_dim, attention_heads, dropout) for _ in range(layers)\n",
    "        ]) # sequence of transformer layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "\n",
    "        for layer in self.transformer_layers: # feed input through each transformer layer in sequence\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        logits = self.classifier(embeddings) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'seed': 0,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'epochs': 1,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'eval_every': 250,\n",
    "    'embed_dim': 768,\n",
    "    'dropout': 0.1,\n",
    "    'attention_heads': 12,\n",
    "    'layers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token_embedding): Embedding(27, 768)\n",
      "    (position_embedding): Embedding(20, 768)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-11): 12 x AttentionHead(\n",
      "            (key): Linear(in_features=768, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=768, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=768, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm()\n",
      "      (feedforward): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BERT(\n",
    "    params['embed_dim'],\n",
    "    params['dropout'],\n",
    "    params['attention_heads'],\n",
    "    params['layers'],\n",
    ") # initialise model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'seed': 0,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 1e-6,\n",
    "    'epochs': 1,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'eval_every': 250,\n",
    "    'embed_dim': 768,\n",
    "    'dropout': 0.1,\n",
    "    'attention_heads': 12,\n",
    "    'layers': 2\n",
    "}\n",
    "model = BERT(\n",
    "    params['embed_dim'],\n",
    "    params['dropout'],\n",
    "    params['attention_heads'],\n",
    "    params['layers'],\n",
    ") # initialise model\n",
    "tokeniser = Tokeniser()\n",
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)\n",
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.Tensor([[[0.2,0.1,0.3],[0.5,0.1,0.1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset_test['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "def lr_scheduler(\n",
    "    warmup_ratio: float,\n",
    "    step_current: int,\n",
    "    step_total: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Defines a custom learning rate scheduler (warmup and decay) to adjust learning rate based on current training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    warmup_ratio : float\n",
    "        The ratio of total training steps that learning rate warmup occurs for. 0 = no warmup, 1 = all warmup.\n",
    "    step_current : int\n",
    "        The current training step during evaluation.\n",
    "    step_total : int\n",
    "        The total number of training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The ratio that the learning rate will be multiplied by for the given training step.\n",
    "    \"\"\"\n",
    "    warmup_steps = int(step_total*warmup_ratio)\n",
    "    if step_current < warmup_steps: # LR warmup for initial steps\n",
    "        return step_current/max(1,warmup_steps)\n",
    "    else: # linear LR decay for remaining steps\n",
    "        return (step_total-step_current) / max(1,step_total-warmup_steps)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BERT,\n",
    "    dataset_test: dict,\n",
    "    loss_fn: nn.CrossEntropyLoss,\n",
    "    plot_data: dict,\n",
    "    step_current: int,\n",
    "    step_total: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Peforms model evaluation by computing the average loss of the entire test dataset. The average loss is printed and 'plot_data' is updated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BERT\n",
    "        An instance of the BERT model to be evaluated.\n",
    "    dataset_test : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    loss_fn : nn.CrossEntropyLoss\n",
    "        The loss function used to compute the loss between the predictions and labels.\n",
    "    plot_data : dict\n",
    "        A dictionary of x and y timeline data of training progress.\n",
    "        - 'train' : dict\n",
    "            Timeline data for the training loss.\n",
    "            - 'x': list\n",
    "                A list of x-coordinate values, representing the given training step.\n",
    "            - 'y': list\n",
    "                A list of y-coordinate values, representing the value at the given training step.\n",
    "        - 'test' : dict\n",
    "            Timeline data for the validation loss.\n",
    "            Refer to 'train'.\n",
    "        - 'lr' : dict\n",
    "            Timeline data for the learning rate.\n",
    "            Refer to 'train'.\n",
    "    step_current : int\n",
    "        The current training step during evaluation.\n",
    "    step_total : int\n",
    "        The total number of training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated plot data dictionary with the test loss added.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    batches = len(dataset_test['input_ids']) # number of batches in the test dataset\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        for batch in range(batches):\n",
    "            \n",
    "            logits = model(dataset_test['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_test['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "\n",
    "            loss_batch = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            loss_total += loss_batch.item()\n",
    "\n",
    "    loss_average = loss_total / batches # loss is the average of all batches\n",
    "    model.train() # revert model to training mode\n",
    "\n",
    "    plot_data['test']['x'].append(step_current)\n",
    "    plot_data['test']['y'].append(loss_average)\n",
    "    print(f'step: {step_current}/{step_total} eval loss: {round(loss_average,2)}')\n",
    "    return plot_data\n",
    "\n",
    "\n",
    "def train_classifier(\n",
    "    model: BERT,\n",
    "    dataset_train: dict,\n",
    "    dataset_test: dict,\n",
    "    learning_rate: float,\n",
    "    epochs: int,\n",
    "    warmup_ratio: float,\n",
    "    eval_every: int,\n",
    "    print_train: bool = False,\n",
    "    plot: bool = True\n",
    ") -> BERT:\n",
    "    \"\"\"\n",
    "    Creates and trains a BERT model for cumulative frequency classification given a training dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BERT\n",
    "        An instance of the BERT model to perform training on.\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the training data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        Refer to 'dataset_train'.\n",
    "    learning_rate : float\n",
    "        The learning rate for the optimiser (magnitiude of weight updates per step).\n",
    "    epochs : int\n",
    "        The number of epochs for training. Each epoch corresponds to one full iteration through training data.\n",
    "    warmup_ratio : float\n",
    "        The ratio of total training steps that learning rate warmup occurs for. 0 = no warmup, 1 = all warmup.\n",
    "\n",
    "    print_train : bool, optional\n",
    "        Whether to print the training state at every training step. Defaults to False.\n",
    "    plot : bool, optional\n",
    "        Whether to display a plot of the training timeline once training is finished. Defaults to True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BERT\n",
    "        The trained BERT model.\n",
    "    \"\"\"\n",
    "    plot_data = {key: {'x':[], 'y':[]} for key in ['train','test','lr']} # dict storing x,y plot data for training progress\n",
    "    \n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    batches = len(dataset_train['input_ids']) # number of batches in the training dataset\n",
    "    step_total = batches*epochs\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate) # initialise AdamW optimiser\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimiser, lr_lambda=lambda step: lr_scheduler(warmup_ratio, step, step_total)) # create custom learning rate scheduler\n",
    "    loss_fn = nn.CrossEntropyLoss() # initialise cross-entropy loss function for classification\n",
    "\n",
    "    print(\"Beginning Training.\")\n",
    "    print_line()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs): # iterate through epochs\n",
    "        for batch in range(batches): # iterate through batches in epoch\n",
    "            step_current = batch*(epoch+1)\n",
    "            \n",
    "            if batch%eval_every == 0: # perform evaluation on test split at set intervals\n",
    "                plot_data = evaluate(model, dataset_test, loss_fn, plot_data, step_current, step_total)\n",
    "\n",
    "            logits = model(dataset_train['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "            \n",
    "            loss = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            \n",
    "            optimiser.zero_grad() # zero the gradients from previous step (no gradient accumulation)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimiser.step() # update model weights\n",
    "            scheduler.step() # update learning rate\n",
    "\n",
    "            plot_data['train']['x'].append(step_current)\n",
    "            plot_data['train']['y'].append(loss.item())\n",
    "            plot_data['lr']['x'].append(step_current)\n",
    "            plot_data['lr']['y'].append(scheduler.get_last_lr()[0])\n",
    "            if print_train:\n",
    "                print(f'step: {step_current}/{step_total} train loss: {round(loss.item(),2)}, LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    if batch%eval_every != 0: # perform final evaluation (as long as not already performed on this step)\n",
    "        plot_data = evaluate(model, dataset_test, loss_fn, plot_data, step_current, step_total)\n",
    "    print(f\"Finishing Training. Time taken: {(time.time()-start_time):.2f} seconds.\")\n",
    "    print_line()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training.\n",
      "--------------------------------------------------------------------------------\n",
      "step: 0/2500 eval loss: 1.13\n",
      "step: 250/2500 eval loss: 0.87\n",
      "step: 500/2500 eval loss: 0.76\n",
      "step: 750/2500 eval loss: 0.72\n",
      "step: 1000/2500 eval loss: 0.7\n",
      "step: 1250/2500 eval loss: 0.69\n",
      "step: 1500/2500 eval loss: 0.69\n",
      "step: 1750/2500 eval loss: 0.68\n",
      "step: 2000/2500 eval loss: 0.68\n",
      "step: 2250/2500 eval loss: 0.68\n",
      "step: 2499/2500 eval loss: 0.68\n",
      "Finishing Training. Time taken: 379.61 seconds.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = train_classifier(\n",
    "    model,\n",
    "    dataset_train,\n",
    "    dataset_test,\n",
    "    params['learning_rate'],\n",
    "    params['epochs'],\n",
    "    params['warmup_ratio'],\n",
    "    params['eval_every'],\n",
    "    print_train=False,\n",
    "    plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, dataset_test):\n",
    "    predictions_list = [] # list to store every batch of predictions\n",
    "    for batch in dataset_test['input_ids']:\n",
    "        logits = model(batch) # derive the logits of one batch of inputs\n",
    "        prediction = torch.argmax(logits, dim=-1) # prediction is the highest value logit for each item in sequence\n",
    "        predictions_list.append(prediction)\n",
    "    \n",
    "    predictions = torch.stack(predictions_list).view(1000, 20) # convert list to tensor and flatten batch dimension\n",
    "    labels = dataset_test['labels'].view(1000, 20) # flatten batch dimension of labels\n",
    "    \n",
    "    print(f\"Test Accuracy: {100.0 * score(predictions, labels):.2f}%\") # calculate score\n",
    "    print_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.85%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_accuracy(model, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
