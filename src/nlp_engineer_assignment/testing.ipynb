{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  0, 13, 26, 12,  0, 13, 24, 26,  8, 13, 26, 19,  7,  4, 26,  0, 13,\n",
      "         0, 17])\n",
      "man many in the anar\n"
     ]
    }
   ],
   "source": [
    "tokeniser = Tokeniser()\n",
    "print(tokeniser.encode(train_inputs[0]))\n",
    "print(tokeniser.decode(tokeniser.encode(train_inputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset\n",
    "\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, length: int, embed_dim: int, dropout: int):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : int\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    encoder_block : nn.TransformerEncoder\n",
    "        Transformer Encoder.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 27, length: int = 20, embed_dim: int = 768, dropout: int = 0.1, attention_heads: int = 12, layers: int = 2):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        embed_dim : int, optional\n",
    "            Dimensionality of the token and position embeddings. Defaults to 768.\n",
    "        dropout : int, optional\n",
    "            Dropout probability, used for regularisation. Defaults to 0.1.\n",
    "        attention_heads : int, optional\n",
    "            The number of attention heads in the Transformer encoder layer. Defaults to 12.\n",
    "        layers : int, optional\n",
    "            The number of Transformer encoder layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()  # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size, length, embed_dim, dropout) # embedding layer which combines token and position embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, attention_heads, dim_feedforward=embed_dim * 4, dropout=dropout, activation=\"gelu\") # instance of transformer encoder layer\n",
    "        self.encoder_block = nn.TransformerEncoder(encoder_layer, layers) # full transformer encoder consisting of multiple layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "        embeddings = embeddings.transpose(0, 1) # rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\n",
    "\n",
    "        encoder_output = self.encoder_block(embeddings) # pass embeddings through transformer encoder block\n",
    "\n",
    "        logits = self.classifier(encoder_output.transpose(0, 1)) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)\n",
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(dataset_train, dataset_test, learning_rate=1e-6, epochs=1):\n",
    "    model = BERT()\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate) # create AdamW optimiser\n",
    "    loss_fn = nn.CrossEntropyLoss() # init loss function, cross entropy for classification\n",
    "\n",
    "    batches = len(dataset_train['input_ids'])\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "\n",
    "            logits = model(dataset_train['input_ids'][batch]) # feed inputs through model to get output logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "            \n",
    "            loss = loss_fn(logits, labels) # calculate loss by comparing output logits to labels\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            print(f'step: {batch*(epoch+1)}/{batches*epochs} loss: {round(loss.item(),2)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0/2500 loss: 1.23\n",
      "step: 1/2500 loss: 1.17\n",
      "step: 2/2500 loss: 1.24\n",
      "step: 3/2500 loss: 1.2\n",
      "step: 4/2500 loss: 1.2\n",
      "step: 5/2500 loss: 1.13\n",
      "step: 6/2500 loss: 1.18\n",
      "step: 7/2500 loss: 1.09\n",
      "step: 8/2500 loss: 1.24\n",
      "step: 9/2500 loss: 1.18\n",
      "step: 10/2500 loss: 1.12\n",
      "step: 11/2500 loss: 1.05\n",
      "step: 12/2500 loss: 1.07\n",
      "step: 13/2500 loss: 1.05\n",
      "step: 14/2500 loss: 1.08\n",
      "step: 15/2500 loss: 1.16\n",
      "step: 16/2500 loss: 1.1\n",
      "step: 17/2500 loss: 1.14\n",
      "step: 18/2500 loss: 1.17\n",
      "step: 19/2500 loss: 1.08\n",
      "step: 20/2500 loss: 1.01\n",
      "step: 21/2500 loss: 1.12\n",
      "step: 22/2500 loss: 0.99\n",
      "step: 23/2500 loss: 0.97\n",
      "step: 24/2500 loss: 1.06\n",
      "step: 25/2500 loss: 1.06\n",
      "step: 26/2500 loss: 1.11\n",
      "step: 27/2500 loss: 1.0\n",
      "step: 28/2500 loss: 1.08\n",
      "step: 29/2500 loss: 1.02\n",
      "step: 30/2500 loss: 1.06\n",
      "step: 31/2500 loss: 1.06\n",
      "step: 32/2500 loss: 1.01\n",
      "step: 33/2500 loss: 1.03\n",
      "step: 34/2500 loss: 1.02\n",
      "step: 35/2500 loss: 1.07\n",
      "step: 36/2500 loss: 1.12\n",
      "step: 37/2500 loss: 0.94\n",
      "step: 38/2500 loss: 1.02\n",
      "step: 39/2500 loss: 0.98\n",
      "step: 40/2500 loss: 0.97\n",
      "step: 41/2500 loss: 1.05\n",
      "step: 42/2500 loss: 1.07\n",
      "step: 43/2500 loss: 1.07\n",
      "step: 44/2500 loss: 1.03\n",
      "step: 45/2500 loss: 1.05\n",
      "step: 46/2500 loss: 1.02\n",
      "step: 47/2500 loss: 1.07\n",
      "step: 48/2500 loss: 1.04\n",
      "step: 49/2500 loss: 0.9\n",
      "step: 50/2500 loss: 1.05\n",
      "step: 51/2500 loss: 1.05\n",
      "step: 52/2500 loss: 1.05\n",
      "step: 53/2500 loss: 0.97\n",
      "step: 54/2500 loss: 1.01\n",
      "step: 55/2500 loss: 1.05\n",
      "step: 56/2500 loss: 0.99\n",
      "step: 57/2500 loss: 1.03\n",
      "step: 58/2500 loss: 0.99\n",
      "step: 59/2500 loss: 0.96\n",
      "step: 60/2500 loss: 1.1\n",
      "step: 61/2500 loss: 0.97\n",
      "step: 62/2500 loss: 0.93\n",
      "step: 63/2500 loss: 0.86\n",
      "step: 64/2500 loss: 1.01\n",
      "step: 65/2500 loss: 1.03\n",
      "step: 66/2500 loss: 0.99\n",
      "step: 67/2500 loss: 1.0\n",
      "step: 68/2500 loss: 1.09\n",
      "step: 69/2500 loss: 1.01\n",
      "step: 70/2500 loss: 0.95\n",
      "step: 71/2500 loss: 0.97\n",
      "step: 72/2500 loss: 1.01\n",
      "step: 73/2500 loss: 1.06\n",
      "step: 74/2500 loss: 1.0\n",
      "step: 75/2500 loss: 0.9\n",
      "step: 76/2500 loss: 0.96\n",
      "step: 77/2500 loss: 0.92\n",
      "step: 78/2500 loss: 1.0\n",
      "step: 79/2500 loss: 0.98\n",
      "step: 80/2500 loss: 0.98\n",
      "step: 81/2500 loss: 0.82\n",
      "step: 82/2500 loss: 0.92\n",
      "step: 83/2500 loss: 0.93\n",
      "step: 84/2500 loss: 1.04\n",
      "step: 85/2500 loss: 0.98\n",
      "step: 86/2500 loss: 1.01\n",
      "step: 87/2500 loss: 0.98\n",
      "step: 88/2500 loss: 1.04\n",
      "step: 89/2500 loss: 0.94\n",
      "step: 90/2500 loss: 0.91\n",
      "step: 91/2500 loss: 0.91\n",
      "step: 92/2500 loss: 0.96\n",
      "step: 93/2500 loss: 0.96\n",
      "step: 94/2500 loss: 1.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset_train, dataset_test, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels) \u001b[38;5;66;03m# calculate loss by comparing output logits to labels\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m*\u001b[39m(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches\u001b[38;5;241m*\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_classifier(dataset_train, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
