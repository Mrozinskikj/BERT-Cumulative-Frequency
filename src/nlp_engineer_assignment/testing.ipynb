{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import uvicorn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import count_letters, print_line, read_inputs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokeniser:\n",
    "    \"\"\"\n",
    "    A class for encoding and decoding strings into tokens for model input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    char_to_id : dict\n",
    "        Dictionary mapping characters to their corresponding token IDs.\n",
    "    id_to_char : dict\n",
    "        Dictionary mapping token IDs to their corresponding characters.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    encode(string: str) -> torch.Tensor\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "    \n",
    "    decode(tokens: torch.Tensor) -> str\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 20):\n",
    "        \"\"\"\n",
    "        Initialises the tokeniser, defining the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        \n",
    "        vocab = [chr(ord('a') + i) for i in range(0, 26)] + [' '] # vocab of lowerchase chars and space\n",
    "\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(vocab)} # dictionary of character to token id\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(vocab)} # dictionary of token id to character\n",
    "    \n",
    "    def encode(self, string: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a string into a tensor of token IDs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        string : str\n",
    "            The input string to encode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor (shape [self.length])\n",
    "            A tensor containing the token IDs corresponding to input string.\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If 'string' is not 'self.length' characters long.\n",
    "            If 'string' contains out-of-vocabulary characters.\n",
    "        \"\"\"\n",
    "        if len(string) != self.length: # ensure input string is correct length\n",
    "            raise ValueError(f\"Input string must be exactly {self.length} characters long, but got {len(string)} characters.\")\n",
    "        \n",
    "        try:\n",
    "            tokens_list = [self.char_to_id[c] for c in string] # convert string to tokens list\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Out of vocabulary character encountered: '{e.args[0]}'\")\n",
    "        \n",
    "        tokens_tensor = torch.tensor(tokens_list, dtype=torch.long) # convert token list into tensor\n",
    "        return tokens_tensor\n",
    "    \n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a tensor of token IDs into a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            A tensor containing token IDs to decode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A decoded string corresponding to input tokens.\n",
    "        \"\"\"\n",
    "        return \"\".join([self.id_to_char[i.item()] for i in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines read\n",
      "--------------------------------------------------------------------------------\n",
      "1000 lines read\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_inputs = read_inputs(\"../../data/train.txt\")\n",
    "test_inputs = read_inputs(\"../../data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  0, 13, 26, 12,  0, 13, 24, 26,  8, 13, 26, 19,  7,  4, 26,  0, 13,\n",
      "         0, 17])\n",
      "man many in the anar\n"
     ]
    }
   ],
   "source": [
    "tokeniser = Tokeniser()\n",
    "print(tokeniser.encode(train_inputs[0]))\n",
    "print(tokeniser.decode(tokeniser.encode(train_inputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def batch_tensor(tensor_list, batch_size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a list of 1D tensors into a batched 3D tensor. Used with 'process_dataset'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_list : list of torch.Tensor\n",
    "        A list of 1D tensors to be batched together.\n",
    "    batch_size : int\n",
    "        The number of tensors to include in each batch.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "        A 3D batched tensor, grouping each input tensor into groups of size 'batch_size'.\n",
    "    \"\"\"\n",
    "    tensor_stacked = torch.stack(tensor_list) # convert list of 1D tensors to stacked 2D tensor\n",
    "    \n",
    "    num_batches = len(tensor_stacked) // batch_size # find whole number of batches (may trim last items)\n",
    "    excess_items = len(tensor_stacked) % batch_size # calculate number of extra items which don't fit into batches\n",
    "    if excess_items != 0:\n",
    "        print(f\"Trimming last {excess_items} items to ensure equal batch sizes.\")\n",
    "        tensor_stacked = tensor_stacked[:-excess_items] # trim tensor\n",
    "    \n",
    "    batched_tensor = tensor_stacked.view(num_batches, batch_size, -1) # reshape 2D tensor into batched 3D tensor\n",
    "    return batched_tensor\n",
    "    \n",
    "\n",
    "def process_dataset(inputs, tokeniser, batch_size = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Processes raw data into input tokens and labels, creating a dataset dictionary of batched tensors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : list of str\n",
    "        Train or test data examples split into a list.\n",
    "    tokeniser : Tokeniser\n",
    "        An instance of the Tokeniser class used to encode the input.\n",
    "    batch_size : int, optional\n",
    "        The number of items to include in each batch. Defaults to 4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If length of 'inputs' is less than 'batch_size'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(inputs) < batch_size:\n",
    "        raise ValueError(\"Input list is too short for a single batch.\")\n",
    "\n",
    "    input_ids_list = [tokeniser.encode(text) for text in inputs] # list of token tensors for each input\n",
    "    labels_list = [count_letters(text) for text in inputs] # list of label tensors for each input\n",
    "\n",
    "    # create dictionary of batched 3D input and label tensors\n",
    "    dataset = {\n",
    "        'input_ids': batch_tensor(input_ids_list, batch_size),\n",
    "        'labels': batch_tensor(labels_list, batch_size)\n",
    "    }\n",
    "    print(\"Dataset created.\", \", \".join([f\"{key}: {tensor.size()}\" for key, tensor in dataset.items()]))\n",
    "    print_line()\n",
    "    return dataset\n",
    "\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT Embedding layer which creates and combines token and position embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    length : int\n",
    "        Expected length of input strings. Defaults to 20.\n",
    "    token_embedding : nn.Embedding\n",
    "        Embedding layer which maps each token to a dense vector of size 'embed_dim'.\n",
    "    position_embedding : nn.Embedding\n",
    "        Embedding layer which maps each position index to a dense vector of size 'embed_dim'.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer for regularisation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, length: int, embed_dim: int, dropout: int):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Total number of unique tokens.\n",
    "        length : int\n",
    "            Expected length of input strings.\n",
    "        embed_dim : int\n",
    "            Dimensionality of the token and position embeddings.\n",
    "        dropout : int\n",
    "            Dropout probability, used for regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__() # initialise the nn.Module parent class\n",
    "        self.length = length # store the sequence length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # map each token to a dense vector of size embed_dim\n",
    "        self.position_embedding = nn.Embedding(length, embed_dim) # map each position index to a dense vector of size embed_dim\n",
    "        self.dropout = nn.Dropout(dropout) # dropout layer for regularisation\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the BERT embeddings used as model input for a given 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, embed_dim])\n",
    "            The tensor containing the BERT embeddings for the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        device = input_ids.device # used to ensure all tensors are on same device\n",
    "\n",
    "        token_embedding = self.token_embedding(input_ids) # look up token embeddings for each token in input_ids\n",
    "\n",
    "        position_input = torch.arange(self.length, device=device).unsqueeze(0) # create position indices for each token\n",
    "        position_embedding = self.position_embedding(position_input) # look up position embeddings for each position index in input_ids\n",
    "        \n",
    "        embedding = token_embedding + position_embedding # BERT embedding is element-wise sum of token embeddings and position embeddings\n",
    "        embedding = self.dropout(embedding) # apply dropout for regularisation\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    A class for a BERT model, used to classify the cumulative frequencies of the respective character of every 'input_ids' item.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding : BERTEmbedding\n",
    "        Embedding layer which combines token and position embeddings.\n",
    "    encoder_block : nn.TransformerEncoder\n",
    "        Transformer Encoder.\n",
    "    classifier : nn.Linear\n",
    "        Output layer, predicting classes 0, 1, 2 for cumulative character frequency for each position in sequence\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(input_ids: torch.Tensor) -> torch.Tensor\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int = 27, length: int = 20, embed_dim: int = 768, dropout: int = 0.1, attention_heads: int = 12, layers: int = 2):\n",
    "        \"\"\"\n",
    "        Initialises the BERT Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int, optional\n",
    "            Total number of unique tokens. Defaults to 27.\n",
    "        length : int, optional\n",
    "            Expected length of input strings. Defaults to 20.\n",
    "        embed_dim : int, optional\n",
    "            Dimensionality of the token and position embeddings. Defaults to 768.\n",
    "        dropout : int, optional\n",
    "            Dropout probability, used for regularisation. Defaults to 0.1.\n",
    "        attention_heads : int, optional\n",
    "            The number of attention heads in the Transformer encoder layer. Defaults to 12.\n",
    "        layers : int, optional\n",
    "            The number of Transformer encoder layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()  # initialise the nn.Module parent class\n",
    "        \n",
    "        self.embedding = BERTEmbedding(vocab_size, length, embed_dim, dropout) # embedding layer which combines token and position embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, attention_heads, dim_feedforward=embed_dim * 4, dropout=dropout, activation=\"gelu\") # instance of transformer encoder layer\n",
    "        self.encoder_block = nn.TransformerEncoder(encoder_layer, layers) # full transformer encoder consisting of multiple layers\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, 3) # output layer, predicting classes 0, 1, 2 for each position in sequence\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a forward pass, computing the logits for each class of each item of 'input_ids'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : torch.Tensor (shape [batch_size, length])\n",
    "            The tensor containing token indices for the input sequences of a given batch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor  (shape [batch_size, length, 3 (classes)])\n",
    "            The tensor containing the class logits for each item of the input sequences of a given batch.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(input_ids) # get embeddings for each token in input_ids\n",
    "        embeddings = embeddings.transpose(0, 1) # rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\n",
    "\n",
    "        encoder_output = self.encoder_block(embeddings) # pass embeddings through transformer encoder block\n",
    "\n",
    "        logits = self.classifier(encoder_output.transpose(0, 1)) # apply classifier to each position to get logits for each class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. input_ids: torch.Size([2500, 4, 20]), labels: torch.Size([2500, 4, 20])\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset created. input_ids: torch.Size([250, 4, 20]), labels: torch.Size([250, 4, 20])\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "dataset_train = process_dataset(train_inputs, tokeniser)\n",
    "dataset_test = process_dataset(test_inputs, tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model : BERT, dataset_test: dict, loss_fn: nn.CrossEntropyLoss) -> float:\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    batches = len(dataset_test['input_ids']) # number of batches in the test dataset\n",
    "    loss_total = 0\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        for batch in range(batches):\n",
    "            \n",
    "            logits = model(dataset_train['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "\n",
    "            loss_batch = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            loss_total += loss_batch.item()\n",
    "\n",
    "    loss_average = loss_total / batches # loss is the average of all batches\n",
    "    print(f'eval loss: {round(loss_average,2)}')\n",
    "    model.train()  # revert model to training mode\n",
    "    return loss_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(dataset_train: dict, dataset_test: dict, learning_rate: float = 1e-6, epochs: int = 1, eval_every: int = 10) -> BERT:\n",
    "    \"\"\"\n",
    "    Creates and trains a BERT model for cumulative frequency classification given a training dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the training data.\n",
    "        - 'input_ids' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of tokenised input strings.\n",
    "        - 'labels' : torch.Tensor (shape [num_batches, batch_size, tensor_length])\n",
    "            The batched tensor of labels corresponding to input IDs.\n",
    "    dataset_train : dict\n",
    "        A dictionary containing the inputs and labels of the test data.\n",
    "        Refer to 'dataset_train'.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for the optimiser (magnitiude of weight updates per step). Defaults to 1e-6.\n",
    "    epochs : int, optional\n",
    "        The number of epochs for training. Each epoch corresponds to one full iteration through training data. Defaults to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BERT\n",
    "        The trained BERT model.\n",
    "    \"\"\"\n",
    "    model = BERT() # initialise model\n",
    "    model.train() # set model to training mode\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=learning_rate) # initialise AdamW optimiser\n",
    "    loss_fn = nn.CrossEntropyLoss() # initialise cross-entropy loss function for classification\n",
    "\n",
    "    batches = len(dataset_train['input_ids']) # number of batches in the training dataset\n",
    "    for epoch in range(epochs): # iterate through epochs\n",
    "        for batch in range(batches): # iterate through batches in epoch\n",
    "            \n",
    "            if batch%eval_every == 0: # perform evaluation on test split at set intervals\n",
    "                loss_test = evaluate(model, dataset_test, loss_fn)\n",
    "\n",
    "            logits = model(dataset_train['input_ids'][batch]) # forward pass to compute logits\n",
    "            logits = logits.view(-1, logits.size(-1)) # flatten batch dimension: [batch_size * length, classes]\n",
    "            labels = dataset_train['labels'][batch].view(-1) # flatten batch dimension: [batch_size * length]\n",
    "            \n",
    "            loss = loss_fn(logits, labels) # calculate loss between output logits and labels\n",
    "            \n",
    "            optimiser.zero_grad() # zero the gradients from previous step (no gradient accumulation)\n",
    "            loss.backward() # backpropagate to compute gradients\n",
    "            optimiser.step() # update model weights\n",
    "\n",
    "            print(f'step: {batch*(epoch+1)}/{batches*epochs} loss: {round(loss.item(),2)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 1.19\n",
      "step: 0/2500 loss: 1.22\n",
      "step: 1/2500 loss: 1.17\n",
      "step: 2/2500 loss: 1.11\n",
      "step: 3/2500 loss: 1.22\n",
      "step: 4/2500 loss: 1.15\n",
      "step: 5/2500 loss: 1.2\n",
      "step: 6/2500 loss: 1.09\n",
      "step: 7/2500 loss: 1.23\n",
      "step: 8/2500 loss: 1.21\n",
      "step: 9/2500 loss: 1.2\n",
      "eval loss: 1.15\n",
      "step: 10/2500 loss: 1.17\n",
      "step: 11/2500 loss: 1.18\n",
      "step: 12/2500 loss: 1.11\n",
      "step: 13/2500 loss: 1.12\n",
      "step: 14/2500 loss: 1.19\n",
      "step: 15/2500 loss: 1.09\n",
      "step: 16/2500 loss: 1.18\n",
      "step: 17/2500 loss: 1.05\n",
      "step: 18/2500 loss: 1.08\n",
      "step: 19/2500 loss: 1.09\n",
      "eval loss: 1.1\n",
      "step: 20/2500 loss: 1.09\n",
      "step: 21/2500 loss: 1.05\n",
      "step: 22/2500 loss: 1.04\n",
      "step: 23/2500 loss: 1.01\n",
      "step: 24/2500 loss: 1.09\n",
      "step: 25/2500 loss: 1.14\n",
      "step: 26/2500 loss: 1.08\n",
      "step: 27/2500 loss: 1.13\n",
      "step: 28/2500 loss: 1.15\n",
      "step: 29/2500 loss: 1.07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 36\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(dataset_train, dataset_test, learning_rate, epochs, eval_every)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batches): \u001b[38;5;66;03m# iterate through batches in epoch\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m%\u001b[39meval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# perform evaluation on test split at set intervals\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         loss_test \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(dataset_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][batch]) \u001b[38;5;66;03m# forward pass to compute logits\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# flatten batch dimension: [batch_size * length, classes]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataset_test, loss_fn)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# disable gradient calculation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batches):\n\u001b[1;32m----> 8\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# forward pass to compute logits\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# flatten batch dimension: [batch_size * length, classes]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         labels \u001b[38;5;241m=\u001b[39m dataset_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][batch]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten batch dimension: [batch_size * length]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[10], line 63\u001b[0m, in \u001b[0;36mBERT.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m     60\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_ids) \u001b[38;5;66;03m# get embeddings for each token in input_ids\u001b[39;00m\n\u001b[0;32m     61\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# rearrange embeddings from [batch_size, length, embed_dim] to [length, batch_size, embed_dim] for encoder block\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# pass embeddings through transformer encoder block\u001b[39;00m\n\u001b[0;32m     65\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(encoder_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# apply classifier to each position to get logits for each class\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m make_causal\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 315\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    318\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    589\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    592\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 599\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1192\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1203\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:5224\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5223\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 5224\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5226\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[0;32m   4763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[0;32m   4764\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[1;32m-> 4765\u001b[0m         proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4766\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   4767\u001b[0m         proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_classifier(dataset_train, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
