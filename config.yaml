model:
  load: true # whether to load the model from the specified local path. if true, skips tuning and training.
  save: false # whether to save the model to the specified local path after training.
  path: 'model/model.pth' # the path to save or load the model

data:
  train_path: "data/train.txt" # path to the train data file
  test_path: "data/test.txt" # path to the test data file

plot:
  train: true # whether to plot train metrics (train loss, test loss, learning rate) after training. will open in a new window, pausing execution until closed.
  tune: true # whether to plot hyperparameter tuning convergence after tuning. will open in a new window, pausing execution until closed.

train:
  embed_dim: 288 # size of the embedding dimension in the model
  layers: 6 # number of transformer layers in the model
  attention_heads: 4 # number of attention heads in each transformer layer
  dropout: 0 # dropout rate for regularisation [0-1]
  
  batch_size: 4 # number of samples per training batch
  learning_rate: 8.0e-5 # magnitude of weight updates during training
  epochs: 1 # number of passes through the train dataset during training
  warmup_ratio: 0.1 # the ratio of total training steps that learning rate warmup occurs for [0-1]
  eval_every: 250 # the step interval between evaluations on test dataset during training. if eval_every>=steps, only eval at the end

  seed: 0 # random seed for reproducibility

tune:
  tune: false # whether to perform hyperparameter tuning before training and serving

  iterations: 64 # number of iterations for hyperparameter tuning
  sample_space: # the search ranges for each specified hyperparameter to be tuned
    learning_rate:
      type: Real
      low: 1.0e-6
      high: 1.0e-2
      prior: 'log-uniform' # log scale ensures searching 1e-5 is equally likely as 1e-4 etc.
    dropout:
      type: Real
      low: 0.0
      high: 0.5
    layers:
      type: Integer
      low: 1
      high: 6
    batch_size:
      type: Categorical
      categories: [1, 2, 4, 8, 16] # possible batch sizes are powers of 2 for efficiency
    attention_heads:
      type: Categorical
      categories: [1, 2, 3, 4] # possible number of attention heads are all factors of embed_dim
    embed_dim:
      type: Categorical
      categories: [12, 24, 36, 48, 60, 72, 84, 96, 108, 120, 132, 144, 156, 168, 180, 192, 204, 216, 228, 240, 252, 264, 276, 288, 300, 312, 324, 336, 348, 360, 372, 384, 396, 408, 420, 432, 444, 456, 468, 480, 492, 504, 516, 528, 540, 552, 564, 576, 588, 600, 612, 624, 636, 648, 660, 672, 684, 696, 708, 720, 732, 744, 756, 768, 780, 792, 804, 816, 828, 840, 852, 864, 876, 888, 900, 912, 924, 936, 948, 960, 972, 984, 996, 1008, 1020, 1032, 1044, 1056, 1068, 1080, 1092, 1104, 1116, 1128, 1140, 1152] # possible embedding dimensions are multiples of 12- all factors of attention_heads